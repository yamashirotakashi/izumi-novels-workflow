#!/usr/bin/env python3
"""
BOOKâ˜†WALKER é«˜åº¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ã‚³ãƒ³ãƒ†ãƒŠãƒ™ãƒ¼ã‚¹æŠ½å‡ºã®ãƒ†ã‚¹ãƒˆ
"""
import asyncio
import sys
import json
from pathlib import Path
from datetime import datetime
import logging

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.scraping.bookwalker_advanced_scraper import BookWalkerAdvancedScraper

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('logs/bookwalker_advanced_test.log')
    ]
)
logger = logging.getLogger(__name__)


async def test_bookwalker_advanced():
    """BOOKâ˜†WALKER é«˜åº¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆå‰å›å¤±æ•—ã—ãŸã‚±ãƒ¼ã‚¹ï¼‰
    test_books = [
        {
            'n_code': 'N0230FK',
            'title': 'ãƒ‘ãƒ©ãƒ¬ã‚¤ãƒ‰ãƒ‡ã‚¤ã‚ºâ‘£',
            'description': 'KADOKAWAç³»ãƒ©ã‚¤ãƒˆãƒãƒ™ãƒ«ï¼ˆä¸¸æ•°å­—å·»æ•°ï¼‰'
        },
        {
            'n_code': 'N7975EJ',
            'title': 'ã‚¨ã‚¢ãƒœãƒ¼ãƒ³ã‚¦ã‚¤ãƒƒãƒâ‘£',
            'description': 'KADOKAWAç³»ãƒ©ã‚¤ãƒˆãƒãƒ™ãƒ«ï¼ˆä¸¸æ•°å­—å·»æ•°ï¼‰'
        },
        {
            'n_code': 'N0000TEST',
            'title': 'ã‚½ãƒ¼ãƒ‰ã‚¢ãƒ¼ãƒˆãƒ»ã‚ªãƒ³ãƒ©ã‚¤ãƒ³1',
            'description': 'äººæ°—ã‚·ãƒªãƒ¼ã‚ºï¼ˆæ•°å­—å·»æ•°ï¼‰'
        }
    ]
    
    print("=== BOOKâ˜†WALKER é«˜åº¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ãƒ†ã‚¹ãƒˆé–‹å§‹ ===")
    print(f"ãƒ†ã‚¹ãƒˆé–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ãƒ†ã‚¹ãƒˆå¯¾è±¡: {len(test_books)}å†Š")
    print("æ”¹è‰¯ç‚¹: ã‚³ãƒ³ãƒ†ãƒŠãƒ™ãƒ¼ã‚¹æŠ½å‡ºã§æ­£ç¢ºãªã‚¿ã‚¤ãƒˆãƒ«å–å¾—")
    print()
    
    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼åˆæœŸåŒ–
    scraper = BookWalkerAdvancedScraper(
        timeout=15,
        max_retries=2
    )
    
    results = []
    
    try:
        async with scraper:
            for i, book in enumerate(test_books, 1):
                print(f"ğŸ“š ãƒ†ã‚¹ãƒˆ {i}/{len(test_books)}: {book['title']}")
                print(f"   èª¬æ˜: {book['description']}")
                
                start_time = datetime.now()
                
                try:
                    # æ›¸ç±æ¤œç´¢å®Ÿè¡Œ
                    url = await scraper.search_book(book['title'], book['n_code'])
                    
                    end_time = datetime.now()
                    processing_time = (end_time - start_time).total_seconds()
                    
                    result = {
                        'n_code': book['n_code'],
                        'title': book['title'],
                        'description': book['description'],
                        'url': url,
                        'success': url is not None,
                        'processing_time': processing_time,
                        'timestamp': end_time.isoformat()
                    }
                    
                    if url:
                        print(f"   âœ… æˆåŠŸ: {url}")
                        print(f"   â±ï¸  å‡¦ç†æ™‚é–“: {processing_time:.1f}ç§’")
                    else:
                        print(f"   âŒ è¦‹ã¤ã‹ã‚‰ãš")
                        print(f"   â±ï¸  å‡¦ç†æ™‚é–“: {processing_time:.1f}ç§’")
                    
                    results.append(result)
                    
                except Exception as e:
                    end_time = datetime.now()
                    processing_time = (end_time - start_time).total_seconds()
                    
                    result = {
                        'n_code': book['n_code'],
                        'title': book['title'],
                        'description': book['description'],
                        'url': None,
                        'success': False,
                        'error': str(e),
                        'processing_time': processing_time,
                        'timestamp': end_time.isoformat()
                    }
                    
                    print(f"   ğŸ’¥ ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    print(f"   â±ï¸  å‡¦ç†æ™‚é–“: {processing_time:.1f}ç§’")
                    results.append(result)
                
                print()
                
                # æ›¸ç±é–“ã®å¾…æ©Ÿï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œï¼‰
                if i < len(test_books):
                    await asyncio.sleep(2)
    
    except Exception as e:
        logger.error(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        print(f"ğŸ’¥ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return
    
    # çµæœã‚µãƒãƒªãƒ¼
    print("=== é«˜åº¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼ ===")
    
    successful_results = [r for r in results if r['success']]
    failed_results = [r for r in results if not r['success']]
    
    success_rate = (len(successful_results) / len(results)) * 100 if results else 0
    avg_processing_time = sum(r['processing_time'] for r in results) / len(results) if results else 0
    
    print(f"ç·å®Ÿè¡Œæ•°: {len(results)}")
    print(f"æˆåŠŸ: {len(successful_results)}ä»¶")
    print(f"å¤±æ•—: {len(failed_results)}ä»¶")
    print(f"æˆåŠŸç‡: {success_rate:.1f}%")
    print(f"å¹³å‡å‡¦ç†æ™‚é–“: {avg_processing_time:.1f}ç§’")
    
    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼çµ±è¨ˆ
    stats = scraper.get_stats()
    print(f"\né«˜åº¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼çµ±è¨ˆ:")
    print(f"  æ¤œç´¢å›æ•°: {stats.get('total_searches', 0)}")
    print(f"  æˆåŠŸå›æ•°: {stats.get('successful_searches', 0)}")
    print(f"  ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°: {stats.get('requests_made', 0)}")
    print(f"  å¹³å‡ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“: {stats.get('avg_response_time', 'N/A')}")
    print(f"  ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚¿ã‚¤ãƒ—: {stats.get('scraper_type', 'Unknown')}")
    
    # è©³ç´°çµæœã®ä¿å­˜
    results_data = {
        'test_info': {
            'scraper': 'BookWalkerAdvancedScraper',
            'test_date': datetime.now().isoformat(),
            'total_tests': len(results),
            'success_count': len(successful_results),
            'failure_count': len(failed_results),
            'success_rate': success_rate,
            'average_processing_time': avg_processing_time,
            'improvements': [
                'ã‚³ãƒ³ãƒ†ãƒŠãƒ™ãƒ¼ã‚¹æŠ½å‡ºå®Ÿè£…',
                'è¤‡æ•°æ–¹æ³•ã«ã‚ˆã‚‹ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º',
                'ã‚ˆã‚Šä½ã„é¡ä¼¼åº¦é–¾å€¤ï¼ˆ0.15ï¼‰',
                'ã‚ˆã‚Šé•·ã„ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ15ç§’ï¼‰'
            ]
        },
        'results': results,
        'scraper_stats': stats
    }
    
    # çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    output_file = f"logs/bookwalker_advanced_test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“„ è©³ç´°çµæœã‚’ä¿å­˜: {output_file}")
    
    # æˆåŠŸã‚±ãƒ¼ã‚¹ã®è©³ç´°è¡¨ç¤º
    if successful_results:
        print(f"\nâœ… æˆåŠŸã‚±ãƒ¼ã‚¹è©³ç´°:")
        for result in successful_results:
            print(f"  - {result['title']}")
            print(f"    URL: {result['url']}")
    
    # å¤±æ•—ã‚±ãƒ¼ã‚¹ã®è©³ç´°è¡¨ç¤º
    if failed_results:
        print(f"\nâŒ å¤±æ•—ã‚±ãƒ¼ã‚¹è©³ç´°:")
        for result in failed_results:
            print(f"  - {result['title']}")
            if 'error' in result:
                print(f"    ã‚¨ãƒ©ãƒ¼: {result['error']}")
    
    # å‰å›çµæœã¨ã®æ¯”è¼ƒ
    print(f"\nğŸ“Š å‰å›çµæœã¨ã®æ¯”è¼ƒ:")
    print(f"  å‰å›ï¼ˆåŸºæœ¬ç‰ˆï¼‰: æˆåŠŸç‡ 0.0% (0/3)")
    print(f"  ä»Šå›ï¼ˆé«˜åº¦ç‰ˆï¼‰: æˆåŠŸç‡ {success_rate:.1f}% ({len(successful_results)}/{len(results)})")
    
    if success_rate > 0:
        print(f"  ğŸ‰ æ”¹å–„ï¼æˆåŠŸç‡ +{success_rate:.1f}%")
    else:
        print(f"  ğŸ¤” ã•ã‚‰ãªã‚‹æ”¹å–„ãŒå¿…è¦")
    
    print(f"\n=== BOOKâ˜†WALKER é«˜åº¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ãƒ†ã‚¹ãƒˆå®Œäº† ===")
    return results


async def test_container_extraction():
    """ã‚³ãƒ³ãƒ†ãƒŠæŠ½å‡ºæ©Ÿèƒ½ã®å˜ä½“ãƒ†ã‚¹ãƒˆ"""
    print("\n=== ã‚³ãƒ³ãƒ†ãƒŠæŠ½å‡ºæ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ ===")
    
    scraper = BookWalkerAdvancedScraper()
    
    # ãƒ‘ãƒ©ãƒ¬ã‚¤ãƒ‰ãƒ‡ã‚¤ã‚ºã§å®Ÿéš›ã®HTMLæ§‹é€ ã‚’ãƒ†ã‚¹ãƒˆ
    test_query = "ãƒ‘ãƒ©ãƒ¬ã‚¤ãƒ‰ãƒ‡ã‚¤ã‚º"
    
    async with scraper:
        try:
            soup = await scraper.make_request(scraper.SEARCH_URL, params={'word': test_query})
            if soup:
                print(f"æ¤œç´¢çµæœã‚’å–å¾—: {test_query}")
                
                # ã‚³ãƒ³ãƒ†ãƒŠç™ºè¦‹ãƒ†ã‚¹ãƒˆ
                containers = scraper._find_book_containers(soup)
                print(f"ç™ºè¦‹ã—ãŸã‚³ãƒ³ãƒ†ãƒŠæ•°: {len(containers)}")
                
                # ä¸Šä½5ã‚³ãƒ³ãƒ†ãƒŠã®è©³ç´°åˆ†æ
                for i, container in enumerate(containers[:5], 1):
                    book_info = scraper._extract_book_info_from_container(container)
                    print(f"\nã‚³ãƒ³ãƒ†ãƒŠ {i}:")
                    if book_info:
                        print(f"  ã‚¿ã‚¤ãƒˆãƒ«: {book_info.get('title', 'N/A')}")
                        print(f"  URL: {book_info.get('url', 'N/A')}")
                        print(f"  è‘—è€…: {book_info.get('author', 'N/A')}")
                        
                        # ã‚¹ã‚³ã‚¢è¨ˆç®—
                        if book_info.get('title'):
                            score = scraper.calculate_similarity_score(test_query, book_info['title'])
                            print(f"  é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢: {score:.3f}")
                    else:
                        print(f"  æƒ…å ±æŠ½å‡ºå¤±æ•—")
            
        except Exception as e:
            print(f"ã‚³ãƒ³ãƒ†ãƒŠæŠ½å‡ºãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")


async def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    Path('logs').mkdir(exist_ok=True)
    
    try:
        # ã‚³ãƒ³ãƒ†ãƒŠæŠ½å‡ºæ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ
        await test_container_extraction()
        
        # é«˜åº¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ãƒ†ã‚¹ãƒˆ
        await test_bookwalker_advanced()
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ãƒ†ã‚¹ãƒˆãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        logger.error(f"ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        print(f"ğŸ’¥ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")


if __name__ == "__main__":
    asyncio.run(main())