#!/usr/bin/env python3
"""
Amazon Kindleã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""
import asyncio
import sys
import json
from pathlib import Path
import logging

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.scraping.amazon_kindle_scraper import AmazonKindleScraper

# ãƒ­ã‚¬ãƒ¼è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


async def test_amazon_scraper():
    """Amazon Kindleã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®ãƒ†ã‚¹ãƒˆ"""
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    test_data_path = project_root / 'config' / 'test_data.json'
    with open(test_data_path, 'r', encoding='utf-8') as f:
        test_data = json.load(f)
    
    test_books = test_data['test_books']
    
    print("=== Amazon Kindleã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ ãƒ†ã‚¹ãƒˆ ===")
    print(f"ãƒ†ã‚¹ãƒˆå¯¾è±¡: {len(test_books)}å†Š")
    print()
    
    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼è¨­å®š
    screenshot_dir = project_root / 'logs' / 'screenshots'
    screenshot_dir.mkdir(parents=True, exist_ok=True)
    
    results = []
    
    async with AmazonKindleScraper(
        headless=False,  # åˆå›ãƒ†ã‚¹ãƒˆã¯ãƒ–ãƒ©ã‚¦ã‚¶è¡¨ç¤º
        timeout=30000,
        screenshot_dir=screenshot_dir
    ) as scraper:
        
        for book in test_books:
            n_code = book['n_code']
            title = book['title']
            
            print(f"ğŸ“– æ¤œç´¢ä¸­: {title} ({n_code})")
            
            try:
                # æ¤œç´¢å®Ÿè¡Œ
                url = await scraper.search_book(title, n_code)
                
                if url:
                    print(f"âœ… æˆåŠŸ: {url}")
                    result = {
                        'n_code': n_code,
                        'title': title,
                        'status': 'success',
                        'url': url,
                        'error': None
                    }
                else:
                    print(f"âŒ å¤±æ•—: URLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                    result = {
                        'n_code': n_code,
                        'title': title,
                        'status': 'not_found',
                        'url': None,
                        'error': 'No URL found'
                    }
                
                results.append(result)
                
            except Exception as e:
                print(f"ğŸ’¥ ã‚¨ãƒ©ãƒ¼: {e}")
                result = {
                    'n_code': n_code,
                    'title': title,
                    'status': 'error',
                    'url': None,
                    'error': str(e)
                }
                results.append(result)
            
            print()
            # æ›¸ç±é–“ã®å¾…æ©Ÿ
            await asyncio.sleep(3)
        
        # çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
        print("=== ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼ ===")
        stats = scraper.get_search_stats()
        for key, value in stats.items():
            print(f"{key}: {value}")
        print()
        
        # çµæœã®è©³ç´°è¡¨ç¤º
        success_count = len([r for r in results if r['status'] == 'success'])
        not_found_count = len([r for r in results if r['status'] == 'not_found'])
        error_count = len([r for r in results if r['status'] == 'error'])
        
        print(f"æˆåŠŸ: {success_count}ä»¶")
        print(f"è¦‹ã¤ã‹ã‚‰ãš: {not_found_count}ä»¶")
        print(f"ã‚¨ãƒ©ãƒ¼: {error_count}ä»¶")
        print(f"æˆåŠŸç‡: {(success_count / len(results)) * 100:.1f}%")
    
    # çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    results_path = project_root / 'logs' / 'amazon_test_results.json'
    results_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump({
            'test_results': results,
            'summary': {
                'total_books': len(results),
                'success_count': success_count,
                'not_found_count': not_found_count,
                'error_count': error_count,
                'success_rate': f"{(success_count / len(results)) * 100:.1f}%"
            },
            'scraper_stats': stats
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nçµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜: {results_path}")
    
    return results


async def test_volume_variants():
    """å·»æ•°ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚¹ãƒˆ"""
    print("=== å·»æ•°ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ ãƒ†ã‚¹ãƒˆ ===")
    
    scraper = AmazonKindleScraper()
    
    test_titles = [
        "ãƒ‘ãƒ©ãƒ¬ã‚¤ãƒ‰ãƒ‡ã‚¤ã‚ºâ‘£",
        "ã‚¨ã‚¢ãƒœãƒ¼ãƒ³ã‚¦ã‚¤ãƒƒãƒâ‘£"
    ]
    
    for title in test_titles:
        print(f"\nğŸ“š ã‚¿ã‚¤ãƒˆãƒ«: {title}")
        variants = scraper.create_volume_variants(title)
        
        for i, variant in enumerate(variants, 1):
            print(f"  {i}. {variant}")


async def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("Amazon Kindleã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ ãƒ†ã‚¹ãƒˆãƒ„ãƒ¼ãƒ«")
    print("1. å·»æ•°ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ")
    print("2. å®Ÿéš›ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ†ã‚¹ãƒˆ")
    print()
    
    choice = input("é¸æŠã—ã¦ãã ã•ã„ (1/2): ").strip()
    
    if choice == "1":
        await test_volume_variants()
    elif choice == "2":
        await test_amazon_scraper()
    else:
        print("ç„¡åŠ¹ãªé¸æŠã§ã™")


if __name__ == "__main__":
    asyncio.run(main())