#!/usr/bin/env python3
"""
ç´€ä¼Šåœ‹å±‹æ›¸åº—ï¼ˆKinoppyï¼‰ã‚µã‚¤ãƒˆæ§‹é€ åˆ†æå™¨
å¾¹åº•çš„æ·±å±¤åˆ†æã«ã‚ˆã‚‹çœŸã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ‰‹æ³•ç¢ºç«‹
"""
import asyncio
import sys
import json
import time
from pathlib import Path
from datetime import datetime
import logging
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, quote
import re

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class KinoppyDeepAnalyzer:
    """ç´€ä¼Šåœ‹å±‹æ›¸åº—ã®æ·±å±¤æ§‹é€ è§£æã‚¯ãƒ©ã‚¹"""
    
    BASE_URL = "https://www.kinokuniya.co.jp"
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
    def analyze_site_structure(self):
        """ã‚µã‚¤ãƒˆæ§‹é€ ã®å¾¹åº•åˆ†æ"""
        print("=== ç´€ä¼Šåœ‹å±‹æ›¸åº—ï¼ˆKinoppyï¼‰æ·±å±¤åˆ†æé–‹å§‹ ===")
        print(f"é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print()
        
        # Phase 1: ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸åˆ†æ
        print("ğŸ” Phase 1: ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸æ§‹é€ åˆ†æ")
        self._analyze_homepage()
        
        # Phase 2: æ¤œç´¢ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆç™ºè¦‹
        print("\nğŸ” Phase 2: æ¤œç´¢ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆç™ºè¦‹")
        self._discover_search_endpoints()
        
        # Phase 3: å®Ÿéš›ã®æ¤œç´¢å®Ÿè¡Œã¨å¿œç­”åˆ†æ
        print("\nğŸ” Phase 3: å®Ÿéš›ã®æ¤œç´¢å®Ÿè¡Œãƒ»å¿œç­”åˆ†æ")
        self._analyze_search_responses()
        
        # Phase 4: JavaScript/Ajaxåˆ†æ
        print("\nğŸ” Phase 4: JavaScript/Ajaxåˆ†æ")
        self._analyze_javascript_ajax()
        
        # Phase 5: ä»£æ›¿ã‚¢ãƒ—ãƒ­ãƒ¼ãƒæ¤œè¨
        print("\nğŸ” Phase 5: ä»£æ›¿ã‚¢ãƒ—ãƒ­ãƒ¼ãƒæ¤œè¨")
        self._explore_alternative_approaches()
        
        print("\n=== ç´€ä¼Šåœ‹å±‹æ›¸åº—æ·±å±¤åˆ†æå®Œäº† ===")
    
    def _analyze_homepage(self):
        """ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã®æ§‹é€ åˆ†æ"""
        try:
            # è¤‡æ•°ã®URLãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦è¡Œ
            urls_to_check = [
                "https://www.kinokuniya.co.jp/",
                "https://www.kinokuniya.co.jp/kinoppystore/",
                "https://www.kinokuniya.co.jp/kinoppystore/search",
                "https://www.kinokuniya.co.jp/f/dsg-08-EK",  # æ—§URL
                "https://kinoppy.jp/",  # åˆ¥ãƒ‰ãƒ¡ã‚¤ãƒ³
                "https://store.kinoppy.jp/",  # ã‚¹ãƒˆã‚¢åˆ¥ãƒ‰ãƒ¡ã‚¤ãƒ³
            ]
            
            for url in urls_to_check:
                print(f"  ğŸŒ ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {url}")
                try:
                    response = self.session.get(url, timeout=10)
                    print(f"    ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {response.status_code}")
                    
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        title = soup.find('title')
                        print(f"    ã‚¿ã‚¤ãƒˆãƒ«: {title.get_text() if title else 'ãªã—'}")
                        
                        # æ¤œç´¢ãƒ•ã‚©ãƒ¼ãƒ æ¤œå‡º
                        search_forms = soup.find_all('form')
                        print(f"    æ¤œç´¢ãƒ•ã‚©ãƒ¼ãƒ æ•°: {len(search_forms)}")
                        
                        for i, form in enumerate(search_forms):
                            action = form.get('action', '')
                            method = form.get('method', 'GET')
                            print(f"      ãƒ•ã‚©ãƒ¼ãƒ {i+1}: {method} {action}")
                            
                            # å…¥åŠ›ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åˆ†æ
                            inputs = form.find_all(['input', 'select'])
                            for inp in inputs:
                                name = inp.get('name', '')
                                input_type = inp.get('type', '')
                                if name and input_type not in ['hidden', 'submit']:
                                    print(f"        å…¥åŠ›: {name} ({input_type})")
                        
                        # ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯
                        nav_links = soup.find_all('a', href=True)
                        ebook_links = [link for link in nav_links if 'kinoppy' in link.get('href', '').lower() or 'ebook' in link.get('href', '').lower()]
                        print(f"    é›»å­æ›¸ç±é–¢é€£ãƒªãƒ³ã‚¯: {len(ebook_links)}")
                        
                        for link in ebook_links[:5]:  # æœ€åˆã®5å€‹ã ã‘è¡¨ç¤º
                            href = link.get('href')
                            text = link.get_text(strip=True)
                            print(f"      - {text}: {href}")
                        
                    elif response.status_code == 404:
                        print(f"    âŒ 404 - URLãŒå­˜åœ¨ã—ã¾ã›ã‚“")
                    elif response.status_code == 302 or response.status_code == 301:
                        print(f"    ğŸ”„ ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆå…ˆ: {response.headers.get('Location', 'ä¸æ˜')}")
                    else:
                        print(f"    âš ï¸ äºˆæœŸã—ãªã„ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {response.status_code}")
                        
                except requests.exceptions.Timeout:
                    print(f"    â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
                except requests.exceptions.ConnectionError:
                    print(f"    ğŸ”Œ æ¥ç¶šã‚¨ãƒ©ãƒ¼")
                except Exception as e:
                    print(f"    ğŸ’¥ ã‚¨ãƒ©ãƒ¼: {str(e)}")
                
                time.sleep(1)  # è² è·è»½æ¸›
                
        except Exception as e:
            logger.error(f"ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
    
    def _discover_search_endpoints(self):
        """æ¤œç´¢ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®ç™ºè¦‹"""
        try:
            # æƒ³å®šã•ã‚Œã‚‹æ¤œç´¢ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
            search_endpoints = [
                "/kinoppystore/search",
                "/search",
                "/f/dsg-08-EK",
                "/f/dsg-08",
                "/dw/search",
                "/book/search",
                "/ebook/search",
                "/api/search",
                "/ajax/search",
            ]
            
            print("  ğŸ“¡ æ¤œç´¢ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆæ¢æŸ»:")
            
            for endpoint in search_endpoints:
                full_url = self.BASE_URL + endpoint
                print(f"    ãƒ†ã‚¹ãƒˆä¸­: {endpoint}")
                
                try:
                    # GET ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
                    response = self.session.get(full_url, timeout=5)
                    print(f"      GET {response.status_code}")
                    
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        
                        # æ¤œç´¢ã«é–¢é€£ã™ã‚‹è¦ç´ ã‚’æ¢ã™
                        search_indicators = [
                            soup.find_all('input', {'type': 'search'}),
                            soup.find_all('input', {'name': re.compile(r'(search|query|q|keyword)', re.I)}),
                            soup.find_all('form', {'action': re.compile(r'search', re.I)}),
                        ]
                        
                        total_indicators = sum(len(indicators) for indicators in search_indicators)
                        if total_indicators > 0:
                            print(f"      âœ… æ¤œç´¢è¦ç´ ç™ºè¦‹: {total_indicators}å€‹")
                            
                            # è©³ç´°åˆ†æ
                            forms = soup.find_all('form')
                            for form in forms:
                                action = form.get('action', '')
                                if action:
                                    print(f"        ãƒ•ã‚©ãƒ¼ãƒ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {action}")
                                    
                        else:
                            print(f"      âšª é€šå¸¸ãƒšãƒ¼ã‚¸ï¼ˆæ¤œç´¢è¦ç´ ãªã—ï¼‰")
                    
                    elif response.status_code == 404:
                        print(f"      âŒ 404")
                    else:
                        print(f"      âš ï¸ {response.status_code}")
                        
                except Exception as e:
                    print(f"      ğŸ’¥ ã‚¨ãƒ©ãƒ¼: {str(e)[:50]}")
                    
                time.sleep(0.5)
                
        except Exception as e:
            logger.error(f"ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆæ¢æŸ»ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _analyze_search_responses(self):
        """å®Ÿéš›ã®æ¤œç´¢å®Ÿè¡Œã¨å¿œç­”åˆ†æ"""
        try:
            test_queries = [
                "ã‚½ãƒ¼ãƒ‰ã‚¢ãƒ¼ãƒˆãƒ»ã‚ªãƒ³ãƒ©ã‚¤ãƒ³",
                "SAO",
                "ã‚¯ã‚½ã‚²ãƒ¼æ‚ªå½¹ä»¤å¬¢",
                "9784048671811",  # ISBN
            ]
            
            print("  ğŸ” å®Ÿéš›ã®æ¤œç´¢å®Ÿè¡Œãƒ†ã‚¹ãƒˆ:")
            
            # æƒ³å®šã•ã‚Œã‚‹æ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³
            search_patterns = [
                {"q": "{query}"},
                {"search": "{query}"},
                {"keyword": "{query}"},
                {"query": "{query}"},
                {"searchKeyword": "{query}"},
                {"searchterm": "{query}"},
                {"q": "{query}", "searchtype": "BOOK"},
                {"q": "{query}", "category": "ebook"},
                {"keyword": "{query}", "type": "digital"},
            ]
            
            search_urls = [
                "https://www.kinokuniya.co.jp/kinoppystore/search",
                "https://www.kinokuniya.co.jp/search",
                "https://www.kinokuniya.co.jp/dw/search",
            ]
            
            for url in search_urls:
                print(f"    ğŸŒ URL: {url}")
                
                for query in test_queries[:2]:  # æœ€åˆã®2ã‚¯ã‚¨ãƒªã®ã¿
                    print(f"      ğŸ“š ã‚¯ã‚¨ãƒª: {query}")
                    
                    for pattern in search_patterns[:3]:  # æœ€åˆã®3ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿
                        try:
                            params = {k: v.format(query=query) for k, v in pattern.items()}
                            print(f"        ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {params}")
                            
                            response = self.session.get(url, params=params, timeout=8)
                            print(f"        ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response.status_code} ({len(response.text)} bytes)")
                            
                            if response.status_code == 200:
                                soup = BeautifulSoup(response.text, 'html.parser')
                                
                                # æ¤œç´¢çµæœã®æŒ‡æ¨™ã‚’æ¢ã™
                                result_indicators = [
                                    len(soup.find_all('a', href=lambda x: x and '/dsg-' in x)),
                                    len(soup.find_all('div', class_=re.compile(r'(book|item|product)', re.I))),
                                    len(soup.find_all(['h2', 'h3', 'h4'], string=re.compile(query[:5], re.I))),
                                ]
                                
                                total_results = sum(result_indicators)
                                print(f"        çµæœè¦ç´ : {total_results}å€‹")
                                
                                if total_results > 0:
                                    print(f"        âœ… æ¤œç´¢çµæœã‚‰ã—ãã‚‚ã®ã‚’ç™ºè¦‹ï¼")
                                    
                                    # å®Ÿéš›ã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™
                                    book_links = soup.find_all('a', href=re.compile(r'/dsg-'))
                                    if book_links:
                                        for link in book_links[:3]:
                                            href = link.get('href')
                                            text = link.get_text(strip=True)
                                            print(f"          ğŸ“– {text[:30]}... -> {href}")
                                
                            time.sleep(1)
                            
                        except Exception as e:
                            print(f"        ğŸ’¥ ã‚¨ãƒ©ãƒ¼: {str(e)[:50]}")
                            
                    print()
                    
        except Exception as e:
            logger.error(f"æ¤œç´¢å¿œç­”åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
    
    def _analyze_javascript_ajax(self):
        """JavaScript/Ajaxåˆ†æ"""
        try:
            print("  ğŸ”§ JavaScript/Ajaxåˆ†æ:")
            
            # ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã®JavaScriptãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡º
            try:
                response = self.session.get("https://www.kinokuniya.co.jp/kinoppystore/", timeout=10)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # Script ã‚¿ã‚°æ¤œå‡º
                    scripts = soup.find_all('script', src=True)
                    print(f"    å¤–éƒ¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆæ•°: {len(scripts)}")
                    
                    # Ajaxé–¢é€£ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã™
                    ajax_patterns = [
                        r'ajax',
                        r'xhr',
                        r'fetch',
                        r'search',
                        r'api',
                    ]
                    
                    inline_scripts = soup.find_all('script', src=False)
                    print(f"    ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆæ•°: {len(inline_scripts)}")
                    
                    ajax_hints = []
                    for script in inline_scripts:
                        script_content = script.get_text()
                        for pattern in ajax_patterns:
                            if re.search(pattern, script_content, re.I):
                                ajax_hints.append(pattern)
                    
                    if ajax_hints:
                        print(f"    Ajaxé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç™ºè¦‹: {', '.join(set(ajax_hints))}")
                    else:
                        print(f"    Ajaxé–¢é€£è¦ç´ ãªã—ï¼ˆé™çš„ã‚µã‚¤ãƒˆã®å¯èƒ½æ€§ï¼‰")
                    
                    # Formã®submitå…ˆåˆ†æ
                    forms = soup.find_all('form')
                    for form in forms:
                        action = form.get('action', '')
                        if 'search' in action.lower():
                            print(f"    æ¤œç´¢ãƒ•ã‚©ãƒ¼ãƒ ç™ºè¦‹: {action}")
                            
                            # ãƒ•ã‚©ãƒ¼ãƒ å†…ã®JavaScriptåˆ†æ
                            onsubmit = form.get('onsubmit', '')
                            if onsubmit:
                                print(f"      onsubmit: {onsubmit[:100]}...")
            
            except Exception as e:
                print(f"    ğŸ’¥ JavaScriptåˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
                
        except Exception as e:
            logger.error(f"JavaScript/Ajaxåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
    
    def _explore_alternative_approaches(self):
        """ä»£æ›¿ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®æ¤œè¨"""
        try:
            print("  ğŸ’¡ ä»£æ›¿ã‚¢ãƒ—ãƒ­ãƒ¼ãƒæ¤œè¨:")
            
            approaches = [
                {
                    'name': 'Selenium WebDriver',
                    'pros': 'JavaScriptå®Ÿè¡Œã€å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å¯¾å¿œ',
                    'cons': 'é‡ã„ã€æ¤œå‡ºã•ã‚Œã‚„ã™ã„',
                    'feasibility': 'é«˜'
                },
                {
                    'name': 'Playwright',
                    'pros': 'é«˜é€Ÿã€æœ€æ–°ãƒ–ãƒ©ã‚¦ã‚¶æŠ€è¡“ã€ã‚¹ãƒ†ãƒ«ã‚¹æ€§',
                    'cons': 'å®Ÿè£…ã‚³ã‚¹ãƒˆ',
                    'feasibility': 'é«˜'
                },
                {
                    'name': 'APIé€†è§£æ',
                    'pros': 'æœ€é«˜é€Ÿã€ç¢ºå®Ÿ',
                    'cons': 'æŠ€è¡“çš„é›£æ˜“åº¦é«˜ã€ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹æ€§',
                    'feasibility': 'ä¸­'
                },
                {
                    'name': 'Chrome DevTools Protocol',
                    'pros': 'æŸ”è»Ÿæ€§ã€è©³ç´°åˆ¶å¾¡',
                    'cons': 'è¤‡é›‘ã€å®Ÿè£…ã‚³ã‚¹ãƒˆé«˜',
                    'feasibility': 'ä¸­'
                },
                {
                    'name': 'requests-html',
                    'pros': 'JavaScriptå¯¾å¿œã€ã‚·ãƒ³ãƒ—ãƒ«',
                    'cons': 'PyQt4ä¾å­˜ã€å®‰å®šæ€§',
                    'feasibility': 'ä¸­'
                },
            ]
            
            print("    ğŸ› ï¸ æŠ€è¡“é¸æŠè‚¢:")
            for approach in approaches:
                print(f"      {approach['name']}:")
                print(f"        åˆ©ç‚¹: {approach['pros']}")
                print(f"        æ¬ ç‚¹: {approach['cons']}")
                print(f"        å®Ÿè£…å¯èƒ½æ€§: {approach['feasibility']}")
                print()
            
            # æ¨å¥¨æˆ¦ç•¥
            print("    ğŸ¯ æ¨å¥¨æˆ¦ç•¥:")
            print("      1. Playwrightå®Ÿè£…ï¼ˆç¬¬ä¸€é¸æŠï¼‰")
            print("        - ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ–ãƒ©ã‚¦ã‚¶ã§ã®ãƒ•ãƒ«æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ")
            print("        - JavaScriptå®Ÿè¡Œç’°å¢ƒã§ã®å‹•çš„æ¤œç´¢")
            print("        - ãƒ¦ãƒ¼ã‚¶ãƒ¼æ“ä½œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
            print()
            print("      2. Selenium WebDriverï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰")
            print("        - Chromeãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã§ã®ç¢ºå®Ÿãªå‹•ä½œ")
            print("        - ãƒ‡ãƒãƒƒã‚°æ©Ÿèƒ½å……å®Ÿ")
            print()
            print("      3. APIæ¢æŸ»ãƒ»ãƒªãƒãƒ¼ã‚¹ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆä¸Šç´šï¼‰")
            print("        - ãƒ–ãƒ©ã‚¦ã‚¶DevToolsã§ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æ")
            print("        - å®Ÿéš›ã®APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆç™ºè¦‹")
            print("        - ç›´æ¥APIã‚¢ã‚¯ã‚»ã‚¹å®Ÿè£…")
            
        except Exception as e:
            logger.error(f"ä»£æ›¿ã‚¢ãƒ—ãƒ­ãƒ¼ãƒæ¤œè¨ã‚¨ãƒ©ãƒ¼: {e}")


async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    analyzer = KinoppyDeepAnalyzer()
    
    try:
        analyzer.analyze_site_structure()
        
        print("\n" + "="*60)
        print("ğŸ¯ åˆ†æçµæœã‚µãƒãƒªãƒ¼")
        print("="*60)
        print("1. ç¾åœ¨ã®requests + BeautifulSoupã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯é™ç•Œ")
        print("2. JavaScriptãƒ™ãƒ¼ã‚¹ã®å‹•çš„ã‚µã‚¤ãƒˆã®å¯èƒ½æ€§ãŒé«˜ã„")
        print("3. Playwrightå®Ÿè£…ãŒæœ€é©è§£ã¨åˆ¤æ–­")
        print("4. æ¬¡ãƒ•ã‚§ãƒ¼ã‚º: Playwrightç‰ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼å®Ÿè£…")
        print("\nğŸ’¡ å³åº§ã«Playwrightç‰ˆå®Ÿè£…ã«ç§»è¡Œã™ã‚‹ã“ã¨ã‚’æ¨å¥¨")
        
    except Exception as e:
        logger.error(f"åˆ†æå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        print(f"ğŸ’¥ åˆ†æã‚¨ãƒ©ãƒ¼: {e}")


if __name__ == "__main__":
    asyncio.run(main())