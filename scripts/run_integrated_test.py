#!/usr/bin/env python3
"""
çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å…¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®çµ±åˆãƒ†ã‚¹ãƒˆã¨Google Sheetsé€£æºãƒ†ã‚¹ãƒˆ
"""
import asyncio
import sys
import json
import uuid
from pathlib import Path
from datetime import datetime
import logging

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.scraping.amazon_kindle_scraper import AmazonKindleScraper
from src.scraping.rakuten_kobo_scraper import RakutenKoboScraper
from src.scraping.google_play_books_scraper import GooglePlayBooksScraper
from src.scraping.bookwalker_scraper import BookWalkerScraper
from src.scraping.google_sheets_client_consolidated import GoogleSheetsClient, SalesLinkUpdate, SalesChannel
from src.scraping.result_exporter import ResultExporter, BatchResult, ScrapingResult, ResultStatus, ExportFormat
from src.scraping.error_handler import ErrorHandler, RetryConfig

# ãƒ­ã‚¬ãƒ¼è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class IntegratedTestRunner:
    """çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.batch_id = f"test_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.results_dir = project_root / 'logs' / 'test_results'
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–
        self.error_handler = ErrorHandler()
        self.result_exporter = ResultExporter(self.results_dir)
        self.batch_result = BatchResult(
            batch_id=self.batch_id,
            started_at=datetime.now()
        )
        
        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®è¨­å®š
        screenshot_dir = self.results_dir / 'screenshots'
        screenshot_dir.mkdir(exist_ok=True)
        
        self.scrapers = {
            'Amazon Kindle': AmazonKindleScraper(
                headless=True,
                timeout=20000,
                screenshot_dir=screenshot_dir
            ),
            'æ¥½å¤©Kobo': RakutenKoboScraper(
                headless=True,
                timeout=20000,  
                screenshot_dir=screenshot_dir
            ),
            'Google Play Books': GooglePlayBooksScraper(
                headless=True,
                timeout=20000,
                screenshot_dir=screenshot_dir
            ),
            'BOOKâ˜†WALKER': BookWalkerScraper(
                headless=True,
                timeout=30000,  # BOOKâ˜†WALKERã¯å°‘ã—é•·ã‚ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                screenshot_dir=screenshot_dir
            )
        }
    
    async def run_full_test(self):
        """ãƒ•ãƒ«ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"""
        print(f"=== çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹ (ID: {self.batch_id}) ===")
        print(f"çµæœä¿å­˜å…ˆ: {self.results_dir}")
        print()
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
        test_books = await self._load_test_data()
        print(f"ãƒ†ã‚¹ãƒˆå¯¾è±¡æ›¸ç±: {len(test_books)}å†Š")
        
        # 1. ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼å€‹åˆ¥ãƒ†ã‚¹ãƒˆ
        print("\nğŸ“– Phase 1: å€‹åˆ¥ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ãƒ†ã‚¹ãƒˆ")
        await self._test_individual_scrapers(test_books)
        
        # 2. Google Sheetsé€£æºãƒ†ã‚¹ãƒˆ
        print("\nğŸ“Š Phase 2: Google Sheetsé€£æºãƒ†ã‚¹ãƒˆ")
        await self._test_google_sheets_integration()
        
        # 3. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ
        print("\nâš¡ Phase 3: ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ")
        await self._test_error_handling()
        
        # 4. çµæœå‡ºåŠ›ãƒ†ã‚¹ãƒˆ
        print("\nğŸ“¤ Phase 4: çµæœå‡ºåŠ›ãƒ†ã‚¹ãƒˆ")
        await self._test_result_export()
        
        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ
        self.batch_result.finalize()
        await self._generate_final_report()
        
        print(f"\nâœ… çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†!")
        print(f"æˆåŠŸç‡: {self.batch_result.success_rate:.1f}%")
        print(f"å‡¦ç†æ™‚é–“: {self.batch_result.processing_time:.1f}ç§’")
    
    async def _load_test_data(self):
        """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
        test_data_path = project_root / 'config' / 'test_data.json'
        with open(test_data_path, 'r', encoding='utf-8') as f:
            test_data = json.load(f)
        return test_data['test_books']
    
    async def _test_individual_scrapers(self, test_books):
        """å€‹åˆ¥ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®ãƒ†ã‚¹ãƒˆ"""
        for scraper_name, scraper in self.scrapers.items():
            print(f"\nğŸ” {scraper_name} ãƒ†ã‚¹ãƒˆé–‹å§‹")
            
            async with scraper:
                for book in test_books:
                    n_code = book['n_code']
                    title = book['title']
                    
                    print(f"  ğŸ“š {title} ({n_code})")
                    start_time = datetime.now()
                    
                    try:
                        url = await scraper.search_book(title, n_code)
                        end_time = datetime.now()
                        processing_time = (end_time - start_time).total_seconds()
                        
                        if url:
                            result = ScrapingResult(
                                n_code=n_code,
                                title=title,
                                site_name=scraper_name,
                                status=ResultStatus.SUCCESS,
                                url=url,
                                processing_time=processing_time
                            )
                            print(f"    âœ… æˆåŠŸ: {url}")
                        else:
                            result = ScrapingResult(
                                n_code=n_code,
                                title=title,
                                site_name=scraper_name,
                                status=ResultStatus.NOT_FOUND,
                                processing_time=processing_time,
                                error_message="URL not found"
                            )
                            print(f"    âŒ è¦‹ã¤ã‹ã‚‰ãš")
                        
                        self.batch_result.add_result(result)
                        
                    except Exception as e:
                        end_time = datetime.now()
                        processing_time = (end_time - start_time).total_seconds()
                        
                        result = ScrapingResult(
                            n_code=n_code,
                            title=title,
                            site_name=scraper_name,
                            status=ResultStatus.ERROR,
                            processing_time=processing_time,
                            error_message=str(e)
                        )
                        self.batch_result.add_result(result)
                        print(f"    ğŸ’¥ ã‚¨ãƒ©ãƒ¼: {str(e)[:50]}...")
                    
                    # æ›¸ç±é–“ã®å¾…æ©Ÿ
                    await asyncio.sleep(2)
                
                # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼çµ±è¨ˆã®è¡¨ç¤º
                stats = scraper.get_stats()
                print(f"  ğŸ“Š çµ±è¨ˆ: æˆåŠŸç‡ {stats.get('success_rate', 'N/A')}")
    
    async def _test_google_sheets_integration(self):
        """Google Sheetsé€£æºãƒ†ã‚¹ãƒˆ"""
        try:
            credentials_path = 'config/credentials/google-sheets-key.json'
            spreadsheet_id = '1hMhDw3HJsUMetceMvy2lszDeLCxarNavn_2E6Inf84M'
            
            client = GoogleSheetsClient(credentials_path, spreadsheet_id)
            
            # 1. æ›¸ç±ãƒ‡ãƒ¼ã‚¿èª­ã¿å–ã‚Šãƒ†ã‚¹ãƒˆ
            print("  ğŸ“– æ›¸ç±ãƒ‡ãƒ¼ã‚¿èª­ã¿å–ã‚Šãƒ†ã‚¹ãƒˆ")
            books = client.read_all_books()
            print(f"    âœ… {len(books)}å†Šã®æ›¸ç±ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿å–ã‚Š")
            
            # 2. çµ±è¨ˆæƒ…å ±ãƒ†ã‚¹ãƒˆ
            print("  ğŸ“Š çµ±è¨ˆæƒ…å ±ãƒ†ã‚¹ãƒˆ")
            stats = client.get_summary_stats()
            print(f"    âœ… åé›†ç‡: {stats.get('collection_rate', 0):.1f}%")
            
            # 3. ãƒ†ã‚¹ãƒˆç”¨ãƒªãƒ³ã‚¯æ›´æ–°ï¼ˆå®Ÿéš›ã®ã‚·ãƒ¼ãƒˆã¯æ›´æ–°ã—ãªã„ï¼‰
            print("  ğŸ”— ãƒªãƒ³ã‚¯æ›´æ–°ãƒ†ã‚¹ãƒˆï¼ˆæ¨¡æ“¬ï¼‰")
            test_updates = []
            for result in self.batch_result.results:
                if result.status == ResultStatus.SUCCESS and result.url:
                    # ãƒãƒ£ãƒ³ãƒãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆã‚µã‚¤ãƒˆåâ†’ãƒãƒ£ãƒ³ãƒãƒ«ï¼‰
                    channel_map = {
                        'Amazon Kindle': SalesChannel.KINDLE,
                        'æ¥½å¤©Kobo': SalesChannel.KOBO,
                        'Google Play Books': SalesChannel.GOOGLE,
                        'BOOKâ˜†WALKER': SalesChannel.BOOKWALKER
                    }
                    
                    if result.site_name in channel_map:
                        update = SalesLinkUpdate(
                            n_code=result.n_code,
                            channel=channel_map[result.site_name],
                            url=result.url,
                            scraped_at=result.scraped_at.isoformat()
                        )
                        test_updates.append(update)
            
            print(f"    ğŸ“ {len(test_updates)}ä»¶ã®æ›´æ–°ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™")
            print("    âš ï¸  å®Ÿéš›ã®ã‚·ãƒ¼ãƒˆæ›´æ–°ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼‰")
            
        except Exception as e:
            print(f"    ğŸ’¥ Google Sheetsé€£æºã‚¨ãƒ©ãƒ¼: {e}")
    
    async def _test_error_handling(self):
        """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ"""
        print("  âš¡ ãƒªãƒˆãƒ©ã‚¤æ©Ÿæ§‹ãƒ†ã‚¹ãƒˆ")
        
        # æ¨¡æ“¬ã‚¨ãƒ©ãƒ¼é–¢æ•°
        async def failing_function(attempt_count=[0]):
            attempt_count[0] += 1
            if attempt_count[0] < 3:
                raise Exception(f"Test error (attempt {attempt_count[0]})")
            return "Success after retries"
        
        try:
            retry_config = RetryConfig(max_attempts=5, base_delay=0.1)
            result = await self.error_handler.execute_with_retry(
                failing_function,
                retry_config=retry_config
            )
            print(f"    âœ… ãƒªãƒˆãƒ©ã‚¤æˆåŠŸ: {result}")
        except Exception as e:
            print(f"    âŒ ãƒªãƒˆãƒ©ã‚¤å¤±æ•—: {e}")
        
        # ã‚¨ãƒ©ãƒ¼çµ±è¨ˆã®è¡¨ç¤º
        health_report = self.error_handler.get_health_report()
        print(f"    ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§: {health_report.get('overall_health', 'unknown')}")
    
    async def _test_result_export(self):
        """çµæœå‡ºåŠ›ãƒ†ã‚¹ãƒˆ"""
        formats = [ExportFormat.JSON, ExportFormat.CSV]
        
        for format in formats:
            try:
                output_path = self.result_exporter.export_batch_result(
                    self.batch_result,
                    format=format
                )
                print(f"    âœ… {format.value.upper()}å‡ºåŠ›: {output_path.name}")
            except Exception as e:
                print(f"    âŒ {format.value.upper()}å‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {e}")
    
    async def _generate_final_report(self):
        """æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        # è©³ç´°çµæœã®å‡ºåŠ›
        json_path = self.result_exporter.export_batch_result(
            self.batch_result,
            format=ExportFormat.JSON,
            filename=f"integrated_test_results_{self.batch_id}.json"
        )
        
        csv_path = self.result_exporter.export_batch_result(
            self.batch_result,
            format=ExportFormat.CSV,
            filename=f"integrated_test_results_{self.batch_id}.csv"
        )
        
        # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ
        summary_path = self.result_exporter.export_summary_report(
            [self.batch_result],
            filename=f"integrated_test_summary_{self.batch_id}.json"
        )
        
        print(f"\nğŸ“‹ æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ:")
        print(f"  è©³ç´°çµæœ (JSON): {json_path}")
        print(f"  è©³ç´°çµæœ (CSV): {csv_path}")
        print(f"  ã‚µãƒãƒªãƒ¼: {summary_path}")
        
        # çµ±è¨ˆã‚µãƒãƒªãƒ¼
        print(f"\nğŸ“Š ãƒ†ã‚¹ãƒˆçµ±è¨ˆ:")
        print(f"  ç·å®Ÿè¡Œæ•°: {self.batch_result.total_books}")
        print(f"  æˆåŠŸ: {self.batch_result.successful_results}")
        print(f"  å¤±æ•—: {self.batch_result.failed_results}")
        print(f"  è¦‹ã¤ã‹ã‚‰ãš: {self.batch_result.skipped_results}")
        print(f"  æˆåŠŸç‡: {self.batch_result.success_rate:.1f}%")
        print(f"  ç·å‡¦ç†æ™‚é–“: {self.batch_result.processing_time:.1f}ç§’")
        
        # ã‚µã‚¤ãƒˆåˆ¥çµ±è¨ˆ
        site_stats = {}
        for result in self.batch_result.results:
            site = result.site_name
            if site not in site_stats:
                site_stats[site] = {'total': 0, 'success': 0}
            site_stats[site]['total'] += 1
            if result.status == ResultStatus.SUCCESS:
                site_stats[site]['success'] += 1
        
        print(f"\nğŸŒ ã‚µã‚¤ãƒˆåˆ¥çµ±è¨ˆ:")
        for site, stats in site_stats.items():
            success_rate = (stats['success'] / stats['total'] * 100) if stats['total'] > 0 else 0
            print(f"  {site}: {stats['success']}/{stats['total']} ({success_rate:.1f}%)")


async def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    test_runner = IntegratedTestRunner()
    await test_runner.run_full_test()


if __name__ == "__main__":
    asyncio.run(main())