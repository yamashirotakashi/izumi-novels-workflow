#!/usr/bin/env python3
"""
BOOKâ˜†WALKER ã‚µã‚¤ãƒˆæ§‹é€ è§£æãƒ„ãƒ¼ãƒ«
å®Ÿéš›ã®HTMLæ§‹é€ ã€APIå‘¼ã³å‡ºã—ã€å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å¾¹åº•èª¿æŸ»
"""
import asyncio
import requests
import json
import re
import sys
from pathlib import Path
from datetime import datetime
from urllib.parse import quote, urljoin
from bs4 import BeautifulSoup
import logging

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ãƒ­ã‚°è¨­å®š  
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class BookWalkerAnalyzer:
    """BOOKâ˜†WALKER ã‚µã‚¤ãƒˆæ§‹é€ è§£æã‚¯ãƒ©ã‚¹"""
    
    BASE_URL = "https://bookwalker.jp"
    SEARCH_URL = "https://bookwalker.jp/search/"
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        })
        
        # çµæœä¿å­˜
        self.analysis_results = {
            'timestamp': datetime.now().isoformat(),
            'tests': []
        }
    
    def analyze_search_page_structure(self):
        """æ¤œç´¢ãƒšãƒ¼ã‚¸ã®åŸºæœ¬æ§‹é€ ã‚’è§£æ"""
        print("=== BOOKâ˜†WALKER æ¤œç´¢ãƒšãƒ¼ã‚¸æ§‹é€ è§£æ ===")
        
        # 1. åŸºæœ¬æ¤œç´¢ãƒšãƒ¼ã‚¸
        print("1. åŸºæœ¬æ¤œç´¢ãƒšãƒ¼ã‚¸ã®å–å¾—...")
        try:
            response = self.session.get(self.SEARCH_URL, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # åŸºæœ¬æƒ…å ±
            print(f"   ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {response.status_code}")
            print(f"   ã‚¿ã‚¤ãƒˆãƒ«: {soup.title.get_text(strip=True) if soup.title else 'N/A'}")
            print(f"   HTMLé•·: {len(response.text):,} æ–‡å­—")
            
            # ãƒ•ã‚©ãƒ¼ãƒ è¦ç´ ã®èª¿æŸ»
            print("\n2. æ¤œç´¢ãƒ•ã‚©ãƒ¼ãƒ è§£æ...")
            forms = soup.find_all('form')
            for i, form in enumerate(forms):
                print(f"   ãƒ•ã‚©ãƒ¼ãƒ  {i+1}:")
                print(f"     action: {form.get('action', 'N/A')}")
                print(f"     method: {form.get('method', 'GET')}")
                
                inputs = form.find_all(['input', 'select'])
                for inp in inputs:
                    print(f"     - {inp.name}: {inp.get('name', 'N/A')} (type: {inp.get('type', 'N/A')})")
            
            # JavaScriptåˆ†æ
            print("\n3. JavaScript/å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„èª¿æŸ»...")
            scripts = soup.find_all('script')
            api_patterns = []
            
            for script in scripts:
                if script.string:
                    # API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®æ¤œç´¢
                    api_matches = re.findall(r'["\']([^"\']*(?:api|search|ajax)[^"\']*)["\']', script.string)
                    api_patterns.extend(api_matches)
                    
                    # æ¤œç´¢é–¢é€£ã®é–¢æ•°
                    if 'search' in script.string.lower():
                        print(f"     æ¤œç´¢é–¢é€£JSç™ºè¦‹: {len(script.string)} æ–‡å­—")
            
            if api_patterns:
                print(f"   ç™ºè¦‹ã•ã‚ŒãŸAPIå€™è£œ: {len(set(api_patterns))}ä»¶")
                for pattern in set(api_patterns)[:5]:  # ä¸Šä½5ã¤
                    print(f"     - {pattern}")
            
            self.analysis_results['tests'].append({
                'test': 'basic_page_structure',
                'success': True,
                'data': {
                    'status_code': response.status_code,
                    'forms_count': len(forms),
                    'scripts_count': len(scripts),
                    'api_patterns': list(set(api_patterns))
                }
            })
            
        except Exception as e:
            print(f"   ã‚¨ãƒ©ãƒ¼: {e}")
            self.analysis_results['tests'].append({
                'test': 'basic_page_structure',
                'success': False,
                'error': str(e)
            })
    
    def test_search_queries(self):
        """æ§˜ã€…ãªæ¤œç´¢ã‚¯ã‚¨ãƒªã®ãƒ†ã‚¹ãƒˆ"""
        print("\n=== æ¤œç´¢ã‚¯ã‚¨ãƒªãƒ†ã‚¹ãƒˆ ===")
        
        test_queries = [
            "ã‚½ãƒ¼ãƒ‰ã‚¢ãƒ¼ãƒˆãƒ»ã‚ªãƒ³ãƒ©ã‚¤ãƒ³",
            "SAO",
            "è»¢ç”Ÿã—ãŸã‚‰ã‚¹ãƒ©ã‚¤ãƒ ã ã£ãŸä»¶",
            "ãƒªã‚¼ãƒ­",
            "ã“ã®ç´ æ™´ã‚‰ã—ã„ä¸–ç•Œã«ç¥ç¦ã‚’",
            "ãƒ‘ãƒ©ãƒ¬ã‚¤ãƒ‰ãƒ‡ã‚¤ã‚º"
        ]
        
        for i, query in enumerate(test_queries, 1):
            print(f"{i}. ã‚¯ã‚¨ãƒªãƒ†ã‚¹ãƒˆ: '{query}'")
            
            try:
                # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒ†ã‚¹ãƒˆ
                param_variations = [
                    {'word': query},
                    {'q': query},
                    {'keyword': query},
                    {'search': query},
                    {'word': query, 'order': 'new'},
                    {'word': query, 'category': 'ebook'}
                ]
                
                for j, params in enumerate(param_variations):
                    response = self.session.get(self.SEARCH_URL, params=params, timeout=10)
                    
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        
                        # æ¤œç´¢çµæœã®è§£æ
                        result_analysis = self.analyze_search_results(soup, query)
                        
                        if result_analysis['found_results']:
                            print(f"   âœ… ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿{j+1} æˆåŠŸ: {params}")
                            print(f"      çµæœæ•°: {result_analysis['result_count']}")
                            print(f"      ã‚»ãƒ¬ã‚¯ã‚¿: {result_analysis['successful_selectors']}")
                            
                            self.analysis_results['tests'].append({
                                'test': f'search_query_{i}',
                                'query': query,
                                'params': params,
                                'success': True,
                                'data': result_analysis
                            })
                            break
                        else:
                            print(f"   âŒ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿{j+1} çµæœãªã—: {params}")
                    else:
                        print(f"   âŒ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿{j+1} HTTPã‚¨ãƒ©ãƒ¼: {response.status_code}")
                
                # é–“éš”ã‚’ç©ºã‘ã‚‹
                if i < len(test_queries):
                    print("   å¾…æ©Ÿä¸­...")
                    import time
                    time.sleep(2)
                    
            except Exception as e:
                print(f"   ğŸ’¥ ã‚¨ãƒ©ãƒ¼: {e}")
                self.analysis_results['tests'].append({
                    'test': f'search_query_{i}',
                    'query': query,
                    'success': False,
                    'error': str(e)
                })
    
    def analyze_search_results(self, soup, query):
        """æ¤œç´¢çµæœãƒšãƒ¼ã‚¸ã®è©³ç´°è§£æ"""
        result_selectors = [
            # ä¸€èˆ¬çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³
            '.search-result-item',
            '.search-item',
            '.book-item',
            '.product-item',
            '.result-item',
            
            # BOOKâ˜†WALKERç‰¹æœ‰ãƒ‘ã‚¿ãƒ¼ãƒ³
            '.c-card-book-list .m-card-book',
            '.m-card-book',
            '.c-card-book',
            '.book-card',
            '.product-card',
            
            # ã‚ˆã‚Šå…·ä½“çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³
            'div[data-book-id]',
            'div[data-product-id]',
            'article',
            '.tile',
            '.grid-item',
            
            # ãƒªãƒ³ã‚¯ãƒ™ãƒ¼ã‚¹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
            'a[href*="/de"]',  # BOOKâ˜†WALKERã®æ›¸ç±URLãƒ‘ã‚¿ãƒ¼ãƒ³
            'a[href*="/series"]',
            'a[href*="/book"]'
        ]
        
        analysis = {
            'found_results': False,
            'result_count': 0,
            'successful_selectors': [],
            'all_links': [],
            'title_patterns': []
        }
        
        # å„ã‚»ãƒ¬ã‚¯ã‚¿ã‚’ãƒ†ã‚¹ãƒˆ
        for selector in result_selectors:
            try:
                elements = soup.select(selector)
                if elements:
                    analysis['successful_selectors'].append({
                        'selector': selector,
                        'count': len(elements)
                    })
                    
                    analysis['found_results'] = True
                    analysis['result_count'] = max(analysis['result_count'], len(elements))
                    
                    # å„è¦ç´ ã‹ã‚‰è©³ç´°æƒ…å ±ã‚’æŠ½å‡º
                    for element in elements[:3]:  # ä¸Šä½3ã¤ã‚’è©³ç´°åˆ†æ
                        link_info = self.extract_element_info(element)
                        if link_info:
                            analysis['all_links'].append(link_info)
                            
            except Exception as e:
                print(f"     ã‚»ãƒ¬ã‚¯ã‚¿ã‚¨ãƒ©ãƒ¼ {selector}: {e}")
        
        # ã‚¿ã‚¤ãƒˆãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æŠ½å‡º
        all_text_elements = soup.find_all(text=True)
        for text in all_text_elements:
            if query.lower() in text.lower() and len(text.strip()) > 5:
                analysis['title_patterns'].append(text.strip()[:100])
        
        return analysis
    
    def extract_element_info(self, element):
        """è¦ç´ ã‹ã‚‰è©³ç´°æƒ…å ±ã‚’æŠ½å‡º"""
        info = {
            'tag': element.name,
            'classes': element.get('class', []),
            'text': element.get_text(strip=True)[:200],
            'links': []
        }
        
        # ãƒªãƒ³ã‚¯ã®æŠ½å‡º
        links = element.find_all('a', href=True)
        for link in links:
            href = link.get('href')
            if href:
                # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                if href.startswith('/'):
                    href = urljoin(self.BASE_URL, href)
                
                info['links'].append({
                    'href': href,
                    'text': link.get_text(strip=True)[:100]
                })
        
        return info if (info['text'] or info['links']) else None
    
    def test_direct_api_calls(self):
        """ç›´æ¥APIå‘¼ã³å‡ºã—ã®ãƒ†ã‚¹ãƒˆ"""
        print("\n=== ç›´æ¥APIå‘¼ã³å‡ºã—ãƒ†ã‚¹ãƒˆ ===")
        
        # æ¨æ¸¬ã•ã‚Œã‚‹APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
        api_endpoints = [
            "/api/search",
            "/api/books/search", 
            "/search/api",
            "/ajax/search",
            "/v1/search",
            "/services/search",
        ]
        
        test_params = {
            'q': 'ã‚½ãƒ¼ãƒ‰ã‚¢ãƒ¼ãƒˆãƒ»ã‚ªãƒ³ãƒ©ã‚¤ãƒ³',
            'word': 'ã‚½ãƒ¼ãƒ‰ã‚¢ãƒ¼ãƒˆãƒ»ã‚ªãƒ³ãƒ©ã‚¤ãƒ³',
            'keyword': 'ã‚½ãƒ¼ãƒ‰ã‚¢ãƒ¼ãƒˆãƒ»ã‚ªãƒ³ãƒ©ã‚¤ãƒ³'
        }
        
        for endpoint in api_endpoints:
            print(f"API ãƒ†ã‚¹ãƒˆ: {self.BASE_URL}{endpoint}")
            
            try:
                # GET ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
                response = self.session.get(
                    f"{self.BASE_URL}{endpoint}",
                    params=test_params,
                    timeout=10
                )
                
                if response.status_code == 200:
                    content_type = response.headers.get('content-type', '')
                    
                    if 'json' in content_type:
                        try:
                            data = response.json()
                            print(f"   âœ… JSON APIç™ºè¦‹!")
                            print(f"      ã‚­ãƒ¼: {list(data.keys()) if isinstance(data, dict) else 'Not dict'}")
                            
                            self.analysis_results['tests'].append({
                                'test': 'api_discovery',
                                'endpoint': endpoint,
                                'success': True,
                                'content_type': content_type,
                                'data_keys': list(data.keys()) if isinstance(data, dict) else None
                            })
                        except json.JSONDecodeError:
                            print(f"   âŒ JSONè§£æã‚¨ãƒ©ãƒ¼")
                    else:
                        print(f"   âš ï¸  éJSONå¿œç­”: {content_type}")
                        
                elif response.status_code == 404:
                    print(f"   âŒ 404 Not Found")
                else:
                    print(f"   âŒ HTTPã‚¨ãƒ©ãƒ¼: {response.status_code}")
                    
            except Exception as e:
                print(f"   ğŸ’¥ ã‚¨ãƒ©ãƒ¼: {e}")
    
    def save_analysis_results(self):
        """è§£æçµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_file = f"logs/bookwalker_analysis_{timestamp}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.analysis_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“„ è§£æçµæœã‚’ä¿å­˜: {output_file}")
        return output_file
    
    def run_full_analysis(self):
        """å®Œå…¨è§£æã®å®Ÿè¡Œ"""
        print("ğŸ” BOOKâ˜†WALKER å®Œå…¨ã‚µã‚¤ãƒˆè§£æé–‹å§‹")
        print(f"é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 60)
        
        try:
            # 1. åŸºæœ¬æ§‹é€ è§£æ
            self.analyze_search_page_structure()
            
            # 2. æ¤œç´¢ã‚¯ã‚¨ãƒªãƒ†ã‚¹ãƒˆ
            self.test_search_queries()
            
            # 3. APIç™ºè¦‹ãƒ†ã‚¹ãƒˆ
            self.test_direct_api_calls()
            
            # 4. çµæœä¿å­˜
            output_file = self.save_analysis_results()
            
            print("=" * 60)
            print("âœ… è§£æå®Œäº†")
            
            # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
            successful_tests = [t for t in self.analysis_results['tests'] if t.get('success')]
            print(f"æˆåŠŸãƒ†ã‚¹ãƒˆ: {len(successful_tests)}/{len(self.analysis_results['tests'])}")
            
            return output_file
            
        except Exception as e:
            print(f"ğŸ’¥ è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return None


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    Path('logs').mkdir(exist_ok=True)
    
    analyzer = BookWalkerAnalyzer()
    result_file = analyzer.run_full_analysis()
    
    if result_file:
        print(f"\nğŸ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: {result_file} ã‚’ç¢ºèªã—ã¦æœ€é©ãªæˆ¦ç•¥ã‚’æ±ºå®š")


if __name__ == "__main__":
    main()