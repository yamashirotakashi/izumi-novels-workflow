#!/usr/bin/env python3
"""
Kinoppy & Reader Store ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
æ–°å®Ÿè£…ãƒ†ã‚¹ãƒˆï¼ˆebookjapanç¶™æ‰¿ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
"""
import asyncio
import sys
import json
from pathlib import Path
from datetime import datetime
import logging

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.scraping.kinoppy_scraper import KinoppyScraper
from src.scraping.reader_store_scraper import ReaderStoreScraper

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


async def test_new_sites():
    """æ–°å®Ÿè£…ã‚µã‚¤ãƒˆã®ãƒ†ã‚¹ãƒˆ"""
    
    # ãƒ†ã‚¹ãƒˆæ›¸ç±ï¼ˆæ–°æ›¸ç±å«ã‚€ï¼‰
    test_books = [
        {
            'n_code': 'N0000TEST',
            'title': 'ã‚½ãƒ¼ãƒ‰ã‚¢ãƒ¼ãƒˆãƒ»ã‚ªãƒ³ãƒ©ã‚¤ãƒ³1',
            'description': 'äººæ°—ã‚·ãƒªãƒ¼ã‚ºãƒ»åŸºæœ¬å½¢'
        },
        {
            'n_code': 'N02402',
            'title': 'ã‚¯ã‚½ã‚²ãƒ¼æ‚ªå½¹ä»¤å¬¢â‘ æ–°è£…ç‰ˆ',
            'description': 'å®Ÿéš›ã®ãªã‚ã†ä½œå“ï¼ˆã‚·ãƒ¼ãƒˆæœªè¨­å®šï¼‰'
        },
        {
            'n_code': 'N0230FK',
            'title': 'ãƒ‘ãƒ©ãƒ¬ã‚¤ãƒ‰ãƒ‡ã‚¤ã‚ºâ‘£',
            'description': 'KADOKAWAç³»ãƒ©ã‚¤ãƒˆãƒãƒ™ãƒ«ï¼ˆä¸¸æ•°å­—å·»æ•°ï¼‰'
        }
    ]
    
    print("=== Kinoppy & Reader Store ãƒ†ã‚¹ãƒˆ ===")
    print(f"é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("å¯¾è±¡: æ–°å®Ÿè£…2ã‚µã‚¤ãƒˆï¼ˆebookjapanç¶™æ‰¿ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰")
    print(f"ãƒ†ã‚¹ãƒˆæ›¸ç±: {len(test_books)}å†Š")
    print()
    
    # æ–°å®Ÿè£…ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼è¨­å®š
    scrapers = [
        (KinoppyScraper, "Kinoppy", "âœ¨ ç´€ä¼Šåœ‹å±‹æ›¸åº—ï¼ˆebookjapanç¶™æ‰¿ï¼‰"),
        (ReaderStoreScraper, "Reader Store", "âœ¨ Sonyï¼ˆebookjapanç¶™æ‰¿ï¼‰")
    ]
    
    all_results = {}
    
    # å„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    for scraper_class, scraper_name, description in scrapers:
        print(f"\n{'='*60}")
        print(f"ğŸ“‹ {scraper_name}: {description}")
        
        results = []
        
        try:
            scraper = scraper_class(timeout=15, max_retries=2)
            
            async with scraper:
                for i, book in enumerate(test_books, 1):
                    print(f"\nğŸ“š ãƒ†ã‚¹ãƒˆ {i}/{len(test_books)}: {book['title']}")
                    print(f"   èª¬æ˜: {book['description']}")
                    
                    start_time = datetime.now()
                    
                    try:
                        url = await scraper.search_book(book['title'], book['n_code'])
                        
                        end_time = datetime.now()
                        processing_time = (end_time - start_time).total_seconds()
                        
                        result = {
                            'n_code': book['n_code'],
                            'title': book['title'],
                            'description': book['description'],
                            'url': url,
                            'success': url is not None,
                            'processing_time': processing_time,
                            'timestamp': end_time.isoformat()
                        }
                        
                        if url:
                            print(f"   âœ… æˆåŠŸ: {url}")
                            print(f"   â±ï¸  å‡¦ç†æ™‚é–“: {processing_time:.1f}ç§’")
                        else:
                            print(f"   âŒ è¦‹ã¤ã‹ã‚‰ãš")
                            print(f"   â±ï¸  å‡¦ç†æ™‚é–“: {processing_time:.1f}ç§’")
                        
                        results.append(result)
                        
                    except Exception as e:
                        end_time = datetime.now()
                        processing_time = (end_time - start_time).total_seconds()
                        
                        result = {
                            'n_code': book['n_code'],
                            'title': book['title'],
                            'description': book['description'],
                            'url': None,
                            'success': False,
                            'error': str(e),
                            'processing_time': processing_time,
                            'timestamp': end_time.isoformat()
                        }
                        
                        print(f"   ğŸ’¥ ã‚¨ãƒ©ãƒ¼: {str(e)}")
                        print(f"   â±ï¸  å‡¦ç†æ™‚é–“: {processing_time:.1f}ç§’")
                        results.append(result)
                    
                    # æ›¸ç±é–“å¾…æ©Ÿ
                    if i < len(test_books):
                        await asyncio.sleep(2)
            
            all_results[scraper_name] = results
            
            # ã‚µã‚¤ãƒˆåˆ¥ã‚µãƒãƒªãƒ¼
            successful = [r for r in results if r['success']]
            success_rate = (len(successful) / len(results)) * 100 if results else 0
            avg_time = sum(r['processing_time'] for r in results) / len(results) if results else 0
            
            print(f"\nğŸ“Š {scraper_name} çµæœ:")
            print(f"  æˆåŠŸ: {len(successful)}/{len(results)} ({success_rate:.1f}%)")
            print(f"  å¹³å‡æ™‚é–“: {avg_time:.1f}ç§’")
            
            # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼çµ±è¨ˆ
            stats = scraper.get_stats()
            print(f"  æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³: {stats.get('search_pattern', 'N/A')}")
            print(f"  ç¶™æ‰¿å…ƒ: {stats.get('success_pattern', 'N/A')}")
            
        except Exception as e:
            logger.error(f"{scraper_name} åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            print(f"ğŸ’¥ {scraper_name} åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            all_results[scraper_name] = []
        
        # ã‚µã‚¤ãƒˆé–“å¾…æ©Ÿ
        await asyncio.sleep(3)
    
    # ç·åˆçµæœ
    print(f"\n{'='*60}")
    print("ğŸ¯ æ–°å®Ÿè£…ã‚µã‚¤ãƒˆç·åˆçµæœ")
    print(f"{'='*60}")
    
    total_tests = sum(len(results) for results in all_results.values())
    total_successes = sum(len([r for r in results if r['success']]) 
                         for results in all_results.values())
    overall_success_rate = (total_successes / total_tests * 100) if total_tests > 0 else 0
    
    print(f"ç·åˆæˆç¸¾: {total_successes}/{total_tests} ({overall_success_rate:.1f}%)")
    print(f"ãƒ†ã‚¹ãƒˆæ›¸ç±: {len(test_books)}å†Š Ã— {len(scrapers)}ã‚µã‚¤ãƒˆ")
    print()
    print("ğŸ“Š ã‚µã‚¤ãƒˆåˆ¥è©³ç´°:")
    
    for scraper_name, results in all_results.items():
        if results:
            successful = [r for r in results if r['success']]
            success_rate = (len(successful) / len(results)) * 100
            avg_time = sum(r['processing_time'] for r in results) / len(results)
            
            if success_rate == 100.0:
                emoji = "ğŸ¥‡"
                status = "Perfect!"
            elif success_rate >= 80.0:
                emoji = "ğŸ¥ˆ"
                status = "Excellent"
            elif success_rate >= 60.0:
                emoji = "ğŸ¥‰"
                status = "Good"
            else:
                emoji = "ğŸ¤”"
                status = "Need Fix"
            
            print(f"  {emoji} {scraper_name:15}: {len(successful)}/{len(results)} ({success_rate:5.1f}%) {avg_time:4.1f}s - {status}")
        else:
            print(f"  ğŸ’¥ {scraper_name:15}: åˆæœŸåŒ–å¤±æ•—")
    
    # ebookjapanç¶™æ‰¿åŠ¹æœåˆ†æ
    print(f"\nğŸ”¬ ebookjapanç¶™æ‰¿åŠ¹æœåˆ†æ:")
    print(f"  ç¶™æ‰¿å…ƒ: ebookjapan 100%")
    
    if all_results:
        inheritance_rates = []
        for scraper_name, results in all_results.items():
            if results:
                successful = [r for r in results if r['success']]
                rate = (len(successful) / len(results)) * 100
                inheritance_rates.append(rate)
                print(f"  {scraper_name}: {rate:.1f}%")
        
        if inheritance_rates:
            avg_inheritance = sum(inheritance_rates) / len(inheritance_rates)
            print(f"  ç¶™æ‰¿ã‚µã‚¤ãƒˆå¹³å‡: {avg_inheritance:.1f}%")
            print(f"  ç¶™æ‰¿åŠ¹æœ: {'+' if avg_inheritance >= 80 else '-'}{abs(avg_inheritance - 100):.1f}%")
    
    # æˆåŠŸãƒ»å¤±æ•—è©³ç´°
    for scraper_name, results in all_results.items():
        if not results:
            continue
        
        successful = [r for r in results if r['success']]
        failed = [r for r in results if not r['success']]
        
        if successful:
            print(f"\nâœ… {scraper_name} æˆåŠŸã‚±ãƒ¼ã‚¹:")
            for result in successful:
                print(f"  - {result['title']}: {result['url']}")
        
        if failed:
            print(f"\nâŒ {scraper_name} å¤±æ•—ã‚±ãƒ¼ã‚¹:")
            for result in failed:
                error = result.get('error', 'è¦‹ã¤ã‹ã‚‰ãš')
                print(f"  - {result['title']}: {error}")
    
    # ææ¡ˆ
    print(f"\nğŸ’¡ Phase 3 ã¸ã®ææ¡ˆ:")
    if overall_success_rate >= 80:
        print(f"  ğŸ‰ ç¶™æ‰¿æˆ¦ç•¥æˆåŠŸï¼å…¨ã‚µã‚¤ãƒˆ80%ä»¥ä¸Šé”æˆ")
        print(f"  ğŸ“‹ æ¬¡: Amazon PODå®Ÿè£… + æœ€çµ‚çµ±åˆãƒ†ã‚¹ãƒˆ")
    else:
        problem_sites = [name for name, results in all_results.items() 
                        if results and (len([r for r in results if r['success']]) / len(results) * 100) < 80]
        if problem_sites:
            print(f"  ğŸ”§ è¦æ”¹å–„: {', '.join(problem_sites)}")
            print(f"  ğŸ“‹ å¯¾ç­–: ã‚»ãƒ¬ã‚¯ã‚¿èª¿æ•´ã€URLæ¤œè¨¼å¼·åŒ–")
    
    print(f"\n=== Kinoppy & Reader Store ãƒ†ã‚¹ãƒˆå®Œäº† ===")
    return all_results


async def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    try:
        await test_new_sites()
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ãƒ†ã‚¹ãƒˆãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        logger.error(f"ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        print(f"ğŸ’¥ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")


if __name__ == "__main__":
    asyncio.run(main())