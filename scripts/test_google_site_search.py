#!/usr/bin/env python3
"""
Google Site Search ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ãƒ†ã‚¹ãƒˆ
Kinoppy & Reader Storeæ”»ç•¥æœ€çµ‚æ‰‹æ®µ
"""
import asyncio
import sys
from pathlib import Path
from datetime import datetime
import logging

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.scraping.google_site_search_scraper import KinoppyGoogleScraper, ReaderStoreGoogleScraper

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


async def test_google_site_search():
    """Google Site Search ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ãƒ†ã‚¹ãƒˆ"""
    
    # ãƒ†ã‚¹ãƒˆæ›¸ç±
    test_books = [
        {
            'n_code': 'N0000TEST',
            'title': 'ã‚½ãƒ¼ãƒ‰ã‚¢ãƒ¼ãƒˆãƒ»ã‚ªãƒ³ãƒ©ã‚¤ãƒ³1',
            'description': 'äººæ°—ã‚·ãƒªãƒ¼ã‚ºãƒ»åŸºæœ¬å½¢ï¼ˆGoogleæ¤œç´¢ã§ç¢ºå®Ÿã«ãƒ’ãƒƒãƒˆï¼‰'
        },
        {
            'n_code': 'N02402',
            'title': 'ã‚¯ã‚½ã‚²ãƒ¼æ‚ªå½¹ä»¤å¬¢â‘ æ–°è£…ç‰ˆ',
            'description': 'å®Ÿéš›ã®ãªã‚ã†ä½œå“ï¼ˆã‚·ãƒ¼ãƒˆæœªè¨­å®šï¼‰'
        }
    ]
    
    print("=== Google Site Search ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ãƒ†ã‚¹ãƒˆ ===")
    print(f"é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("å¯¾è±¡: Kinoppy & Reader Storeï¼ˆGoogleçµŒç”±æ”»ç•¥ï¼‰")
    print(f"ãƒ†ã‚¹ãƒˆæ›¸ç±: {len(test_books)}å†Š")
    print()
    
    # Google Site Searchã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼è¨­å®š
    scrapers = [
        (KinoppyGoogleScraper, "Kinoppy Google", "ğŸ” ç´€ä¼Šåœ‹å±‹ï¼ˆGoogle Site Searchï¼‰"),
        (ReaderStoreGoogleScraper, "Reader Store Google", "ğŸ” Sonyï¼ˆGoogle Site Searchï¼‰")
    ]
    
    all_results = {}
    
    # å„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    for scraper_class, scraper_name, description in scrapers:
        print(f"\n{'='*60}")
        print(f"ğŸ“‹ {scraper_name}: {description}")
        
        results = []
        
        try:
            scraper = scraper_class(timeout=20, max_retries=2)
            
            async with scraper:
                for i, book in enumerate(test_books, 1):
                    print(f"\nğŸ“š ãƒ†ã‚¹ãƒˆ {i}/{len(test_books)}: {book['title']}")
                    print(f"   èª¬æ˜: {book['description']}")
                    
                    start_time = datetime.now()
                    
                    try:
                        url = await scraper.search_book(book['title'], book['n_code'])
                        
                        end_time = datetime.now()
                        processing_time = (end_time - start_time).total_seconds()
                        
                        result = {
                            'n_code': book['n_code'],
                            'title': book['title'],
                            'description': book['description'],
                            'url': url,
                            'success': url is not None,
                            'processing_time': processing_time,
                            'timestamp': end_time.isoformat()
                        }
                        
                        if url:
                            print(f"   âœ… æˆåŠŸ: {url}")
                            print(f"   â±ï¸  å‡¦ç†æ™‚é–“: {processing_time:.1f}ç§’")
                        else:
                            print(f"   âŒ è¦‹ã¤ã‹ã‚‰ãš")
                            print(f"   â±ï¸  å‡¦ç†æ™‚é–“: {processing_time:.1f}ç§’")
                        
                        results.append(result)
                        
                    except Exception as e:
                        end_time = datetime.now()
                        processing_time = (end_time - start_time).total_seconds()
                        
                        result = {
                            'n_code': book['n_code'],
                            'title': book['title'],
                            'description': book['description'],
                            'url': None,
                            'success': False,
                            'error': str(e),
                            'processing_time': processing_time,
                            'timestamp': end_time.isoformat()
                        }
                        
                        print(f"   ğŸ’¥ ã‚¨ãƒ©ãƒ¼: {str(e)}")
                        print(f"   â±ï¸  å‡¦ç†æ™‚é–“: {processing_time:.1f}ç§’")
                        results.append(result)
                    
                    # æ›¸ç±é–“å¾…æ©Ÿï¼ˆGoogleè² è·è»½æ¸›ï¼‰
                    if i < len(test_books):
                        await asyncio.sleep(3)
            
            all_results[scraper_name] = results
            
            # ã‚µã‚¤ãƒˆåˆ¥ã‚µãƒãƒªãƒ¼
            successful = [r for r in results if r['success']]
            success_rate = (len(successful) / len(results)) * 100 if results else 0
            avg_time = sum(r['processing_time'] for r in results) / len(results) if results else 0
            
            print(f"\nğŸ“Š {scraper_name} çµæœ:")
            print(f"  æˆåŠŸ: {len(successful)}/{len(results)} ({success_rate:.1f}%)")
            print(f"  å¹³å‡æ™‚é–“: {avg_time:.1f}ç§’")
            
            # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼çµ±è¨ˆ
            stats = scraper.get_stats()
            print(f"  æ¤œç´¢æ–¹å¼: {stats.get('search_method', 'N/A')}")
            print(f"  å¯¾è±¡ã‚µã‚¤ãƒˆ: {stats.get('target_site', 'N/A')}")
            
        except Exception as e:
            logger.error(f"{scraper_name} åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            print(f"ğŸ’¥ {scraper_name} åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            all_results[scraper_name] = []
        
        # ã‚µã‚¤ãƒˆé–“å¾…æ©Ÿï¼ˆGoogleè² è·è»½æ¸›ï¼‰
        await asyncio.sleep(5)
    
    # ç·åˆçµæœ
    print(f"\n{'='*60}")
    print("ğŸ¯ Google Site Search ç·åˆçµæœ")
    print(f"{'='*60}")
    
    total_tests = sum(len(results) for results in all_results.values())
    total_successes = sum(len([r for r in results if r['success']]) 
                         for results in all_results.values())
    overall_success_rate = (total_successes / total_tests * 100) if total_tests > 0 else 0
    
    print(f"ç·åˆæˆç¸¾: {total_successes}/{total_tests} ({overall_success_rate:.1f}%)")
    print(f"ãƒ†ã‚¹ãƒˆæ›¸ç±: {len(test_books)}å†Š Ã— {len(scrapers)}ã‚µã‚¤ãƒˆ")
    print()
    print("ğŸ“Š ã‚µã‚¤ãƒˆåˆ¥è©³ç´°:")
    
    for scraper_name, results in all_results.items():
        if results:
            successful = [r for r in results if r['success']]
            success_rate = (len(successful) / len(results)) * 100
            avg_time = sum(r['processing_time'] for r in results) / len(results)
            
            if success_rate >= 50.0:
                emoji = "ğŸ¥‡"
                status = "Googleçªç ´æˆåŠŸï¼"
            elif success_rate > 0:
                emoji = "ğŸ¥ˆ"
                status = "éƒ¨åˆ†æˆåŠŸ"
            else:
                emoji = "ğŸ¤”"
                status = "Googleåˆ¶é™ï¼Ÿ"
            
            print(f"  {emoji} {scraper_name:20}: {len(successful)}/{len(results)} ({success_rate:5.1f}%) {avg_time:4.1f}s - {status}")
        else:
            print(f"  ğŸ’¥ {scraper_name:20}: åˆæœŸåŒ–å¤±æ•—")
    
    # Google Site SearchåŠ¹æœåˆ†æ
    print(f"\nğŸ”¬ Google Site SearchåŠ¹æœåˆ†æ:")
    print(f"  å¾“æ¥æ‰‹æ³•: Requests + BeautifulSoup 0%")
    print(f"  GoogleçµŒç”±: {overall_success_rate:.1f}%")
    
    if overall_success_rate > 0:
        print(f"  ğŸ‰ Google Site SearchåŠ¹æœ: +{overall_success_rate:.1f}%ã®æ”¹å–„ï¼")
        print(f"  ğŸ’¡ é–“æ¥ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«ã‚ˆã‚‹å›°é›£ã‚µã‚¤ãƒˆæ”»ç•¥æˆåŠŸ")
    else:
        print(f"  ğŸ¤” Googleåˆ¶é™ã¾ãŸã¯ã‚µã‚¤ãƒˆæ§‹é€ ã®å•é¡Œ")
        print(f"  ğŸ“‹ æ¬¡æ®µéš: ã‚ˆã‚Šé«˜åº¦ãªæ‰‹æ³•ï¼ˆAPIè§£æã€ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ–ãƒ©ã‚¦ã‚¶ï¼‰")
    
    # æˆåŠŸãƒ»å¤±æ•—è©³ç´°
    for scraper_name, results in all_results.items():
        if not results:
            continue
        
        successful = [r for r in results if r['success']]
        failed = [r for r in results if not r['success']]
        
        if successful:
            print(f"\nâœ… {scraper_name} æˆåŠŸã‚±ãƒ¼ã‚¹:")
            for result in successful:
                print(f"  - {result['title']}: {result['url']}")
        
        if failed:
            print(f"\nâŒ {scraper_name} å¤±æ•—ã‚±ãƒ¼ã‚¹:")
            for result in failed:
                error = result.get('error', 'è¦‹ã¤ã‹ã‚‰ãš')
                print(f"  - {result['title']}: {error}")
    
    print(f"\n=== Google Site Search ãƒ†ã‚¹ãƒˆå®Œäº† ===")
    return all_results


async def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    try:
        await test_google_site_search()
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ãƒ†ã‚¹ãƒˆãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        logger.error(f"ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        print(f"ğŸ’¥ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")


if __name__ == "__main__":
    asyncio.run(main())