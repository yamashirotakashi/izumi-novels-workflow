#!/usr/bin/env python3
"""
å…¨ã‚µã‚¤ãƒˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Phase 2å®Ÿè£…ã®åŒ…æ‹¬çš„è©•ä¾¡
"""
import asyncio
import sys
import json
from pathlib import Path
from datetime import datetime
import logging

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.scraping.bookwalker_scraper import BookWalkerScraper
from src.scraping.ebookjapan_scraper import EbookjapanScraper
from src.scraping.booklive_scraper import BookLiveScraper
from src.scraping.apple_books_scraper import AppleBooksLinkGenerator
from src.scraping.honto_scraper import HontoScraper
from src.scraping.kinoppy_scraper import KinoppyScraper
from src.scraping.reader_store_scraper import ReaderStoreScraper

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,  # DEBUGã‹ã‚‰INFOã«å¤‰æ›´ï¼ˆå‡ºåŠ›é‡å‰Šæ¸›ï¼‰
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('logs/all_scrapers_test.log')
    ]
)
logger = logging.getLogger(__name__)


async def test_scraper(scraper_class, scraper_name: str, test_books: list, isbn_books: dict = None):
    """å€‹åˆ¥ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    print(f"\nğŸ” {scraper_name} ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼åˆ¥åˆæœŸåŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    if scraper_name == "Apple Books":
        scraper = scraper_class(timeout=20, max_retries=2)
    elif scraper_name == "BOOKâ˜†WALKER":
        # Playwright ãƒ™ãƒ¼ã‚¹ï¼ˆBaseScraperç¶™æ‰¿ï¼‰
        scraper = scraper_class(headless=True, timeout=15000)
    else:
        # Requests ãƒ™ãƒ¼ã‚¹ï¼ˆRequestsScraperç¶™æ‰¿ï¼‰
        scraper = scraper_class(timeout=15, max_retries=2)
    
    results = []
    
    try:
        async with scraper:
            for i, book in enumerate(test_books, 1):
                print(f"  ğŸ“š {i}/{len(test_books)}: {book['title'][:30]}...")
                
                start_time = datetime.now()
                
                try:
                    # Apple Booksç‰¹æ®Šå¯¾å¿œï¼ˆISBNä»˜ãï¼‰
                    if scraper_name == "Apple Books" and isbn_books and book['n_code'] in isbn_books:
                        isbn = isbn_books[book['n_code']]
                        url = await scraper.search_book(book['title'], book['n_code'], isbn)
                    else:
                        url = await scraper.search_book(book['title'], book['n_code'])
                    
                    end_time = datetime.now()
                    processing_time = (end_time - start_time).total_seconds()
                    
                    result = {
                        'n_code': book['n_code'],
                        'title': book['title'],
                        'url': url,
                        'success': url is not None,
                        'processing_time': processing_time,
                        'timestamp': end_time.isoformat()
                    }
                    
                    if url:
                        print(f"    âœ… æˆåŠŸ ({processing_time:.1f}s)")
                    else:
                        print(f"    âŒ å¤±æ•— ({processing_time:.1f}s)")
                    
                    results.append(result)
                    
                except Exception as e:
                    end_time = datetime.now()
                    processing_time = (end_time - start_time).total_seconds()
                    
                    result = {
                        'n_code': book['n_code'],
                        'title': book['title'],
                        'url': None,
                        'success': False,
                        'error': str(e),
                        'processing_time': processing_time,
                        'timestamp': end_time.isoformat()
                    }
                    
                    print(f"    ğŸ’¥ ã‚¨ãƒ©ãƒ¼ ({processing_time:.1f}s): {str(e)[:50]}...")
                    results.append(result)
                
                # ã‚µã‚¤ãƒˆé–“å¾…æ©Ÿ
                if i < len(test_books):
                    await asyncio.sleep(1)
    
    except Exception as e:
        logger.error(f"{scraper_name} åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        print(f"  ğŸ’¥ {scraper_name} åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return []
    
    return results


async def main():
    """ãƒ¡ã‚¤ãƒ³çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    print("=== IzumiNovels-Workflow Phase 2 çµ±åˆãƒ†ã‚¹ãƒˆ ===")
    print(f"é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("å¯¾è±¡: å…¨7ã‚µã‚¤ãƒˆã®åŒ…æ‹¬çš„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ†ã‚¹ãƒˆ")
    print()
    
    # å…±é€šãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    test_books = [
        {
            'n_code': 'N0000TEST',
            'title': 'ã‚½ãƒ¼ãƒ‰ã‚¢ãƒ¼ãƒˆãƒ»ã‚ªãƒ³ãƒ©ã‚¤ãƒ³1',
            'description': 'äººæ°—ã‚·ãƒªãƒ¼ã‚ºãƒ»åŸºæœ¬å½¢'
        },
        {
            'n_code': 'N02402',
            'title': 'ã‚¯ã‚½ã‚²ãƒ¼æ‚ªå½¹ä»¤å¬¢â‘ æ–°è£…ç‰ˆ',
            'description': 'å®Ÿéš›ã®ãªã‚ã†ä½œå“ï¼ˆã‚·ãƒ¼ãƒˆæœªè¨­å®šï¼‰'
        },
        {
            'n_code': 'N0230FK',
            'title': 'ãƒ‘ãƒ©ãƒ¬ã‚¤ãƒ‰ãƒ‡ã‚¤ã‚ºâ‘£',
            'description': 'KADOKAWAç³»ãƒ©ã‚¤ãƒˆãƒãƒ™ãƒ«ï¼ˆä¸¸æ•°å­—å·»æ•°ï¼‰'
        }
    ]
    
    # Apple Booksç”¨ISBNæƒ…å ±
    isbn_books = {
        'N0000TEST': '9784048671811',  # SAO
        'N02402': '',                  # ã‚¯ã‚½ã‚²ãƒ¼æ‚ªå½¹ä»¤å¬¢ï¼ˆISBNãªã—ï¼‰
        'N0230FK': '9784040738222',   # ãƒ‘ãƒ©ãƒ¬ã‚¤ãƒ‰ãƒ‡ã‚¤ã‚º
    }
    
    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼è¨­å®š
    scrapers = [
        (EbookjapanScraper, "ebookjapan", "ğŸ¥‡ 100%æˆåŠŸã®åŸºæº–ã‚µã‚¤ãƒˆ"),
        (BookWalkerScraper, "BOOKâ˜†WALKER", "ğŸ“ˆ æ”¹å–„ä¸­ï¼ˆ66.7%ï¼‰"),
        (BookLiveScraper, "BookLive", "ğŸ“ˆ æ”¹å–„ä¸­ï¼ˆ66.7%ï¼‰"),
        (AppleBooksLinkGenerator, "Apple Books", "ğŸ¯ iTunes APIç‰¹æ®Šå®Ÿè£…"),
        (HontoScraper, "honto", "âœ¨ æ–°å®Ÿè£…ï¼ˆebookjapanç¶™æ‰¿ï¼‰"),
        (KinoppyScraper, "Kinoppy", "âœ¨ æ–°å®Ÿè£…ï¼ˆebookjapanç¶™æ‰¿ï¼‰"),
        (ReaderStoreScraper, "Reader Store", "âœ¨ æ–°å®Ÿè£…ï¼ˆebookjapanç¶™æ‰¿ï¼‰")
    ]
    
    # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    Path('logs').mkdir(exist_ok=True)
    
    all_results = {}
    summary_stats = {}
    
    # å„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    for scraper_class, scraper_name, description in scrapers:
        print(f"\n{'='*60}")
        print(f"ğŸ“‹ {scraper_name}: {description}")
        
        # Apple Booksç‰¹æ®Šå¯¾å¿œ
        if scraper_name == "Apple Books":
            results = await test_scraper(scraper_class, scraper_name, test_books, isbn_books)
        else:
            results = await test_scraper(scraper_class, scraper_name, test_books)
        
        all_results[scraper_name] = results
        
        # çµ±è¨ˆè¨ˆç®—
        if results:
            successful = [r for r in results if r['success']]
            success_rate = (len(successful) / len(results)) * 100
            avg_time = sum(r['processing_time'] for r in results) / len(results)
            
            summary_stats[scraper_name] = {
                'total_tests': len(results),
                'successes': len(successful),
                'success_rate': success_rate,
                'avg_processing_time': avg_time
            }
            
            print(f"  ğŸ“Š çµæœ: {len(successful)}/{len(results)} ({success_rate:.1f}%) | å¹³å‡æ™‚é–“: {avg_time:.1f}s")
        else:
            summary_stats[scraper_name] = {
                'total_tests': 0,
                'successes': 0,
                'success_rate': 0.0,
                'avg_processing_time': 0.0
            }
            print(f"  ğŸ“Š çµæœ: ãƒ†ã‚¹ãƒˆå¤±æ•—")
        
        # ã‚µã‚¤ãƒˆé–“ã®å¾…æ©Ÿ
        await asyncio.sleep(3)
    
    # çµ±åˆçµæœã‚µãƒãƒªãƒ¼
    print(f"\n{'='*60}")
    print("ğŸ¯ Phase 2 çµ±åˆãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
    print(f"{'='*60}")
    
    total_tests = sum(stats['total_tests'] for stats in summary_stats.values())
    total_successes = sum(stats['successes'] for stats in summary_stats.values())
    overall_success_rate = (total_successes / total_tests * 100) if total_tests > 0 else 0
    
    print(f"ç·åˆæˆç¸¾: {total_successes}/{total_tests} ({overall_success_rate:.1f}%)")
    print()
    print("ğŸ“Š ã‚µã‚¤ãƒˆåˆ¥æˆç¸¾:")
    
    # æˆåŠŸç‡é †ã§ã‚½ãƒ¼ãƒˆ
    sorted_stats = sorted(summary_stats.items(), key=lambda x: x[1]['success_rate'], reverse=True)
    
    for scraper_name, stats in sorted_stats:
        rate = stats['success_rate']
        successes = stats['successes']
        total = stats['total_tests']
        avg_time = stats['avg_processing_time']
        
        if rate == 100.0:
            emoji = "ğŸ¥‡"
            status = "Perfect!"
        elif rate >= 80.0:
            emoji = "ğŸ¥ˆ"
            status = "Excellent"
        elif rate >= 60.0:
            emoji = "ğŸ¥‰"
            status = "Good"
        else:
            emoji = "ğŸ¤”"
            status = "Need Fix"
        
        print(f"  {emoji} {scraper_name:15}: {successes:1}/{total} ({rate:5.1f}%) {avg_time:4.1f}s - {status}")
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
    print(f"\nâš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ:")
    fastest_site = min(sorted_stats, key=lambda x: x[1]['avg_processing_time'])
    slowest_site = max(sorted_stats, key=lambda x: x[1]['avg_processing_time'])
    
    print(f"  æœ€é€Ÿ: {fastest_site[0]} ({fastest_site[1]['avg_processing_time']:.1f}s)")
    print(f"  æœ€é…: {slowest_site[0]} ({slowest_site[1]['avg_processing_time']:.1f}s)")
    
    # æŠ€è¡“ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
    print(f"\\nğŸ”¬ æŠ€è¡“ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ:")
    ebookjapan_rate = summary_stats.get('ebookjapan', {}).get('success_rate', 0)
    inheritance_sites = ['honto', 'Kinoppy', 'Reader Store']
    
    inheritance_rates = [summary_stats.get(site, {}).get('success_rate', 0) 
                        for site in inheritance_sites if site in summary_stats]
    
    if inheritance_rates:
        avg_inheritance_rate = sum(inheritance_rates) / len(inheritance_rates)
        print(f"  ebookjapanåŸºæº–: {ebookjapan_rate:.1f}%")
        print(f"  ç¶™æ‰¿ã‚µã‚¤ãƒˆå¹³å‡: {avg_inheritance_rate:.1f}%")
        print(f"  ç¶™æ‰¿åŠ¹æœ: {'+' if avg_inheritance_rate >= 80 else '-'}{abs(avg_inheritance_rate - ebookjapan_rate):.1f}%")
    
    # æ”¹å–„ææ¡ˆ
    print(f"\\nğŸ’¡ Phase 3 ã¸ã®ææ¡ˆ:")
    problem_sites = [name for name, stats in summary_stats.items() 
                    if stats['success_rate'] < 80.0 and stats['total_tests'] > 0]
    
    if problem_sites:
        print(f"  ğŸ”§ è¦æ”¹å–„: {', '.join(problem_sites)}")
        print(f"  ğŸ“‹ å¯¾ç­–: ã‚µã‚¤ãƒˆæ§‹é€ å†èª¿æŸ»ã€ã‚»ãƒ¬ã‚¯ã‚¿æœ€é©åŒ–")
    else:
        print(f"  ğŸ‰ å…¨ã‚µã‚¤ãƒˆ80%ä»¥ä¸Šé”æˆï¼æœ€çµ‚èª¿æ•´ã¸")
    
    # çµæœä¿å­˜
    comprehensive_results = {
        'test_info': {
            'test_type': 'Phase2_Integration_Test',
            'test_date': datetime.now().isoformat(),
            'total_sites': len(scrapers),
            'total_tests': total_tests,
            'total_successes': total_successes,
            'overall_success_rate': overall_success_rate,
            'test_books': test_books,
            'target_achievement': 'å…¨ã‚µã‚¤ãƒˆ80%ä»¥ä¸Š',
        },
        'site_results': all_results,
        'summary_stats': summary_stats,
        'recommendations': {
            'problem_sites': problem_sites,
            'next_phase': 'Phase3_Final_Optimization' if not problem_sites else 'Phase2_Improvement'
        }
    }
    
    output_file = f"logs/integration_test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(comprehensive_results, f, ensure_ascii=False, indent=2)
    
    print(f"\\nğŸ“„ è©³ç´°çµæœä¿å­˜: {output_file}")
    print(f"\\n{'='*60}")
    print("ğŸ¯ Phase 2 çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†")
    print(f"{'='*60}")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\\nâš ï¸ ãƒ†ã‚¹ãƒˆãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        logger.error(f"çµ±åˆãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        print(f"ğŸ’¥ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")