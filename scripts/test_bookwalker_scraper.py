#!/usr/bin/env python3
"""
BOOKâ˜†WALKERã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""
import asyncio
import sys
import json
from pathlib import Path
from datetime import datetime
import logging

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.scraping.bookwalker_scraper import BookWalkerScraper

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('logs/bookwalker_test.log')
    ]
)
logger = logging.getLogger(__name__)


async def test_bookwalker_scraper():
    """BOOKâ˜†WALKERã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    test_books = [
        {
            'n_code': 'N0230FK',
            'title': 'ãƒ‘ãƒ©ãƒ¬ã‚¤ãƒ‰ãƒ‡ã‚¤ã‚ºâ‘£',
            'description': 'KADOKAWAç³»ãƒ©ã‚¤ãƒˆãƒãƒ™ãƒ«ï¼ˆä¸¸æ•°å­—å·»æ•°ï¼‰'
        },
        {
            'n_code': 'N7975EJ',
            'title': 'ã‚¨ã‚¢ãƒœãƒ¼ãƒ³ã‚¦ã‚¤ãƒƒãƒâ‘£',
            'description': 'KADOKAWAç³»ãƒ©ã‚¤ãƒˆãƒãƒ™ãƒ«ï¼ˆä¸¸æ•°å­—å·»æ•°ï¼‰'
        },
        {
            'n_code': 'N0000TEST',
            'title': 'ã‚½ãƒ¼ãƒ‰ã‚¢ãƒ¼ãƒˆãƒ»ã‚ªãƒ³ãƒ©ã‚¤ãƒ³1',
            'description': 'äººæ°—ã‚·ãƒªãƒ¼ã‚ºï¼ˆæ•°å­—å·»æ•°ï¼‰'
        },
        {
            'n_code': 'N0000TEST2',
            'title': 'ã“ã®ç´ æ™´ã‚‰ã—ã„ä¸–ç•Œã«ç¥ç¦ã‚’ï¼',
            'description': 'ã‚·ãƒªãƒ¼ã‚ºåã®ã¿ï¼ˆå·»æ•°ãªã—ï¼‰'
        }
    ]
    
    print("=== BOOKâ˜†WALKERã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ãƒ†ã‚¹ãƒˆé–‹å§‹ ===")
    print(f"ãƒ†ã‚¹ãƒˆé–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ãƒ†ã‚¹ãƒˆå¯¾è±¡: {len(test_books)}å†Š")
    print()
    
    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼åˆæœŸåŒ–
    scraper = BookWalkerScraper(
        headless=True,
        timeout=30000,
        screenshot_dir="logs/screenshots"
    )
    
    results = []
    
    try:
        async with scraper:
            for i, book in enumerate(test_books, 1):
                print(f"ğŸ“š ãƒ†ã‚¹ãƒˆ {i}/{len(test_books)}: {book['title']}")
                print(f"   èª¬æ˜: {book['description']}")
                
                start_time = datetime.now()
                
                try:
                    # æ›¸ç±æ¤œç´¢å®Ÿè¡Œ
                    url = await scraper.search_book(book['title'], book['n_code'])
                    
                    end_time = datetime.now()
                    processing_time = (end_time - start_time).total_seconds()
                    
                    result = {
                        'n_code': book['n_code'],
                        'title': book['title'],
                        'description': book['description'],
                        'url': url,
                        'success': url is not None,
                        'processing_time': processing_time,
                        'timestamp': end_time.isoformat()
                    }
                    
                    if url:
                        print(f"   âœ… æˆåŠŸ: {url}")
                        print(f"   â±ï¸  å‡¦ç†æ™‚é–“: {processing_time:.1f}ç§’")
                    else:
                        print(f"   âŒ è¦‹ã¤ã‹ã‚‰ãš")
                        print(f"   â±ï¸  å‡¦ç†æ™‚é–“: {processing_time:.1f}ç§’")
                    
                    results.append(result)
                    
                except Exception as e:
                    end_time = datetime.now()
                    processing_time = (end_time - start_time).total_seconds()
                    
                    result = {
                        'n_code': book['n_code'],
                        'title': book['title'],
                        'description': book['description'],
                        'url': None,
                        'success': False,
                        'error': str(e),
                        'processing_time': processing_time,
                        'timestamp': end_time.isoformat()
                    }
                    
                    print(f"   ğŸ’¥ ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    print(f"   â±ï¸  å‡¦ç†æ™‚é–“: {processing_time:.1f}ç§’")
                    results.append(result)
                
                print()
                
                # æ›¸ç±é–“ã®å¾…æ©Ÿï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œï¼‰
                if i < len(test_books):
                    await asyncio.sleep(3)
    
    except Exception as e:
        logger.error(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        print(f"ğŸ’¥ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return
    
    # çµæœã‚µãƒãƒªãƒ¼
    print("=== ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼ ===")
    
    successful_results = [r for r in results if r['success']]
    failed_results = [r for r in results if not r['success']]
    
    success_rate = (len(successful_results) / len(results)) * 100 if results else 0
    avg_processing_time = sum(r['processing_time'] for r in results) / len(results) if results else 0
    
    print(f"ç·å®Ÿè¡Œæ•°: {len(results)}")
    print(f"æˆåŠŸ: {len(successful_results)}ä»¶")
    print(f"å¤±æ•—: {len(failed_results)}ä»¶")
    print(f"æˆåŠŸç‡: {success_rate:.1f}%")
    print(f"å¹³å‡å‡¦ç†æ™‚é–“: {avg_processing_time:.1f}ç§’")
    
    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼çµ±è¨ˆ
    stats = scraper.get_stats()
    print(f"\nã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼çµ±è¨ˆ:")
    print(f"  æ¤œç´¢å›æ•°: {stats.get('search_count', 0)}")
    print(f"  æˆåŠŸå›æ•°: {stats.get('success_count', 0)}")
    print(f"  ã‚¨ãƒ©ãƒ¼å›æ•°: {stats.get('error_count', 0)}")
    
    # è©³ç´°çµæœã®ä¿å­˜
    results_data = {
        'test_info': {
            'scraper': 'BookWalkerScraper',
            'test_date': datetime.now().isoformat(),
            'total_tests': len(results),
            'success_count': len(successful_results),
            'failure_count': len(failed_results),
            'success_rate': success_rate,
            'average_processing_time': avg_processing_time
        },
        'results': results,
        'scraper_stats': stats
    }
    
    # çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    output_file = f"logs/bookwalker_test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“„ è©³ç´°çµæœã‚’ä¿å­˜: {output_file}")
    
    # å¤±æ•—ã‚±ãƒ¼ã‚¹ã®è©³ç´°è¡¨ç¤º
    if failed_results:
        print(f"\nâŒ å¤±æ•—ã‚±ãƒ¼ã‚¹è©³ç´°:")
        for result in failed_results:
            print(f"  - {result['title']}")
            if 'error' in result:
                print(f"    ã‚¨ãƒ©ãƒ¼: {result['error']}")
    
    print(f"\n=== BOOKâ˜†WALKERã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ãƒ†ã‚¹ãƒˆå®Œäº† ===")
    return results


async def test_title_normalization():
    """ã‚¿ã‚¤ãƒˆãƒ«æ­£è¦åŒ–ã®ãƒ†ã‚¹ãƒˆ"""
    print("\n=== ã‚¿ã‚¤ãƒˆãƒ«æ­£è¦åŒ–ãƒ†ã‚¹ãƒˆ ===")
    
    scraper = BookWalkerScraper()
    
    test_titles = [
        "ãƒ‘ãƒ©ãƒ¬ã‚¤ãƒ‰ãƒ‡ã‚¤ã‚ºâ‘£",
        "ã‚½ãƒ¼ãƒ‰ã‚¢ãƒ¼ãƒˆãƒ»ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ï¼‘",
        "ã“ã®ç´ æ™´ã‚‰ã—ã„ä¸–ç•Œã«ç¥ç¦ã‚’ï¼",
        "BOOKâ˜†WALKERé™å®šç‰ˆ",
        "é­”æ³•ç§‘é«˜æ ¡ã®åŠ£ç­‰ç”Ÿã€€ç¬¬1å·»",
        "Re:ã‚¼ãƒ­ã‹ã‚‰å§‹ã‚ã‚‹ç•°ä¸–ç•Œç”Ÿæ´»ï¼ˆï¼‘ï¼‰",
        "ã€æœŸé–“é™å®šã€‘ç‰¹å…¸ä»˜ãç‰ˆ"
    ]
    
    for title in test_titles:
        normalized = scraper.normalize_title(title)
        variants = scraper._create_bookwalker_title_variants(title)
        volume = scraper.extract_volume_number(title)
        
        print(f"åŸé¡Œ: {title}")
        print(f"æ­£è¦åŒ–: {normalized}")
        print(f"å·»æ•°: {volume}")
        print(f"ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³: {variants[:3]}")  # ä¸Šä½3ã¤ã‚’è¡¨ç¤º
        print()


async def test_scraper_initialization():
    """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼åˆæœŸåŒ–ã®åŸºæœ¬ãƒ†ã‚¹ãƒˆ"""
    print("\n=== ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ ===")
    
    print("1. ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ...")
    scraper = BookWalkerScraper(
        headless=True,
        timeout=15000,  # çŸ­ã‚ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        screenshot_dir="logs/screenshots"
    )
    print("   âœ… ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆæˆåŠŸ")
    
    print("2. åŸºæœ¬è¨­å®šç¢ºèª...")
    print(f"   ã‚µã‚¤ãƒˆå: {scraper.SITE_NAME}")
    print(f"   ãƒ™ãƒ¼ã‚¹URL: {scraper.BASE_URL}")
    print(f"   æ¤œç´¢URL: {scraper.SEARCH_URL}")
    
    print("3. ãƒ–ãƒ©ã‚¦ã‚¶åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ...")
    try:
        await scraper.initialize()
        print("   âœ… ãƒ–ãƒ©ã‚¦ã‚¶åˆæœŸåŒ–æˆåŠŸ")
        
        print("4. ç°¡å˜ãªãƒšãƒ¼ã‚¸é·ç§»ãƒ†ã‚¹ãƒˆ...")
        await scraper.page.goto("https://www.google.com", timeout=5000)
        title = await scraper.page.title()
        print(f"   âœ… ãƒšãƒ¼ã‚¸é·ç§»æˆåŠŸ: {title}")
        
        print("5. ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—...")
        await scraper.cleanup()
        print("   âœ… ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")
        
    except Exception as e:
        print(f"   âŒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        try:
            await scraper.cleanup()
        except:
            pass


async def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    Path('logs').mkdir(exist_ok=True)
    Path('logs/screenshots').mkdir(exist_ok=True)
    
    try:
        # ã‚¿ã‚¤ãƒˆãƒ«æ­£è¦åŒ–ãƒ†ã‚¹ãƒˆ
        await test_title_normalization()
        
        # åŸºæœ¬çš„ãªã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ
        await test_scraper_initialization()
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ãƒ†ã‚¹ãƒˆãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        logger.error(f"ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        print(f"ğŸ’¥ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")


if __name__ == "__main__":
    asyncio.run(main())