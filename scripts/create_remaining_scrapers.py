#!/usr/bin/env python3
"""
æ®‹ã‚Šã®å…¨ã‚µã‚¤ãƒˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ä¸€æ‹¬ä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
åŠ¹ç‡çš„ãªä¸¦è¡Œå®Ÿè£…ã®ãŸã‚
"""
import asyncio
from pathlib import Path

def create_honto_scraper():
    """hontoã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ä½œæˆ"""
    scraper_code = '''"""
honto ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
ebookjapanãƒ»BookLiveã®æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é©ç”¨
"""
import asyncio
import re
import logging
from typing import Optional, List, Dict, Any
from urllib.parse import quote
from bs4 import BeautifulSoup, Tag

from .requests_scraper import RequestsScraper

logger = logging.getLogger(__name__)


class HontoScraper(RequestsScraper):
    """honto é«˜åº¦è§£æç‰ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    SITE_NAME = "honto"
    BASE_URL = "https://honto.jp"
    SEARCH_URL = "https://honto.jp/netstore/search.html"
    
    def __init__(self, timeout: int = 10, max_retries: int = 3):
        super().__init__(timeout, max_retries, delay_between_requests=1.0)
        
    def _get_search_strategies(self, title: str, n_code: str = "") -> List[Dict[str, Any]]:
        """æ¤œç´¢æˆ¦ç•¥ãƒªã‚¹ãƒˆã®ç”Ÿæˆ"""
        strategies = []
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ
        title_variants = self._create_honto_title_variants(title)
        
        for i, variant in enumerate(title_variants):
            strategies.append({
                'query': variant,
                'description': f'ã‚¿ã‚¤ãƒˆãƒ«æ¤œç´¢ (ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ {i+1})',
                'params': {
                    'k': variant,  # hontoã§ã¯'k'ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
                    'tb': 'ebook'  # é›»å­æ›¸ç±ã®ã¿
                }
            })
        
        return strategies
    
    def _create_honto_title_variants(self, title: str) -> List[str]:
        """hontoç”¨ã‚¿ã‚¤ãƒˆãƒ«ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ"""
        variants = set()
        
        # åŸºæœ¬æ­£è¦åŒ–
        base_title = self.normalize_title(title)
        variants.add(base_title)
        variants.add(title)
        
        # å·»æ•°è¡¨è¨˜ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
        volume_variants = self._create_volume_variants_honto(title)
        variants.update(volume_variants)
        
        # éƒ¨åˆ†ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆå·»æ•°ãªã—ï¼‰
        series_only = self._extract_series_name(title)
        if series_only != title:
            variants.add(series_only)
        
        # ç©ºæ–‡å­—å‰Šé™¤
        variants = {v for v in variants if v.strip()}
        
        return list(variants)[:7]
    
    def _create_volume_variants_honto(self, title: str) -> List[str]:
        """å·»æ•°ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ"""
        variants = []
        
        # ä¸¸æ•°å­—ãƒãƒƒãƒ”ãƒ³ã‚°
        circle_to_variants = {
            'â‘ ': ['1', 'ç¬¬1å·»', '(1)', ' 1', 'ï¼‘'],
            'â‘¡': ['2', 'ç¬¬2å·»', '(2)', ' 2', 'ï¼’'],
            'â‘¢': ['3', 'ç¬¬3å·»', '(3)', ' 3', 'ï¼“'],
            'â‘£': ['4', 'ç¬¬4å·»', '(4)', ' 4', 'ï¼”'],
            'â‘¤': ['5', 'ç¬¬5å·»', '(5)', ' 5', 'ï¼•'],
        }
        
        for circle, replacements in circle_to_variants.items():
            if circle in title:
                for replacement in replacements:
                    variants.append(title.replace(circle, replacement))
        
        return variants
    
    def _extract_series_name(self, title: str) -> str:
        """ã‚·ãƒªãƒ¼ã‚ºåã®æŠ½å‡º"""  
        patterns = [
            r'[â‘ -â‘³]',
            r'ç¬¬\\d+å·»',
            r'\\d+å·»',
            r'\\(\\d+\\)',
            r'[ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼]+',
            r'[ä¸Šä¸­ä¸‹]',
            r'å‰ç·¨|å¾Œç·¨|å®Œçµç·¨',
            r'ã€[^ã€‘]*ã€‘',
        ]
        
        series_name = title
        for pattern in patterns:
            series_name = re.sub(pattern, '', series_name).strip()
        
        return series_name if series_name else title
    
    async def _search_impl(self, book_title: str, n_code: str) -> Optional[str]:
        """hontoæ¤œç´¢ã®å®Ÿè£…"""
        try:
            search_strategies = self._get_search_strategies(book_title, n_code)
            
            for i, strategy in enumerate(search_strategies, 1):
                logger.debug(f"æ¤œç´¢æˆ¦ç•¥ {i}/{len(search_strategies)}: {strategy['description']} - '{strategy['query']}'")
                
                url = await self._try_search_strategy(strategy)
                if url:
                    return url
                
                if i < len(search_strategies):
                    await asyncio.sleep(0.3)
            
            return None
            
        except Exception as e:
            logger.error(f"æ¤œç´¢å®Ÿè£…ã‚¨ãƒ©ãƒ¼: {book_title} - {str(e)}")
            return None
    
    async def _try_search_strategy(self, strategy: Dict[str, Any]) -> Optional[str]:
        """å€‹åˆ¥æ¤œç´¢æˆ¦ç•¥ã®å®Ÿè¡Œ"""
        try:
            soup = await self.make_request(self.SEARCH_URL, params=strategy['params'])
            if not soup:
                return None
            
            # é«˜åº¦ãªæ¤œç´¢çµæœæŠ½å‡º
            best_match = await self._advanced_find_best_match(soup, strategy['query'])
            return best_match
            
        except Exception as e:
            logger.warning(f"æ¤œç´¢æˆ¦ç•¥å¤±æ•— ({strategy['description']}): {str(e)}")
            return None
    
    async def _advanced_find_best_match(self, soup: BeautifulSoup, query: str) -> Optional[str]:
        """é«˜åº¦ãªæœ€é©ãƒãƒƒãƒæ¤œç´¢"""
        try:
            # Step 1: æ›¸ç±ã‚³ãƒ³ãƒ†ãƒŠã‚’æ¢ã™
            book_containers = self._find_book_containers(soup)
            
            if not book_containers:
                logger.debug("æ›¸ç±ã‚³ãƒ³ãƒ†ãƒŠãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return None
            
            logger.debug(f"{len(book_containers)} å€‹ã®æ›¸ç±ã‚³ãƒ³ãƒ†ãƒŠã‚’ç™ºè¦‹")
            
            best_match = None
            best_score = 0
            
            for i, container in enumerate(book_containers[:20]):
                try:
                    # ã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰æ›¸ç±æƒ…å ±ã‚’æŠ½å‡º
                    book_info = self._extract_book_info_from_container(container)
                    
                    if not book_info or not book_info.get('title') or not book_info.get('url'):
                        continue
                    
                    title = book_info['title']
                    url = book_info['url']
                    
                    # ã‚¹ã‚³ã‚¢è¨ˆç®—
                    score = self.calculate_similarity_score(query, title)
                    
                    # è¿½åŠ ãƒœãƒ¼ãƒŠã‚¹
                    if '/ebook/' in url:  # é›»å­æ›¸ç±ãƒšãƒ¼ã‚¸
                        score += 0.15
                    if len(title) > 5:  # ååˆ†ãªé•·ã•ã®ã‚¿ã‚¤ãƒˆãƒ«
                        score += 0.05
                    
                    logger.debug(f"æ›¸ç± {i+1}: '{title[:50]}...' -> ã‚¹ã‚³ã‚¢ {score:.3f}")
                    
                    if score > best_score and score >= 0.15:
                        best_match = url
                        best_score = score
                        
                except Exception as e:
                    logger.warning(f"ã‚³ãƒ³ãƒ†ãƒŠå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    continue
            
            if best_match:
                logger.info(f"æœ€é©ãƒãƒƒãƒç™ºè¦‹ (ã‚¹ã‚³ã‚¢: {best_score:.3f}): {best_match}")
                return best_match
            else:
                logger.warning(f"ã‚¯ã‚¨ãƒª '{query}' ã«å¯¾ã™ã‚‹é©åˆ‡ãªãƒãƒƒãƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return None
            
        except Exception as e:
            logger.error(f"é«˜åº¦ãƒãƒƒãƒãƒ³ã‚°å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return None
    
    def _find_book_containers(self, soup: BeautifulSoup) -> List[Tag]:
        """æ›¸ç±ã‚³ãƒ³ãƒ†ãƒŠè¦ç´ ã‚’ç™ºè¦‹"""
        container_selectors = [
            # hontoå›ºæœ‰ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
            '.book-item',
            '.search-result-item',
            '.book-card',
            'article',
            '.item',
            
            # ã‚ˆã‚Šåºƒç¯„å›²ãªæ¤œç´¢
            'div[class*="book"]',
            'div[class*="item"]',
            'div[class*="product"]',
            'div[class*="card"]',
            
            # ãƒªãƒ³ã‚¯ã‚’å«ã‚€è¦ç´ ã®è¦ª
            '*:has(a[href*="/ebook/"])',
        ]
        
        all_containers = []
        
        for selector in container_selectors:
            try:
                if ':has(' in selector:
                    # :has() ç–‘ä¼¼ã‚¯ãƒ©ã‚¹ã¯æ‰‹å‹•å®Ÿè£…
                    link_elements = soup.select('a[href*="/ebook/"]')
                    for link in link_elements:
                        parent = link.parent
                        if parent and parent not in all_containers:
                            all_containers.append(parent)
                else:
                    containers = soup.select(selector)
                    all_containers.extend(containers)
            except Exception as e:
                logger.debug(f"ã‚»ãƒ¬ã‚¯ã‚¿ã‚¨ãƒ©ãƒ¼ {selector}: {e}")
                continue
        
        # é‡è¤‡é™¤å»ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        unique_containers = []
        seen = set()
        
        for container in all_containers:
            container_id = id(container)
            if container_id not in seen:
                # æ›¸ç±é–¢é€£ã®ãƒªãƒ³ã‚¯ã‚’å«ã‚€ã‚³ãƒ³ãƒ†ãƒŠã®ã¿ä¿æŒ
                links = container.find_all('a', href=True)
                book_links = [link for link in links if '/ebook/' in link.get('href', '')]
                
                if book_links:
                    unique_containers.append(container)
                    seen.add(container_id)
        
        return unique_containers
    
    def _extract_book_info_from_container(self, container: Tag) -> Optional[Dict[str, str]]:
        """ã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰æ›¸ç±æƒ…å ±ã‚’æŠ½å‡º"""
        try:
            info = {}
            
            # URLã®æŠ½å‡ºï¼ˆå„ªå…ˆåº¦é †ï¼‰  
            url_selectors = [
                'a[href*="/ebook/"]',      # æœ€å„ªå…ˆï¼ˆhontoé›»å­æ›¸ç±ï¼‰
                'a[href*="/book/"]',       # æ¬¡å„ªå…ˆ
                'a[href*="/detail/"]',     # è£œåŠ©
                'a[href]'                  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            ]
            
            url = None
            url_element = None
            
            for selector in url_selectors:
                url_element = container.select_one(selector)
                if url_element and url_element.get('href'):
                    url = url_element.get('href')
                    if url.startswith('/'):
                        url = self.BASE_URL + url
                    break
            
            if not url:
                return None
            
            info['url'] = url
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã®æŠ½å‡ºï¼ˆè¤‡æ•°ã®æ–¹æ³•ã‚’è©¦è¡Œï¼‰
            title = self._extract_title_from_container(container, url_element)
            
            if not title or len(title.strip()) < 3:
                return None
                
            info['title'] = title.strip()
            
            # è‘—è€…æƒ…å ±ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            author_selectors = ['.author', '.writer', '[class*="author"]', '[class*="writer"]']
            for selector in author_selectors:
                author_element = container.select_one(selector)
                if author_element:
                    info['author'] = author_element.get_text(strip=True)
                    break
            
            return info
            
        except Exception as e:
            logger.debug(f"æ›¸ç±æƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return None
    
    def _extract_title_from_container(self, container: Tag, url_element: Tag = None) -> Optional[str]:
        """ã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡ºï¼ˆå¤šæ®µéšã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰"""
        
        # Method 1: URLãƒªãƒ³ã‚¯ã®ã‚¿ã‚¤ãƒˆãƒ«å±æ€§
        if url_element and url_element.get('title'):
            title = url_element.get('title').strip()
            if len(title) > 3 and not title.lower() in ['è©³ç´°', 'more', 'ç¶šãã‚’èª­ã‚€']:
                return title
        
        # Method 2: URLãƒªãƒ³ã‚¯ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæ„å‘³ã®ã‚ã‚‹ã‚‚ã®ï¼‰
        if url_element:
            link_text = url_element.get_text(strip=True)
            if len(link_text) > 5 and not any(word in link_text.lower() for word in ['è¦‹ã‚‹', 'more', 'è©³ç´°', 'ç¶šã']):
                return link_text
        
        # Method 3: ç‰¹å®šã®ã‚¿ã‚¤ãƒˆãƒ«ã‚»ãƒ¬ã‚¯ã‚¿
        title_selectors = [
            'h1', 'h2', 'h3', 'h4',
            '.title', '.book-title', '.product-title',
            '[class*="title"]', '[class*="name"]',
            '.heading', '.book-name'
        ]
        
        for selector in title_selectors:
            title_element = container.select_one(selector)
            if title_element:
                title = title_element.get_text(strip=True)
                if len(title) > 3 and not any(word in title.lower() for word in ['è¦‹ã‚‹', 'more', 'è©³ç´°']):
                    return title
        
        # Method 4: ã‚³ãƒ³ãƒ†ãƒŠå†…ã®æœ€ã‚‚é•·ã„ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ¼ãƒ‰
        all_texts = []
        for text_node in container.find_all(string=True):
            text = text_node.strip()
            if len(text) > 5 and not any(word in text.lower() for word in ['è¦‹ã‚‹', 'more', 'è©³ç´°', 'javascript', 'function']):
                all_texts.append(text)
        
        if all_texts:
            # æœ€ã‚‚é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’é¸æŠ
            longest_text = max(all_texts, key=len)
            if len(longest_text) > 3:
                return longest_text
        
        # Method 5: altå±æ€§ã‹ã‚‰æŠ½å‡º
        img_element = container.select_one('img[alt]')
        if img_element and img_element.get('alt'):
            alt_text = img_element.get('alt').strip()
            if len(alt_text) > 3:
                return alt_text
        
        return None
    
    async def _verify_url(self, url: str, expected_title: str) -> bool:
        """URLæ¤œè¨¼ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        try:
            if not url or not url.startswith(self.BASE_URL):
                return False
            
            # hontoã®æ›¸ç±URLãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
            valid_patterns = ['/ebook/', '/book/', '/detail/']
            return any(pattern in url for pattern in valid_patterns)
            
        except Exception as e:
            logger.error(f"URLæ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {url} - {str(e)}")
            return False
    
    def get_site_specific_headers(self) -> Dict[str, str]:
        """hontoç”¨ãƒ˜ãƒƒãƒ€ãƒ¼"""
        return {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        }
'''
    
    with open('src/scraping/honto_scraper.py', 'w', encoding='utf-8') as f:
        f.write(scraper_code)
    print("âœ… hontoã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ä½œæˆå®Œäº†")

def create_kinoppy_scraper():
    """ç´€ä¼Šåœ‹å±‹æ›¸åº—ï¼ˆKinoppyï¼‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ä½œæˆ"""  
    scraper_code = '''"""
ç´€ä¼Šåœ‹å±‹æ›¸åº—ï¼ˆKinoppyï¼‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
ebookjapanã®æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é©ç”¨
"""
import asyncio
import re
import logging
from typing import Optional, List, Dict, Any
from urllib.parse import quote
from bs4 import BeautifulSoup, Tag

from .requests_scraper import RequestsScraper

logger = logging.getLogger(__name__)


class KinoppyScraper(RequestsScraper):
    """ç´€ä¼Šåœ‹å±‹æ›¸åº—ï¼ˆKinoppyï¼‰é«˜åº¦è§£æç‰ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    SITE_NAME = "kinoppy"
    BASE_URL = "https://www.kinokuniya.co.jp"
    SEARCH_URL = "https://www.kinokuniya.co.jp/f/dsg-08-EK"
    
    def __init__(self, timeout: int = 10, max_retries: int = 3):
        super().__init__(timeout, max_retries, delay_between_requests=1.0)
        
    def _get_search_strategies(self, title: str, n_code: str = "") -> List[Dict[str, Any]]:
        """æ¤œç´¢æˆ¦ç•¥ãƒªã‚¹ãƒˆã®ç”Ÿæˆ"""
        strategies = []
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ
        title_variants = self._create_kinoppy_title_variants(title)
        
        for i, variant in enumerate(title_variants):
            strategies.append({
                'query': variant,
                'description': f'ã‚¿ã‚¤ãƒˆãƒ«æ¤œç´¢ (ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ {i+1})',
                'params': {
                    'q': variant,  # Kinoppyã§ã¯'q'ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
                    'searchtype': 'BOOK'
                }
            })
        
        return strategies
    
    def _create_kinoppy_title_variants(self, title: str) -> List[str]:
        """Kinoppyç”¨ã‚¿ã‚¤ãƒˆãƒ«ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ"""
        variants = set()
        
        # åŸºæœ¬æ­£è¦åŒ–
        base_title = self.normalize_title(title)
        variants.add(base_title)
        variants.add(title)
        
        # å·»æ•°è¡¨è¨˜ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
        volume_variants = self._create_volume_variants_kinoppy(title)
        variants.update(volume_variants)
        
        # éƒ¨åˆ†ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆå·»æ•°ãªã—ï¼‰
        series_only = self._extract_series_name(title)
        if series_only != title:
            variants.add(series_only)
        
        # ç©ºæ–‡å­—å‰Šé™¤
        variants = {v for v in variants if v.strip()}
        
        return list(variants)[:6]
    
    def _create_volume_variants_kinoppy(self, title: str) -> List[str]:
        """å·»æ•°ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ"""
        variants = []
        
        # ä¸¸æ•°å­—ãƒãƒƒãƒ”ãƒ³ã‚°
        circle_to_variants = {
            'â‘ ': ['1', 'ç¬¬1å·»', '(1)', ' 1'],
            'â‘¡': ['2', 'ç¬¬2å·»', '(2)', ' 2'],
            'â‘¢': ['3', 'ç¬¬3å·»', '(3)', ' 3'],
            'â‘£': ['4', 'ç¬¬4å·»', '(4)', ' 4'],
            'â‘¤': ['5', 'ç¬¬5å·»', '(5)', ' 5'],
        }
        
        for circle, replacements in circle_to_variants.items():
            if circle in title:
                for replacement in replacements:
                    variants.append(title.replace(circle, replacement))
        
        return variants
    
    def _extract_series_name(self, title: str) -> str:
        """ã‚·ãƒªãƒ¼ã‚ºåã®æŠ½å‡º"""  
        patterns = [
            r'[â‘ -â‘³]',
            r'ç¬¬\\d+å·»',
            r'\\d+å·»',
            r'\\(\\d+\\)',
            r'[ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼]+',
            r'[ä¸Šä¸­ä¸‹]',
            r'å‰ç·¨|å¾Œç·¨|å®Œçµç·¨',
            r'ã€[^ã€‘]*ã€‘',
        ]
        
        series_name = title
        for pattern in patterns:
            series_name = re.sub(pattern, '', series_name).strip()
        
        return series_name if series_name else title
    
    async def _search_impl(self, book_title: str, n_code: str) -> Optional[str]:
        """Kinoppyæ¤œç´¢ã®å®Ÿè£…"""
        try:
            search_strategies = self._get_search_strategies(book_title, n_code)
            
            for i, strategy in enumerate(search_strategies, 1):
                logger.debug(f"æ¤œç´¢æˆ¦ç•¥ {i}/{len(search_strategies)}: {strategy['description']} - '{strategy['query']}'")
                
                url = await self._try_search_strategy(strategy)
                if url:
                    return url
                
                if i < len(search_strategies):
                    await asyncio.sleep(0.5)  # ç´€ä¼Šåœ‹å±‹ã¯æ…é‡ã«
            
            return None
            
        except Exception as e:
            logger.error(f"æ¤œç´¢å®Ÿè£…ã‚¨ãƒ©ãƒ¼: {book_title} - {str(e)}")
            return None
    
    async def _try_search_strategy(self, strategy: Dict[str, Any]) -> Optional[str]:
        """å€‹åˆ¥æ¤œç´¢æˆ¦ç•¥ã®å®Ÿè¡Œ"""
        try:
            soup = await self.make_request(self.SEARCH_URL, params=strategy['params'])
            if not soup:
                return None
            
            # é«˜åº¦ãªæ¤œç´¢çµæœæŠ½å‡º
            best_match = await self._advanced_find_best_match(soup, strategy['query'])
            return best_match
            
        except Exception as e:
            logger.warning(f"æ¤œç´¢æˆ¦ç•¥å¤±æ•— ({strategy['description']}): {str(e)}")
            return None
    
    async def _advanced_find_best_match(self, soup: BeautifulSoup, query: str) -> Optional[str]:
        """é«˜åº¦ãªæœ€é©ãƒãƒƒãƒæ¤œç´¢"""
        try:
            # Step 1: æ›¸ç±ã‚³ãƒ³ãƒ†ãƒŠã‚’æ¢ã™
            book_containers = self._find_book_containers(soup)
            
            if not book_containers:
                logger.debug("æ›¸ç±ã‚³ãƒ³ãƒ†ãƒŠãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return None
            
            logger.debug(f"{len(book_containers)} å€‹ã®æ›¸ç±ã‚³ãƒ³ãƒ†ãƒŠã‚’ç™ºè¦‹")
            
            best_match = None
            best_score = 0
            
            for i, container in enumerate(book_containers[:15]):
                try:
                    # ã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰æ›¸ç±æƒ…å ±ã‚’æŠ½å‡º
                    book_info = self._extract_book_info_from_container(container)
                    
                    if not book_info or not book_info.get('title') or not book_info.get('url'):
                        continue
                    
                    title = book_info['title']
                    url = book_info['url']
                    
                    # ã‚¹ã‚³ã‚¢è¨ˆç®—
                    score = self.calculate_similarity_score(query, title)
                    
                    # è¿½åŠ ãƒœãƒ¼ãƒŠã‚¹
                    if 'kinokuniya.co.jp' in url:  # ç´€ä¼Šåœ‹å±‹ã‚µã‚¤ãƒˆå†…
                        score += 0.1
                    if len(title) > 5:  # ååˆ†ãªé•·ã•ã®ã‚¿ã‚¤ãƒˆãƒ«
                        score += 0.05
                    
                    logger.debug(f"æ›¸ç± {i+1}: '{title[:50]}...' -> ã‚¹ã‚³ã‚¢ {score:.3f}")
                    
                    if score > best_score and score >= 0.2:  # ã‚„ã‚„é«˜ã‚ã®é–¾å€¤
                        best_match = url
                        best_score = score
                        
                except Exception as e:
                    logger.warning(f"ã‚³ãƒ³ãƒ†ãƒŠå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    continue
            
            if best_match:
                logger.info(f"æœ€é©ãƒãƒƒãƒç™ºè¦‹ (ã‚¹ã‚³ã‚¢: {best_score:.3f}): {best_match}")
                return best_match
            else:
                logger.warning(f"ã‚¯ã‚¨ãƒª '{query}' ã«å¯¾ã™ã‚‹é©åˆ‡ãªãƒãƒƒãƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return None
            
        except Exception as e:
            logger.error(f"é«˜åº¦ãƒãƒƒãƒãƒ³ã‚°å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return None
    
    def _find_book_containers(self, soup: BeautifulSoup) -> List[Tag]:
        """æ›¸ç±ã‚³ãƒ³ãƒ†ãƒŠè¦ç´ ã‚’ç™ºè¦‹"""
        container_selectors = [
            # ç´€ä¼Šåœ‹å±‹å›ºæœ‰ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
            '.searchDetailBox',
            '.book-item',
            '.search-result-item',
            'article',
            '.item',
            
            # ã‚ˆã‚Šåºƒç¯„å›²ãªæ¤œç´¢
            'div[class*="book"]',
            'div[class*="item"]',
            'div[class*="product"]',
            'div[class*="detail"]',
            
            # ãƒªãƒ³ã‚¯ã‚’å«ã‚€è¦ç´ ã®è¦ª
            '*:has(a[href*="/dsg-"])',
        ]
        
        all_containers = []
        
        for selector in container_selectors:
            try:
                if ':has(' in selector:
                    # :has() ç–‘ä¼¼ã‚¯ãƒ©ã‚¹ã¯æ‰‹å‹•å®Ÿè£…
                    link_elements = soup.select('a[href*="/dsg-"]')
                    for link in link_elements:
                        parent = link.parent
                        if parent and parent not in all_containers:
                            all_containers.append(parent)
                else:
                    containers = soup.select(selector)
                    all_containers.extend(containers)
            except Exception as e:
                logger.debug(f"ã‚»ãƒ¬ã‚¯ã‚¿ã‚¨ãƒ©ãƒ¼ {selector}: {e}")
                continue
        
        # é‡è¤‡é™¤å»ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        unique_containers = []
        seen = set()
        
        for container in all_containers:
            container_id = id(container)
            if container_id not in seen:
                # æ›¸ç±é–¢é€£ã®ãƒªãƒ³ã‚¯ã‚’å«ã‚€ã‚³ãƒ³ãƒ†ãƒŠã®ã¿ä¿æŒ
                links = container.find_all('a', href=True)
                book_links = [link for link in links if '/dsg-' in link.get('href', '') or 'detail' in link.get('href', '')]
                
                if book_links:
                    unique_containers.append(container)
                    seen.add(container_id)
        
        return unique_containers
    
    def _extract_book_info_from_container(self, container: Tag) -> Optional[Dict[str, str]]:
        """ã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰æ›¸ç±æƒ…å ±ã‚’æŠ½å‡º"""
        try:
            info = {}
            
            # URLã®æŠ½å‡ºï¼ˆå„ªå…ˆåº¦é †ï¼‰
            url_selectors = [
                'a[href*="/dsg-"]',        # æœ€å„ªå…ˆï¼ˆç´€ä¼Šåœ‹å±‹è©³ç´°ãƒšãƒ¼ã‚¸ï¼‰
                'a[href*="/detail/"]',     # æ¬¡å„ªå…ˆ
                'a[href*="/book/"]',       # è£œåŠ©
                'a[href]'                  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            ]
            
            url = None
            url_element = None
            
            for selector in url_selectors:
                url_element = container.select_one(selector)
                if url_element and url_element.get('href'):
                    url = url_element.get('href')
                    if url.startswith('/'):
                        url = self.BASE_URL + url
                    break
            
            if not url:
                return None
            
            info['url'] = url
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã®æŠ½å‡ºï¼ˆè¤‡æ•°ã®æ–¹æ³•ã‚’è©¦è¡Œï¼‰
            title = self._extract_title_from_container(container, url_element)
            
            if not title or len(title.strip()) < 3:
                return None
                
            info['title'] = title.strip()
            
            return info
            
        except Exception as e:
            logger.debug(f"æ›¸ç±æƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return None
    
    def _extract_title_from_container(self, container: Tag, url_element: Tag = None) -> Optional[str]:
        """ã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡ºï¼ˆå¤šæ®µéšã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰"""
        
        # Method 1: URLãƒªãƒ³ã‚¯ã®ã‚¿ã‚¤ãƒˆãƒ«å±æ€§
        if url_element and url_element.get('title'):
            title = url_element.get('title').strip()
            if len(title) > 3:
                return title
        
        # Method 2: URLãƒªãƒ³ã‚¯ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæ„å‘³ã®ã‚ã‚‹ã‚‚ã®ï¼‰
        if url_element:
            link_text = url_element.get_text(strip=True)
            if len(link_text) > 5:
                return link_text
        
        # Method 3: ç‰¹å®šã®ã‚¿ã‚¤ãƒˆãƒ«ã‚»ãƒ¬ã‚¯ã‚¿
        title_selectors = [
            'h1', 'h2', 'h3', 'h4',
            '.title', '.book-title', '.product-title',
            '[class*="title"]', '[class*="name"]',
            '.heading', '.book-name'
        ]
        
        for selector in title_selectors:
            title_element = container.select_one(selector)
            if title_element:
                title = title_element.get_text(strip=True)
                if len(title) > 3:
                    return title
        
        # Method 4: ã‚³ãƒ³ãƒ†ãƒŠå†…ã®æœ€ã‚‚é•·ã„ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ¼ãƒ‰
        all_texts = []
        for text_node in container.find_all(string=True):
            text = text_node.strip()
            if len(text) > 5:
                all_texts.append(text)
        
        if all_texts:
            # æœ€ã‚‚é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’é¸æŠ
            longest_text = max(all_texts, key=len)
            if len(longest_text) > 3:
                return longest_text
        
        return None
    
    async def _verify_url(self, url: str, expected_title: str) -> bool:
        """URLæ¤œè¨¼ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        try:
            if not url or not url.startswith(self.BASE_URL):
                return False
            
            # ç´€ä¼Šåœ‹å±‹ã®æ›¸ç±URLãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
            valid_patterns = ['/dsg-', '/detail/', '/book/']
            return any(pattern in url for pattern in valid_patterns)
            
        except Exception as e:
            logger.error(f"URLæ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {url} - {str(e)}")
            return False
    
    def get_site_specific_headers(self) -> Dict[str, str]:
        """ç´€ä¼Šåœ‹å±‹ç”¨ãƒ˜ãƒƒãƒ€ãƒ¼"""
        return {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
            'Referer': 'https://www.kinokuniya.co.jp/',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
'''
    
    with open('src/scraping/kinoppy_scraper.py', 'w', encoding='utf-8') as f:
        f.write(scraper_code)
    print("âœ… Kinoppyã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ä½œæˆå®Œäº†")

def create_reader_store_scraper():
    """Reader Storeï¼ˆSonyï¼‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ä½œæˆ"""
    scraper_code = '''"""
Reader Storeï¼ˆSonyï¼‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
ebookjapanã®æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é©ç”¨
"""
import asyncio
import re
import logging
from typing import Optional, List, Dict, Any
from urllib.parse import quote
from bs4 import BeautifulSoup, Tag

from .requests_scraper import RequestsScraper

logger = logging.getLogger(__name__)


class ReaderStoreScraper(RequestsScraper):
    """Reader Storeï¼ˆSonyï¼‰é«˜åº¦è§£æç‰ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    SITE_NAME = "reader_store"
    BASE_URL = "https://store.sony.jp"
    SEARCH_URL = "https://store.sony.jp/search"
    
    def __init__(self, timeout: int = 10, max_retries: int = 3):
        super().__init__(timeout, max_retries, delay_between_requests=1.2)
        
    def _get_search_strategies(self, title: str, n_code: str = "") -> List[Dict[str, Any]]:
        """æ¤œç´¢æˆ¦ç•¥ãƒªã‚¹ãƒˆã®ç”Ÿæˆ"""
        strategies = []
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ
        title_variants = self._create_reader_store_title_variants(title)
        
        for i, variant in enumerate(title_variants):
            strategies.append({
                'query': variant,
                'description': f'ã‚¿ã‚¤ãƒˆãƒ«æ¤œç´¢ (ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ {i+1})',
                'params': {
                    'keyword': variant,  # Reader Storeã§ã¯'keyword'ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
                    'category': 'ebook'
                }
            })
        
        return strategies
    
    def _create_reader_store_title_variants(self, title: str) -> List[str]:
        """Reader Storeç”¨ã‚¿ã‚¤ãƒˆãƒ«ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ"""
        variants = set()
        
        # åŸºæœ¬æ­£è¦åŒ–
        base_title = self.normalize_title(title)
        variants.add(base_title)
        variants.add(title)
        
        # å·»æ•°è¡¨è¨˜ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
        volume_variants = self._create_volume_variants_reader_store(title)
        variants.update(volume_variants)
        
        # éƒ¨åˆ†ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆå·»æ•°ãªã—ï¼‰
        series_only = self._extract_series_name(title)
        if series_only != title:
            variants.add(series_only)
        
        # ç©ºæ–‡å­—å‰Šé™¤
        variants = {v for v in variants if v.strip()}
        
        return list(variants)[:6]
    
    def _create_volume_variants_reader_store(self, title: str) -> List[str]:
        """å·»æ•°ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ"""
        variants = []
        
        # ä¸¸æ•°å­—ãƒãƒƒãƒ”ãƒ³ã‚°
        circle_to_variants = {
            'â‘ ': ['1', 'ç¬¬1å·»', '(1)', ' 1'],
            'â‘¡': ['2', 'ç¬¬2å·»', '(2)', ' 2'],
            'â‘¢': ['3', 'ç¬¬3å·»', '(3)', ' 3'],
            'â‘£': ['4', 'ç¬¬4å·»', '(4)', ' 4'],
            'â‘¤': ['5', 'ç¬¬5å·»', '(5)', ' 5'],
        }
        
        for circle, replacements in circle_to_variants.items():
            if circle in title:
                for replacement in replacements:
                    variants.append(title.replace(circle, replacement))
        
        return variants
    
    def _extract_series_name(self, title: str) -> str:
        """ã‚·ãƒªãƒ¼ã‚ºåã®æŠ½å‡º"""  
        patterns = [
            r'[â‘ -â‘³]',
            r'ç¬¬\\d+å·»',
            r'\\d+å·»',
            r'\\(\\d+\\)',
            r'[ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼]+',
            r'[ä¸Šä¸­ä¸‹]',
            r'å‰ç·¨|å¾Œç·¨|å®Œçµç·¨',
            r'ã€[^ã€‘]*ã€‘',
        ]
        
        series_name = title
        for pattern in patterns:
            series_name = re.sub(pattern, '', series_name).strip()
        
        return series_name if series_name else title
    
    async def _search_impl(self, book_title: str, n_code: str) -> Optional[str]:
        """Reader Storeæ¤œç´¢ã®å®Ÿè£…"""
        try:
            search_strategies = self._get_search_strategies(book_title, n_code)
            
            for i, strategy in enumerate(search_strategies, 1):
                logger.debug(f"æ¤œç´¢æˆ¦ç•¥ {i}/{len(search_strategies)}: {strategy['description']} - '{strategy['query']}'")
                
                url = await self._try_search_strategy(strategy)
                if url:
                    return url
                
                if i < len(search_strategies):
                    await asyncio.sleep(0.4)  # Sonyé…æ…®
            
            return None
            
        except Exception as e:
            logger.error(f"æ¤œç´¢å®Ÿè£…ã‚¨ãƒ©ãƒ¼: {book_title} - {str(e)}")
            return None
    
    async def _try_search_strategy(self, strategy: Dict[str, Any]) -> Optional[str]:
        """å€‹åˆ¥æ¤œç´¢æˆ¦ç•¥ã®å®Ÿè¡Œ"""
        try:
            soup = await self.make_request(self.SEARCH_URL, params=strategy['params'])
            if not soup:
                return None
            
            # é«˜åº¦ãªæ¤œç´¢çµæœæŠ½å‡º
            best_match = await self._advanced_find_best_match(soup, strategy['query'])
            return best_match
            
        except Exception as e:
            logger.warning(f"æ¤œç´¢æˆ¦ç•¥å¤±æ•— ({strategy['description']}): {str(e)}")
            return None
    
    async def _advanced_find_best_match(self, soup: BeautifulSoup, query: str) -> Optional[str]:
        """é«˜åº¦ãªæœ€é©ãƒãƒƒãƒæ¤œç´¢"""
        try:
            # Step 1: æ›¸ç±ã‚³ãƒ³ãƒ†ãƒŠã‚’æ¢ã™
            book_containers = self._find_book_containers(soup)
            
            if not book_containers:
                logger.debug("æ›¸ç±ã‚³ãƒ³ãƒ†ãƒŠãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return None
            
            logger.debug(f"{len(book_containers)} å€‹ã®æ›¸ç±ã‚³ãƒ³ãƒ†ãƒŠã‚’ç™ºè¦‹")
            
            best_match = None
            best_score = 0
            
            for i, container in enumerate(book_containers[:15]):
                try:
                    # ã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰æ›¸ç±æƒ…å ±ã‚’æŠ½å‡º
                    book_info = self._extract_book_info_from_container(container)
                    
                    if not book_info or not book_info.get('title') or not book_info.get('url'):
                        continue
                    
                    title = book_info['title']
                    url = book_info['url']
                    
                    # ã‚¹ã‚³ã‚¢è¨ˆç®—
                    score = self.calculate_similarity_score(query, title)
                    
                    # è¿½åŠ ãƒœãƒ¼ãƒŠã‚¹
                    if 'store.sony.jp' in url:  # Sonyå…¬å¼ã‚µã‚¤ãƒˆå†…
                        score += 0.1
                    if len(title) > 5:  # ååˆ†ãªé•·ã•ã®ã‚¿ã‚¤ãƒˆãƒ«
                        score += 0.05
                    
                    logger.debug(f"æ›¸ç± {i+1}: '{title[:50]}...' -> ã‚¹ã‚³ã‚¢ {score:.3f}")
                    
                    if score > best_score and score >= 0.2:
                        best_match = url
                        best_score = score
                        
                except Exception as e:
                    logger.warning(f"ã‚³ãƒ³ãƒ†ãƒŠå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    continue
            
            if best_match:
                logger.info(f"æœ€é©ãƒãƒƒãƒç™ºè¦‹ (ã‚¹ã‚³ã‚¢: {best_score:.3f}): {best_match}")
                return best_match
            else:
                logger.warning(f"ã‚¯ã‚¨ãƒª '{query}' ã«å¯¾ã™ã‚‹é©åˆ‡ãªãƒãƒƒãƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return None
            
        except Exception as e:
            logger.error(f"é«˜åº¦ãƒãƒƒãƒãƒ³ã‚°å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return None
    
    def _find_book_containers(self, soup: BeautifulSoup) -> List[Tag]:
        """æ›¸ç±ã‚³ãƒ³ãƒ†ãƒŠè¦ç´ ã‚’ç™ºè¦‹"""
        container_selectors = [
            # Sony Reader Storeå›ºæœ‰ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
            '.product-item',
            '.book-item',
            '.search-result-item',
            'article',
            '.item',
            
            # ã‚ˆã‚Šåºƒç¯„å›²ãªæ¤œç´¢
            'div[class*="book"]',
            'div[class*="item"]',
            'div[class*="product"]',
            'div[class*="card"]',
            
            # ãƒªãƒ³ã‚¯ã‚’å«ã‚€è¦ç´ ã®è¦ª
            '*:has(a[href*="/item/"])',
        ]
        
        all_containers = []
        
        for selector in container_selectors:
            try:
                if ':has(' in selector:
                    # :has() ç–‘ä¼¼ã‚¯ãƒ©ã‚¹ã¯æ‰‹å‹•å®Ÿè£…
                    link_elements = soup.select('a[href*="/item/"]')
                    for link in link_elements:
                        parent = link.parent
                        if parent and parent not in all_containers:
                            all_containers.append(parent)
                else:
                    containers = soup.select(selector)
                    all_containers.extend(containers)
            except Exception as e:
                logger.debug(f"ã‚»ãƒ¬ã‚¯ã‚¿ã‚¨ãƒ©ãƒ¼ {selector}: {e}")
                continue
        
        # é‡è¤‡é™¤å»ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        unique_containers = []
        seen = set()
        
        for container in all_containers:
            container_id = id(container)
            if container_id not in seen:
                # æ›¸ç±é–¢é€£ã®ãƒªãƒ³ã‚¯ã‚’å«ã‚€ã‚³ãƒ³ãƒ†ãƒŠã®ã¿ä¿æŒ
                links = container.find_all('a', href=True)
                book_links = [link for link in links if '/item/' in link.get('href', '')]
                
                if book_links:
                    unique_containers.append(container)
                    seen.add(container_id)
        
        return unique_containers
    
    def _extract_book_info_from_container(self, container: Tag) -> Optional[Dict[str, str]]:
        """ã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰æ›¸ç±æƒ…å ±ã‚’æŠ½å‡º"""
        try:
            info = {}
            
            # URLã®æŠ½å‡ºï¼ˆå„ªå…ˆåº¦é †ï¼‰
            url_selectors = [
                'a[href*="/item/"]',       # æœ€å„ªå…ˆï¼ˆSonyå•†å“ãƒšãƒ¼ã‚¸ï¼‰
                'a[href*="/product/"]',    # æ¬¡å„ªå…ˆ
                'a[href*="/detail/"]',     # è£œåŠ©
                'a[href]'                  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            ]
            
            url = None
            url_element = None
            
            for selector in url_selectors:
                url_element = container.select_one(selector)
                if url_element and url_element.get('href'):
                    url = url_element.get('href')
                    if url.startswith('/'):
                        url = self.BASE_URL + url
                    break
            
            if not url:
                return None
            
            info['url'] = url
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã®æŠ½å‡ºï¼ˆè¤‡æ•°ã®æ–¹æ³•ã‚’è©¦è¡Œï¼‰
            title = self._extract_title_from_container(container, url_element)
            
            if not title or len(title.strip()) < 3:
                return None
                
            info['title'] = title.strip()
            
            return info
            
        except Exception as e:
            logger.debug(f"æ›¸ç±æƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return None
    
    def _extract_title_from_container(self, container: Tag, url_element: Tag = None) -> Optional[str]:
        """ã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡ºï¼ˆå¤šæ®µéšã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰"""
        
        # Method 1: URLãƒªãƒ³ã‚¯ã®ã‚¿ã‚¤ãƒˆãƒ«å±æ€§
        if url_element and url_element.get('title'):
            title = url_element.get('title').strip()
            if len(title) > 3:
                return title
        
        # Method 2: URLãƒªãƒ³ã‚¯ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæ„å‘³ã®ã‚ã‚‹ã‚‚ã®ï¼‰
        if url_element:
            link_text = url_element.get_text(strip=True)
            if len(link_text) > 5:
                return link_text
        
        # Method 3: ç‰¹å®šã®ã‚¿ã‚¤ãƒˆãƒ«ã‚»ãƒ¬ã‚¯ã‚¿
        title_selectors = [
            'h1', 'h2', 'h3', 'h4',
            '.title', '.book-title', '.product-title',
            '[class*="title"]', '[class*="name"]',
            '.heading', '.book-name'
        ]
        
        for selector in title_selectors:
            title_element = container.select_one(selector)
            if title_element:
                title = title_element.get_text(strip=True)
                if len(title) > 3:
                    return title
        
        # Method 4: ã‚³ãƒ³ãƒ†ãƒŠå†…ã®æœ€ã‚‚é•·ã„ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ¼ãƒ‰
        all_texts = []
        for text_node in container.find_all(string=True):
            text = text_node.strip()
            if len(text) > 5:
                all_texts.append(text)
        
        if all_texts:
            # æœ€ã‚‚é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’é¸æŠ
            longest_text = max(all_texts, key=len)
            if len(longest_text) > 3:
                return longest_text
        
        return None
    
    async def _verify_url(self, url: str, expected_title: str) -> bool:
        """URLæ¤œè¨¼ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        try:
            if not url or not url.startswith(self.BASE_URL):
                return False
            
            # Sony Reader Storeã®æ›¸ç±URLãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
            valid_patterns = ['/item/', '/product/', '/detail/']
            return any(pattern in url for pattern in valid_patterns)
            
        except Exception as e:
            logger.error(f"URLæ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {url} - {str(e)}")
            return False
    
    def get_site_specific_headers(self) -> Dict[str, str]:
        """Sony Reader Storeç”¨ãƒ˜ãƒƒãƒ€ãƒ¼"""
        return {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
            'Referer': 'https://store.sony.jp/',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
'''
    
    with open('src/scraping/reader_store_scraper.py', 'w', encoding='utf-8') as f:
        f.write(scraper_code)
    print("âœ… Reader Storeã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ä½œæˆå®Œäº†")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç† - å…¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ä½œæˆ"""
    print("=== æ®‹ã‚Šã‚µã‚¤ãƒˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ä¸€æ‹¬ä½œæˆé–‹å§‹ ===")
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç¢ºèª
    Path('src/scraping').mkdir(parents=True, exist_ok=True)
    
    # å…¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ä½œæˆ
    create_honto_scraper()
    create_kinoppy_scraper()
    create_reader_store_scraper()
    
    print("âœ… å…¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ä½œæˆå®Œäº†ï¼")
    print("\nğŸ“Š ä½œæˆã•ã‚ŒãŸã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼:")
    print("  - honto")
    print("  - ç´€ä¼Šåœ‹å±‹æ›¸åº—ï¼ˆKinoppyï¼‰")
    print("  - Reader Storeï¼ˆSonyï¼‰")
    print("\nğŸ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("  1. å„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
    print("  2. æˆåŠŸç‡ã®ç¢ºèªãƒ»èª¿æ•´")
    print("  3. Amazon PODå®Ÿè£…")
    print("  4. Apple Books iTunes APIèª¿æ•´")

if __name__ == "__main__":
    main()