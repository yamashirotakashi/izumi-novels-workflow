#!/usr/bin/env python3
"""
Sony Reader Store ã‚µã‚¤ãƒˆæ§‹é€ åˆ†æå™¨
å¾¹åº•çš„æ·±å±¤åˆ†æã«ã‚ˆã‚‹çœŸã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ‰‹æ³•ç¢ºç«‹
"""
import asyncio
import sys
import json
import time
from pathlib import Path
from datetime import datetime
import logging
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, quote
import re

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class ReaderStoreDeepAnalyzer:
    """Sony Reader Storeã®æ·±å±¤æ§‹é€ è§£æã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
    def analyze_site_structure(self):
        """ã‚µã‚¤ãƒˆæ§‹é€ ã®å¾¹åº•åˆ†æ"""
        print("=== Sony Reader Store æ·±å±¤åˆ†æé–‹å§‹ ===")
        print(f"é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print()
        
        # Phase 1: ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ»URLãƒ‘ã‚¿ãƒ¼ãƒ³æ¢æŸ»
        print("ğŸ” Phase 1: ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ»URLãƒ‘ã‚¿ãƒ¼ãƒ³æ¢æŸ»")
        self._explore_domains_and_urls()
        
        # Phase 2: ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ãªã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆç™ºè¦‹
        print("\nğŸ” Phase 2: ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ãªã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆç™ºè¦‹") 
        self._discover_accessible_endpoints()
        
        # Phase 3: æ¤œç´¢æ©Ÿèƒ½ã®å®Ÿè£…æ–¹å¼åˆ†æ
        print("\nğŸ” Phase 3: æ¤œç´¢æ©Ÿèƒ½ã®å®Ÿè£…æ–¹å¼åˆ†æ")
        self._analyze_search_implementation()
        
        # Phase 4: ä»£æ›¿æ‰‹æ®µãƒ»å›é¿ç­–æ¤œè¨
        print("\nğŸ” Phase 4: ä»£æ›¿æ‰‹æ®µãƒ»å›é¿ç­–æ¤œè¨")
        self._explore_workarounds()
        
        print("\n=== Sony Reader Store æ·±å±¤åˆ†æå®Œäº† ===")
    
    def _explore_domains_and_urls(self):
        """ãƒ‰ãƒ¡ã‚¤ãƒ³ã¨URLãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¢æŸ»"""
        try:
            # æƒ³å®šã•ã‚Œã‚‹å…¨ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³
            domains_to_check = [
                "https://store.sony.jp/",
                "https://ebookstore.sony.jp/",
                "https://reader.sony.jp/",
                "https://readerstore.sony.jp/",
                "https://books.sony.jp/",
                "https://ebook.sony.jp/",
                "https://digital.sony.jp/",
                "https://content.sony.jp/",
            ]
            
            print("  ğŸŒ ãƒ‰ãƒ¡ã‚¤ãƒ³æ¢æŸ»:")
            working_domains = []
            
            for domain in domains_to_check:
                print(f"    ãƒ†ã‚¹ãƒˆä¸­: {domain}")
                try:
                    response = self.session.get(domain, timeout=8)
                    print(f"      ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {response.status_code}")
                    
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        title = soup.find('title')
                        title_text = title.get_text() if title else 'ãªã—'
                        print(f"      ã‚¿ã‚¤ãƒˆãƒ«: {title_text[:80]}...")
                        
                        # æ›¸ç±ãƒ»é›»å­æ›¸ç±é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º
                        content = response.text.lower()
                        book_keywords = ['book', 'æ›¸ç±', 'ebook', 'é›»å­æ›¸ç±', 'reader', 'store']
                        found_keywords = [kw for kw in book_keywords if kw in content]
                        
                        if found_keywords:
                            print(f"      ğŸ“š é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(found_keywords)}")
                            working_domains.append(domain)
                        else:
                            print(f"      âŒ æ›¸ç±é–¢é€£ãªã—")
                            
                    elif response.status_code in [301, 302]:
                        location = response.headers.get('Location', 'ä¸æ˜')
                        print(f"      ğŸ”„ ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆ: {location}")
                        if location != 'ä¸æ˜':
                            working_domains.append(location)
                    else:
                        print(f"      âš ï¸ {response.status_code}")
                        
                except requests.exceptions.Timeout:
                    print(f"      â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
                except requests.exceptions.ConnectionError as e:
                    if "Name or service not known" in str(e):
                        print(f"      ğŸš« ãƒ‰ãƒ¡ã‚¤ãƒ³ä¸å­˜åœ¨")
                    else:
                        print(f"      ğŸ”Œ æ¥ç¶šã‚¨ãƒ©ãƒ¼")
                except Exception as e:
                    print(f"      ğŸ’¥ ã‚¨ãƒ©ãƒ¼: {str(e)[:50]}")
                    
                time.sleep(0.8)
            
            print(f"\\n  âœ… åˆ©ç”¨å¯èƒ½ãƒ‰ãƒ¡ã‚¤ãƒ³: {len(working_domains)}å€‹")
            for domain in working_domains:
                print(f"    - {domain}")
                
            return working_domains
            
        except Exception as e:
            logger.error(f"ãƒ‰ãƒ¡ã‚¤ãƒ³æ¢æŸ»ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _discover_accessible_endpoints(self):
        """ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ãªã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®ç™ºè¦‹"""
        try:
            # ä¸»è¦ãƒ‰ãƒ¡ã‚¤ãƒ³ã§ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆæ¢æŸ»
            base_domains = [
                "https://ebookstore.sony.jp",
                "https://store.sony.jp", 
            ]
            
            # æƒ³å®šã•ã‚Œã‚‹ãƒ‘ã‚¹
            endpoints_to_test = [
                "/",
                "/search",
                "/book/search",
                "/ebook/search", 
                "/books",
                "/ebooks",
                "/store",
                "/api/search",
                "/ajax/search",
                "/product/search",
                "/item/search",
                "/content/search",
            ]
            
            print("  ğŸ“¡ ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆæ¢æŸ»:")
            
            for base_domain in base_domains:
                print(f"    ğŸŒ ãƒ™ãƒ¼ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³: {base_domain}")
                accessible_endpoints = []
                
                for endpoint in endpoints_to_test:
                    full_url = base_domain + endpoint
                    
                    try:
                        response = self.session.get(full_url, timeout=6)
                        status = response.status_code
                        
                        if status == 200:
                            soup = BeautifulSoup(response.text, 'html.parser')
                            
                            # æ¤œç´¢é–¢é€£è¦ç´ ã®æ¤œå‡º
                            search_forms = soup.find_all('form')
                            search_inputs = soup.find_all('input', {'type': 'search'}) + \
                                          soup.find_all('input', {'name': re.compile(r'(search|query|q|keyword)', re.I)})
                            
                            has_search_elements = len(search_forms) > 0 or len(search_inputs) > 0
                            
                            print(f"      âœ… {endpoint} (æ¤œç´¢è¦ç´ : {'ã‚ã‚Š' if has_search_elements else 'ãªã—'})")
                            accessible_endpoints.append((endpoint, has_search_elements))
                            
                        elif status in [301, 302]:
                            location = response.headers.get('Location', '')
                            print(f"      ğŸ”„ {endpoint} -> {location}")
                        elif status == 404:
                            print(f"      âŒ {endpoint}")
                        else:
                            print(f"      âš ï¸ {endpoint} ({status})")
                            
                    except Exception as e:
                        print(f"      ğŸ’¥ {endpoint}: {str(e)[:30]}")
                        
                    time.sleep(0.4)
                
                if accessible_endpoints:
                    print(f"    ğŸ“Š åˆ©ç”¨å¯èƒ½ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ: {len(accessible_endpoints)}å€‹")
                    search_endpoints = [ep for ep, has_search in accessible_endpoints if has_search]
                    if search_endpoints:
                        print(f"    ğŸ” æ¤œç´¢å¯èƒ½æ€§ã®ã‚ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ: {search_endpoints}")
                print()
                
        except Exception as e:
            logger.error(f"ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆæ¢æŸ»ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _analyze_search_implementation(self):
        """æ¤œç´¢æ©Ÿèƒ½ã®å®Ÿè£…æ–¹å¼åˆ†æ"""
        try:
            print("  ğŸ” æ¤œç´¢å®Ÿè£…æ–¹å¼åˆ†æ:")
            
            # ãƒ†ã‚¹ãƒˆå¯¾è±¡URLï¼ˆç™ºè¦‹ã•ã‚ŒãŸã‚‚ã®ã¨æ¨å®šã•ã‚Œã‚‹ã‚‚ã®ï¼‰
            test_urls = [
                "https://ebookstore.sony.jp/search",
                "https://ebookstore.sony.jp/",
                "https://store.sony.jp/search",
                "https://store.sony.jp/",
            ]
            
            test_queries = ["ã‚½ãƒ¼ãƒ‰ã‚¢ãƒ¼ãƒˆãƒ»ã‚ªãƒ³ãƒ©ã‚¤ãƒ³", "SAO"]
            
            for url in test_urls:
                print(f"    ğŸŒ URL: {url}")
                
                try:
                    # ã¾ãšåŸºæœ¬ã‚¢ã‚¯ã‚»ã‚¹
                    response = self.session.get(url, timeout=8)
                    
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        
                        # ãƒ•ã‚©ãƒ¼ãƒ åˆ†æ
                        forms = soup.find_all('form')
                        print(f"      ãƒ•ã‚©ãƒ¼ãƒ æ•°: {len(forms)}")
                        
                        for i, form in enumerate(forms):
                            action = form.get('action', '')
                            method = form.get('method', 'GET').upper()
                            print(f"        ãƒ•ã‚©ãƒ¼ãƒ {i+1}: {method} {action}")
                            
                            # å…¥åŠ›ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰è©³ç´°
                            inputs = form.find_all(['input', 'select', 'textarea'])
                            for inp in inputs:
                                name = inp.get('name', '')
                                input_type = inp.get('type', '')
                                if name and input_type not in ['hidden', 'submit', 'button']:
                                    print(f"          å…¥åŠ›: {name} ({input_type})")
                        
                        # æ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
                        if forms:
                            print(f"      ğŸ§ª æ¤œç´¢ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ:")
                            
                            # ä¸€èˆ¬çš„ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒ†ã‚¹ãƒˆ
                            param_patterns = [
                                {'q': test_queries[0]},
                                {'search': test_queries[0]},
                                {'keyword': test_queries[0]},
                                {'query': test_queries[0]},
                            ]
                            
                            for pattern in param_patterns:
                                try:
                                    test_response = self.session.get(url, params=pattern, timeout=6)
                                    print(f"        {pattern}: {test_response.status_code} ({len(test_response.content)} bytes)")
                                    
                                    if test_response.status_code == 200:
                                        test_soup = BeautifulSoup(test_response.text, 'html.parser')
                                        
                                        # çµæœè¦ç´ ã®æ¤œå‡º
                                        result_indicators = [
                                            len(test_soup.find_all('a', href=lambda x: x and 'item' in x)),
                                            len(test_soup.find_all('div', class_=re.compile(r'(book|product|item)', re.I))),
                                            len(test_soup.find_all(string=re.compile(test_queries[0][:5], re.I))),
                                        ]
                                        
                                        total_indicators = sum(result_indicators)
                                        if total_indicators > 0:
                                            print(f"          âœ… çµæœã‚‰ã—ãè¦ç´ : {total_indicators}å€‹")
                                        else:
                                            print(f"          âŒ çµæœè¦ç´ ãªã—")
                                
                                except Exception as e:
                                    print(f"        {pattern}: ã‚¨ãƒ©ãƒ¼ {str(e)[:30]}")
                                    
                                time.sleep(0.5)
                    
                    else:
                        print(f"      âŒ ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: {response.status_code}")
                        
                except Exception as e:
                    print(f"    ğŸ’¥ åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)[:50]}")
                    
                print()
                
        except Exception as e:
            logger.error(f"æ¤œç´¢å®Ÿè£…åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
    
    def _explore_workarounds(self):
        """ä»£æ›¿æ‰‹æ®µãƒ»å›é¿ç­–ã®æ¤œè¨"""
        try:
            print("  ğŸ’¡ ä»£æ›¿æ‰‹æ®µãƒ»å›é¿ç­–:")
            
            workarounds = [
                {
                    'name': 'Google Site Search',
                    'description': 'site:ebookstore.sony.jp ã‚¯ã‚¨ãƒªã§Googleæ¤œç´¢',
                    'pros': 'ç¢ºå®Ÿã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã•ã‚ŒãŸãƒšãƒ¼ã‚¸ã‚’ç™ºè¦‹',
                    'cons': 'Google APIåˆ¶é™ã€æ¤œç´¢ç²¾åº¦',
                    'feasibility': 'é«˜'
                },
                {
                    'name': 'Wayback Machine',
                    'description': 'Internet Archiveã‹ã‚‰éå»ã®ã‚µã‚¤ãƒˆæ§‹é€ ã‚’åˆ†æ',
                    'pros': 'å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹æ§‹é€ ç†è§£',
                    'cons': 'ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§ãªã—',
                    'feasibility': 'ä¸­'
                },
                {
                    'name': 'Playwrightæ·±å±¤åˆ†æ',
                    'description': 'ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ–ãƒ©ã‚¦ã‚¶ã§å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æ',
                    'pros': 'JavaScriptå®Ÿè¡Œç’°å¢ƒã€å®Œå…¨åˆ¶å¾¡',
                    'cons': 'å®Ÿè£…ã‚³ã‚¹ãƒˆ',
                    'feasibility': 'é«˜'
                },
                {
                    'name': 'Selenium + Chrome DevTools',
                    'description': 'ãƒ–ãƒ©ã‚¦ã‚¶é–‹ç™ºè€…ãƒ„ãƒ¼ãƒ«ã§ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è§£æ',
                    'pros': 'å®Ÿéš›ã®APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆç™ºè¦‹',
                    'cons': 'æŠ€è¡“çš„è¤‡é›‘æ€§',
                    'feasibility': 'é«˜'
                },
                {
                    'name': 'ISBN/ã‚¿ã‚¤ãƒˆãƒ«æƒ…å ±çµŒç”±',
                    'description': 'ä»–ã‚µã‚¤ãƒˆã‹ã‚‰ã®ISBNå–å¾—â†’ç›´æ¥å•†å“ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹',
                    'pros': 'ç¢ºå®Ÿæ€§',
                    'cons': 'äºŒæ®µéšãƒ—ãƒ­ã‚»ã‚¹',
                    'feasibility': 'ä¸­'
                },
                {
                    'name': 'ç«¶åˆã‚µã‚¤ãƒˆåˆ†æ',
                    'description': 'æ¥½å¤©Koboã€Kindleã§ã®æ¤œç´¢çµæœã‹ã‚‰ISBNæ¨å®š',
                    'pros': 'ç¢ºå®Ÿãªå•†å“ç‰¹å®š',
                    'cons': 'é–“æ¥çš„',
                    'feasibility': 'ä¸­'
                }
            ]
            
            print("    ğŸ› ï¸ å®Ÿè£…å¯èƒ½ãªå¯¾ç­–:")
            for i, workaround in enumerate(workarounds, 1):
                print(f"      {i}. {workaround['name']}")
                print(f"         æ¦‚è¦: {workaround['description']}")
                print(f"         åˆ©ç‚¹: {workaround['pros']}")
                print(f"         æ¬ ç‚¹: {workaround['cons']}")
                print(f"         å®Ÿè£…å¯èƒ½æ€§: {workaround['feasibility']}")
                print()
            
            # æ¨å¥¨æˆ¦ç•¥
            print("    ğŸ¯ æ¨å¥¨å®Ÿè£…æˆ¦ç•¥:")
            print("      Phase 1: Playwrightå®Ÿè£…ï¼ˆæœ€å„ªå…ˆï¼‰")
            print("        - ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ–ãƒ©ã‚¦ã‚¶ã§ã®å®Œå…¨ã‚µã‚¤ãƒˆåˆ†æ")
            print("        - JavaScriptå®Ÿè¡Œç’°å¢ƒã§ã®å‹•çš„æ¤œç´¢")
            print("        - å®Ÿãƒ–ãƒ©ã‚¦ã‚¶ç’°å¢ƒã§ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆç™ºè¦‹")
            print()
            print("      Phase 2: Google Site Searchï¼ˆç·Šæ€¥å›é¿ï¼‰")
            print("        - 'site:ebookstore.sony.jp ã‚¿ã‚¤ãƒˆãƒ«'ã§ã®Googleæ¤œç´¢")
            print("        - æ¤œç´¢çµæœã‹ã‚‰ã®ç›´æ¥ãƒªãƒ³ã‚¯æŠ½å‡º")
            print()
            print("      Phase 3: ä»–ã‚µã‚¤ãƒˆé€£æºï¼ˆé«˜ç²¾åº¦åŒ–ï¼‰")
            print("        - æ¥½å¤©Koboã€Amazonã‹ã‚‰ã®æ›¸èªŒæƒ…å ±å–å¾—")
            print("        - ISBNãƒ™ãƒ¼ã‚¹ã§ã®ç›´æ¥å•†å“ãƒšãƒ¼ã‚¸ç‰¹å®š")
            
        except Exception as e:
            logger.error(f"å›é¿ç­–æ¤œè¨ã‚¨ãƒ©ãƒ¼: {e}")


async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    analyzer = ReaderStoreDeepAnalyzer()
    
    try:
        analyzer.analyze_site_structure()
        
        print("\n" + "="*60)
        print("ğŸ¯ Sony Reader Store åˆ†æçµæœã‚µãƒãƒªãƒ¼")
        print("="*60)
        print("1. ç›´æ¥çš„ãªæ¤œç´¢ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¯ç™ºè¦‹å›°é›£")
        print("2. å‹•çš„ã‚µã‚¤ãƒˆãƒ»SPAæ§‹æˆã®å¯èƒ½æ€§")
        print("3. Playwrightå®Ÿè£…ãŒæœ€é©è§£")
        print("4. Google Site Searchä½µç”¨ã‚’æ¨å¥¨")
        print("\nğŸ’¡ å¤šè§’çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã®æ”»ç•¥ãŒå¿…è¦")
        
    except Exception as e:
        logger.error(f"åˆ†æå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        print(f"ğŸ’¥ åˆ†æã‚¨ãƒ©ãƒ¼: {e}")


if __name__ == "__main__":
    asyncio.run(main())