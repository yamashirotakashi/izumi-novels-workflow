#!/usr/bin/env python3
"""
é¡ä¼¼åº¦è¨ˆç®—ã®ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ„ãƒ¼ãƒ«
"""
import sys
import os
sys.path.append('/mnt/c/Users/tky99/DEV/izumi-novels-workflow/src')

from scraping.selenium_base_scraper import SeleniumBaseScraper

class DebugScraper(SeleniumBaseScraper):
    async def _search_impl(self, book_title: str, n_code: str):
        return None
    async def _verify_url(self, url: str, expected_title: str):
        return True
    
    def debug_similarity_score(self, query: str, title: str) -> float:
        """é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—ã®ãƒ‡ãƒãƒƒã‚°ç‰ˆï¼ˆæ”¹è‰¯ç‰ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºå¯¾å¿œï¼‰"""
        print(f"\nğŸ” ãƒ‡ãƒãƒƒã‚°: '{query}' vs '{title}'")
        
        # æ­£è¦åŒ–
        query_norm = self.normalize_title(query)
        title_norm = self.normalize_title(title)
        print(f"  æ­£è¦åŒ–å¾Œ: '{query_norm}' vs '{title_norm}'")
        
        # å®Œå…¨ä¸€è‡´
        if query_norm == title_norm:
            print("  â†’ å®Œå…¨ä¸€è‡´: 1.0")
            return 1.0
        
        # éƒ¨åˆ†ä¸€è‡´ï¼ˆã‚¯ã‚¨ãƒªãŒã‚¿ã‚¤ãƒˆãƒ«ã«å«ã¾ã‚Œã‚‹ï¼‰
        if query_norm in title_norm:
            print("  â†’ éƒ¨åˆ†ä¸€è‡´ï¼ˆã‚¯ã‚¨ãƒª in ã‚¿ã‚¤ãƒˆãƒ«ï¼‰: 0.9")
            return 0.9
        
        # é€†æ–¹å‘ã®éƒ¨åˆ†ä¸€è‡´ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ãŒã‚¯ã‚¨ãƒªã«å«ã¾ã‚Œã‚‹ï¼‰
        if title_norm in query_norm:
            print("  â†’ é€†éƒ¨åˆ†ä¸€è‡´ï¼ˆã‚¿ã‚¤ãƒˆãƒ« in ã‚¯ã‚¨ãƒªï¼‰: 0.85")
            return 0.85
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã«ã‚ˆã‚‹é¡ä¼¼åº¦è¨ˆç®—
        query_keywords = self._extract_keywords(query_norm)
        title_keywords = self._extract_keywords(title_norm)
        print(f"  ã‚¯ã‚¨ãƒªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {query_keywords}")
        print(f"  ã‚¿ã‚¤ãƒˆãƒ«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {title_keywords}")
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã®é¡ä¼¼åº¦è¨ˆç®—
        if query_keywords and title_keywords:
            # å…±é€šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            common_keywords = query_keywords.intersection(title_keywords)
            total_keywords = query_keywords.union(title_keywords)
            
            if common_keywords:
                # Jaccardä¿‚æ•°
                jaccard = len(common_keywords) / len(total_keywords)
                
                # å…±é€šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¯”ç‡
                common_ratio = len(common_keywords) / min(len(query_keywords), len(title_keywords))
                
                print(f"  å…±é€šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {common_keywords}")
                print(f"  Jaccardä¿‚æ•°: {jaccard:.4f}")
                print(f"  å…±é€šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¯”ç‡: {common_ratio:.4f}")
                
                # é«˜ã„é¡ä¼¼åº¦ã®å ´åˆ
                if jaccard >= 0.3 or common_ratio >= 0.5:
                    result = max(0.6, min(0.9, jaccard * 1.5 + common_ratio * 0.5))
                    print(f"  â†’ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é¡ä¼¼åº¦ï¼ˆé«˜ï¼‰: {result:.4f}")
                    return result
                
                # ä¸­ç¨‹åº¦ã®é¡ä¼¼åº¦ã®å ´åˆ
                elif jaccard >= 0.15 or common_ratio >= 0.3:
                    result = max(0.4, min(0.7, jaccard * 1.8 + common_ratio * 0.6))
                    print(f"  â†’ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é¡ä¼¼åº¦ï¼ˆä¸­ï¼‰: {result:.4f}")
                    return result
                
                # ä½ã„é¡ä¼¼åº¦ã§ã‚‚å…±é€šç‚¹ãŒã‚ã‚‹å ´åˆ
                else:
                    result = max(0.25, min(0.5, jaccard * 2.0 + common_ratio * 0.8))
                    print(f"  â†’ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é¡ä¼¼åº¦ï¼ˆä½ï¼‰: {result:.4f}")
                    return result
        
        # å˜èªãƒ¬ãƒ™ãƒ«ã§ã®é¡ä¼¼åº¦è¨ˆç®—ï¼ˆè‹±èªç­‰ã®å ´åˆï¼‰
        query_words = set(query_norm.split())
        title_words = set(title_norm.split())
        print(f"  ã‚¯ã‚¨ãƒªå˜èª: {query_words}")
        print(f"  ã‚¿ã‚¤ãƒˆãƒ«å˜èª: {title_words}")
        
        if query_words and title_words and len(query_words) > 1 and len(title_words) > 1:
            # Jaccardä¿‚æ•°ï¼ˆå…±é€šå˜èª/å…¨å˜èªï¼‰
            common_words = query_words.intersection(title_words)
            total_words = query_words.union(title_words)
            jaccard = len(common_words) / len(total_words) if total_words else 0
            
            # å…±é€šå˜èªã®é‡ã¿ã‚’è€ƒæ…®ã—ãŸé¡ä¼¼åº¦
            common_ratio = len(common_words) / min(len(query_words), len(title_words)) if min(len(query_words), len(title_words)) > 0 else 0
            
            print(f"  å…±é€šå˜èª: {common_words}")
            print(f"  Jaccardä¿‚æ•°: {jaccard:.4f}")
            print(f"  å…±é€šå˜èªæ¯”ç‡: {common_ratio:.4f}")
            
            # å˜èªãƒ¬ãƒ™ãƒ«ã®é¡ä¼¼åº¦è¨ˆç®—ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
            if jaccard > 0.15:  # é–¾å€¤ã‚’ä¸‹ã’ã‚‹
                word_similarity = max(jaccard * 1.2, common_ratio * 0.8)
                result = max(0.3, min(0.8, word_similarity))
                print(f"  â†’ å˜èªé¡ä¼¼åº¦ï¼ˆé«˜ï¼‰: {result:.4f}")
                return result
            elif common_words:  # ä½•ã‚‰ã‹ã®å…±é€šå˜èªãŒã‚ã‚‹å ´åˆ
                result = max(0.25, jaccard * 1.5)
                print(f"  â†’ å˜èªé¡ä¼¼åº¦ï¼ˆä½ï¼‰: {result:.4f}")
                return result
        
        # ç·¨é›†è·é›¢ã«ã‚ˆã‚‹é¡ä¼¼åº¦è¨ˆç®—ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        import editdistance
        max_len = max(len(query_norm), len(title_norm))
        if max_len == 0:
            return 0.0
        
        distance = editdistance.eval(query_norm, title_norm)
        print(f"  ç·¨é›†è·é›¢: {distance}, æœ€å¤§é•·: {max_len}")
        
        # é•·ã„æ–‡å­—åˆ—ã®å ´åˆã¯ç·¨é›†è·é›¢ã®å½±éŸ¿ã‚’è»½æ¸›
        if max_len > 20:
            # é•·ã„æ–‡å­—åˆ—ã§ã¯ç›¸å¯¾çš„ãªé¡ä¼¼åº¦ã‚’é‡è¦–
            similarity = 1 - (distance / max_len)
            # æœ€ä½ã‚¹ã‚³ã‚¢ã‚’0.15ã«è¨­å®šï¼ˆå®Œå…¨ã«ç„¡é–¢ä¿‚ã§ã‚‚å°‘ã—ã¯æ®‹ã‚‹ï¼‰
            result = max(0.15, similarity)
            print(f"  â†’ ç·¨é›†è·é›¢ï¼ˆé•·ï¼‰: {result:.4f}")
            return result
        else:
            # çŸ­ã„æ–‡å­—åˆ—ã§ã¯ç·¨é›†è·é›¢ã‚’ãã®ã¾ã¾åˆ©ç”¨
            similarity = 1 - (distance / max_len)
            result = max(0.0, similarity)
            print(f"  â†’ ç·¨é›†è·é›¢ï¼ˆçŸ­ï¼‰: {result:.4f}")
            return result

def debug_failed_cases():
    """å¤±æ•—ã—ãŸã‚±ãƒ¼ã‚¹ã®ãƒ‡ãƒãƒƒã‚°"""
    scraper = DebugScraper()
    
    print("ğŸ› å¤±æ•—ã—ãŸã‚±ãƒ¼ã‚¹ã®ãƒ‡ãƒãƒƒã‚°åˆ†æ")
    print("="*60)
    
    failed_cases = [
        ("èª²é•·ãŒç›®è¦šã‚ãŸã‚‰ç•°ä¸–ç•ŒSFè‰¦éšŠã®æç£ã«ãªã£ã¦ãŸä»¶ã§ã™â‘ ", "ç•°ä¸–ç•Œè»¢ç”ŸRPGç‰©èª", 0.3),
        ("é­”æ³•ä½¿ã„ã®å†’é™º", "å†’é™ºè€…ã¨é­”æ³•ä½¿ã„", 0.4),
        ("ãƒ‡ã‚¶ã‚¤ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³", "ãƒ‘ã‚¿ãƒ¼ãƒ³è¨­è¨ˆæ‰‹æ³•", 0.3),
    ]
    
    for i, (query, title, expected) in enumerate(failed_cases, 1):
        print(f"\nã€ã‚±ãƒ¼ã‚¹ {i}ã€‘æœŸå¾…å€¤: â‰¥{expected}")
        score = scraper.debug_similarity_score(query, title)
        status = "âœ… PASS" if score >= expected else "âŒ FAIL"
        print(f"  æœ€çµ‚ã‚¹ã‚³ã‚¢: {score:.4f} {status}")

if __name__ == '__main__':
    debug_failed_cases()