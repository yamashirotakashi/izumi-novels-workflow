"""
Amazon Kindle scraper with unified type-safe architecture.

Enhanced version using the new unified base scraper with comprehensive
type hints and error handling.
"""

from __future__ import annotations
from typing import Dict, List, Optional, Any
from datetime import datetime
import time

from selenium.webdriver.common.by import By

from .base_scraper import (
    BaseScraper, 
    BookInfo, 
    ScrapingResult,
    CaptchaError,
    RateLimitError, 
    NetworkError,
    ValidationError,
    ElementNotFoundError,
    ParseError
)
"""
Amazon Kindle scraper with unified type-safe architecture.

Enhanced version using the new unified base scraper with comprehensive
type hints and error handling.
"""

from __future__ import annotations
from typing import Dict, List, Optional, Any
from datetime import datetime
import time

from selenium.webdriver.common.by import By

from .base_scraper import (
    BaseScraper, 
    BookInfo, 
    ScrapingResult,
    CaptchaError,
    RateLimitError, 
    NetworkError,
    ValidationError,
    ElementNotFoundError,
    ParseError
)
class AmazonKindleScraper(BaseScraper):
    """
    Amazon Kindleå°‚ç”¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ with enhanced type safety.
    
    Updated to use the unified architecture with comprehensive type hints
    and proper error handling.
    """
    
    def __init__(self) -> None:
        super().__init__("amazon", "Amazon Kindle")
        
        # Amazonå›ºæœ‰è¨­å®š with type hints
        self.base_url: str = self.site_config.get('base_url', 'https://www.amazon.co.jp/')
        self.search_url: str = self.site_config.get('search_url', 'https://www.amazon.co.jp/s')
        self.selectors: Dict[str, List[str]] = self.site_config.get('selectors', {})
        self.wait_times: Dict[str, float] = self.site_config.get('wait_times', {})
        self.search_params: Dict[str, Any] = self.site_config.get('search_params', {})
    
    def search_books(self, query: str) -> ScrapingResult:
        """Amazon Kindleæ›¸ç±æ¤œç´¢å®Ÿè¡Œ with enhanced error handling."""
        try:
            print(f"ğŸš€ {self.site_name} æ¤œç´¢é–‹å§‹: '{query}'")
            
            # WebDriverã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
            if not self.driver:
                self.setup_driver()
            
            # æ¤œç´¢ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹
            if 'direct_url_pattern' in self.search_params:
                # ç›´æ¥æ¤œç´¢URLä½¿ç”¨
                search_url: str = self.search_params['direct_url_pattern'].format(query=query)
                print(f"ğŸ¯ ç›´æ¥æ¤œç´¢URL: {search_url}")
                self.driver.get(search_url)
            else:
                # é€šå¸¸ã®æ¤œç´¢ãƒ•ãƒ­ãƒ¼
                self.driver.get(self.base_url)
                self._perform_search(query)
            
            # ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿å¾…æ©Ÿ
            page_load_wait: float = self.wait_times.get('page_load', 4.0)
            time.sleep(page_load_wait)
            
            # æ¤œç´¢çµæœå–å¾—
            books: List[BookInfo] = self._extract_books()
            
            success: bool = len(books) > 0
            return ScrapingResult(
                site_name=self.site_name,
                site_id=self.site_id,
                query=query,
                success=success,
                books_found=books,
                error_message=None if success else "æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ",
                timestamp=datetime.now()
            )
            
        except CaptchaError as e:
            print(f"ğŸ¤– CAPTCHAæ¤œå‡º: {e}")
            return self._create_error_result(query, f"CAPTCHA challenge: {str(e)}")
        except RateLimitError as e:
            print(f"â±ï¸ ãƒ¬ãƒ¼ãƒˆåˆ¶é™: {e}")
            return self._create_error_result(query, f"Rate limit exceeded: {str(e)}")
        except NetworkError as e:
            print(f"ğŸŒ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            return self._create_error_result(query, f"Network error: {str(e)}")
        except Exception as e:
            error_msg: str = f"Amazonæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}"
            print(f"âŒ {error_msg}")
            return self._create_error_result(query, error_msg)
    
    def _create_error_result(self, query: str, error_message: str) -> ScrapingResult:
        """Create standardized error result."""
        return ScrapingResult(
            site_name=self.site_name,
            site_id=self.site_id,
            query=query,
            success=False,
            books_found=[],
            error_message=error_message,
            timestamp=datetime.now()
        )
    
    def _perform_search(self, query: str) -> None:
        """æ¤œç´¢å®Ÿè¡Œ with enhanced error handling."""
        try:
            # æ¤œç´¢ãƒœãƒƒã‚¯ã‚¹è¦ç´ å–å¾—ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯¾å¿œï¼‰
            search_selectors: List[str] = self.selectors.get('search_input', ['#twotabsearchtextbox'])
            search_box = self.find_element_flexible(search_selectors)
            
            # äººé–“ã‚‰ã—ã„ã‚¿ã‚¤ãƒ”ãƒ³ã‚°
            print(f"âŒ¨ï¸ æ¤œç´¢ã‚¯ã‚¨ãƒªå…¥åŠ›: '{query}'")
            self.human_like_typing(search_box, query)
            
            # æ¤œç´¢å®Ÿè¡Œ
            self.human_like_delay(0.5, 1.0)
            search_box.submit()
            
            # æ¤œç´¢çµæœèª­ã¿è¾¼ã¿å¾…æ©Ÿ
            results_wait: float = self.wait_times.get('search_results', 3.0)
            time.sleep(results_wait)
            
        except TimeoutException:
            raise ElementNotFoundError(
                "æ¤œç´¢ãƒœãƒƒã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“", 
                selectors=search_selectors
            )
        except Exception as e:
            raise NetworkError(f"æ¤œç´¢å®Ÿè¡Œå¤±æ•—: {e}")
    
    def _extract_books(self) -> List[BookInfo]:
        """æ›¸ç±æƒ…å ±æŠ½å‡º with enhanced type safety."""
        books: List[BookInfo] = []
        
        try:
            # æ¤œç´¢çµæœè¦ç´ å–å¾—
            result_selectors: List[str] = self.selectors.get(
                'search_results', 
                ['[data-component-type="s-search-result"]']
            )
            results = self.driver.find_elements(By.CSS_SELECTOR, result_selectors[0])
            
            print(f"ğŸ“š æ¤œç´¢çµæœ: {len(results)}ä»¶ç™ºè¦‹")
            
            # å„çµæœã‹ã‚‰æƒ…å ±æŠ½å‡ºï¼ˆæœ€å¤§10ä»¶ï¼‰
            for i, result in enumerate(results[:10]):
                try:
                    book_info: Optional[BookInfo] = self._extract_book_info(result, i + 1)
                    if book_info:
                        books.append(book_info)
                        
                except ValidationError as e:
                    print(f"âš ï¸ æ›¸ç±{i+1}ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
                except Exception as e:
                    print(f"âš ï¸ æ›¸ç±{i+1}æƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            print(f"âœ… æœ‰åŠ¹ãªæ›¸ç±æƒ…å ±: {len(books)}ä»¶æŠ½å‡º")
            return books
            
        except Exception as e:
            print(f"âŒ æ›¸ç±æƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _extract_book_info(self, result_element: Any, index: int) -> Optional[BookInfo]:
        """å€‹åˆ¥æ›¸ç±æƒ…å ±æŠ½å‡º with comprehensive validation."""
        try:
            # ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º
            title_selectors: List[str] = self.selectors.get('book_title', ['h2 a span'])
            title: str = ""
            for selector in title_selectors:
                try:
                    title_elem = result_element.find_element(By.CSS_SELECTOR, selector)
                    title = self.safe_get_text(title_elem)
                    if title:
                        break
                except:
                    continue
            
            # ãƒªãƒ³ã‚¯æŠ½å‡º
            link_selectors: List[str] = self.selectors.get('book_link', ['h2 a'])
            url: str = ""
            for selector in link_selectors:
                try:
                    link_elem = result_element.find_element(By.CSS_SELECTOR, selector)
                    href: str = self.safe_get_attribute(link_elem, 'href')
                    if href:
                        # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                        url = href if href.startswith('http') else f"https://www.amazon.co.jp{href}"
                        break
                except:
                    continue
            
            # ä¾¡æ ¼æŠ½å‡ºï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            price: str = ""
            price_selectors: List[str] = ['.a-price-whole', '.a-price', '.a-offscreen']
            for selector in price_selectors:
                try:
                    price_elem = result_element.find_element(By.CSS_SELECTOR, selector)
                    price = self.safe_get_text(price_elem)
                    if price:
                        break
                except:
                    continue
            
            # è‘—è€…æŠ½å‡ºï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            author: str = ""
            author_selectors: List[str] = ['[data-cy="title-recipe-author"]', '.a-size-base.a-link-normal']
            for selector in author_selectors:
                try:
                    author_elem = result_element.find_element(By.CSS_SELECTOR, selector)
                    author = self.safe_get_text(author_elem)
                    if author:
                        break
                except:
                    continue
            
            # å¿…é ˆæƒ…å ±ãƒã‚§ãƒƒã‚¯
            if not title or not url:
                raise ValidationError(
                    f"å¿…é ˆæƒ…å ±ä¸è¶³: title={bool(title)}, url={bool(url)}"
                )
            
            book_info = BookInfo(
                title=title,
                url=url,
                price=price if price else None,
                author=author if author else None,
                scraped_at=datetime.now(),
                site_id=self.site_id,
                site_name=self.site_name
            )
            
            print(f"ğŸ“– æ›¸ç±{index}: {title[:50]}{'...' if len(title) > 50 else ''}")
            return book_info
            
        except ValidationError:
            raise  # Re-raise validation errors
        except Exception as e:
            raise ParseError(f"æ›¸ç±æƒ…å ±è§£æå¤±æ•—: {e}")
    
    def validate_result(self, result: ScrapingResult) -> bool:
        """çµæœæ¤œè¨¼ with enhanced validation."""
        if not result.success:
            return False
        
        # æœ€ä½é™ã®çµæœæ•°ãƒã‚§ãƒƒã‚¯
        if len(result.books_found) == 0:
            print(f"âš ï¸ {self.site_name}: æ¤œç´¢çµæœãªã—")
            return False
        
        # æœ‰åŠ¹ãªURLæ•°ãƒã‚§ãƒƒã‚¯
        valid_urls: int = sum(
            1 for book in result.books_found 
            if book.url and book.url.startswith('http')
        )
        valid_threshold: float = 0.8  # 80%ä»¥ä¸Šã®URLãŒæœ‰åŠ¹
        
        if valid_urls < len(result.books_found) * valid_threshold:
            print(f"âš ï¸ {self.site_name}: æœ‰åŠ¹URLãŒä¸è¶³ ({valid_urls}/{len(result.books_found)})")
            return False
        
        print(f"âœ… {self.site_name}: çµæœæ¤œè¨¼åˆæ ¼ ({len(result.books_found)}ä»¶)")
        return True