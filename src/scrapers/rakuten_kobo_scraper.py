#!/usr/bin/env python3
"""
æ¥½å¤©Koboå®Ÿå‹•ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ - Chrome for Testingçµ±åˆç‰ˆ
Rakuten Kobo Production Scraper - Chrome for Testing Integration

Chrome for Testingçµ±åˆã«ã‚ˆã‚Šã€å®‰å®šæ€§ã¨æ€§èƒ½ã‚’å¤§å¹…æ”¹å–„
Significantly improved stability and performance through Chrome for Testing integration
"""
import asyncio
import time
import json
import random
from pathlib import Path
from urllib.parse import quote_plus
from typing import List, Dict, Any, Optional
from dataclasses import asdict

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import sys
sys.path.append(str(Path(__file__).parent.parent))

from scrapers.playwright_base_scraper import PlaywrightBaseScraper, BookInfo, ScrapingResult

class RakutenKoboScraper(PlaywrightBaseScraper):
    """æ¥½å¤©Koboå®Ÿå‹•ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ - Chrome for Testingæœ€é©åŒ–ç‰ˆ"""
    
    def __init__(self):
        super().__init__("rakuten_kobo", "æ¥½å¤©Kobo")
        self.base_url = "https://books.rakuten.co.jp"
        self.search_url = f"{self.base_url}/search"
        self.max_results_per_search = 20
        
        # æ¥½å¤©Koboå°‚ç”¨ã‚»ãƒ¬ã‚¯ã‚¿
        self.selectors = {
            "search_box": [
                'input[name="g"]',
                '#searchKeyword',
                '.js-search-input'
            ],
            "search_button": [
                'input[type="submit"]',
                '.js-search-btn',
                'button[type="submit"]'
            ],
            "ebook_filter": [
                'a[href*="ebook"]',
                'input[value*="é›»å­æ›¸ç±"]',
                '.ebook-filter'
            ],
            "result_items": [
                '.js-item',
                '.item-normal',
                '.search-item'
            ],
            "book_title": [
                '.item-title a',
                'h3 a',
                '.title-link'
            ],
            "book_link": [
                '.item-title a',
                'h3 a',
                'a[href*="/e-book/"]'
            ],
            "price": [
                '.item-price',
                '.price-value',
                '.current-price'
            ],
            "rating": [
                '.item-rating',
                '.rating-value',
                '.review-rating'
            ],
            "author": [
                '.item-author',
                '.author-name',
                '.book-author'
            ],
            "publisher": [
                '.item-publisher', 
                '.publisher-name',
                '.book-publisher'
            ]
        }
    
    async def search_books(self, query: str) -> ScrapingResult:
        """æ¥½å¤©Koboã§æ›¸ç±æ¤œç´¢å®Ÿè¡Œ"""
        print(f"ğŸ” æ¥½å¤©Koboæ¤œç´¢é–‹å§‹: '{query}'")
        start_time = time.time()
        
        try:
            # æ¥½å¤©ãƒ–ãƒƒã‚¯ã‚¹ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹
            await self.page.goto(self.base_url, wait_until='domcontentloaded')
            await self.human_like_delay(1.0, 2.0)
            
            print("âœ… æ¥½å¤©ãƒ–ãƒƒã‚¯ã‚¹ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹å®Œäº†")
            
            # æ¤œç´¢ãƒœãƒƒã‚¯ã‚¹ç™ºè¦‹ãƒ»å…¥åŠ›
            search_box = await self.find_element_flexible(self.selectors["search_box"])
            await search_box.fill("")  # ã‚¯ãƒªã‚¢
            await self.human_like_typing('input[name="g"]', query)
            print(f"âœ… æ¤œç´¢ã‚¯ã‚¨ãƒªå…¥åŠ›: '{query}'")
            
            # æ¤œç´¢å®Ÿè¡Œ
            search_button = await self.find_element_flexible(self.selectors["search_button"])
            await search_button.click()
            await self.page.wait_for_load_state('networkidle')
            await self.human_like_delay(2.0, 3.0)
            
            print("âœ… æ¤œç´¢å®Ÿè¡Œå®Œäº†")
            
            # é›»å­æ›¸ç±ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨
            await self.apply_ebook_filter()
            
            # æ¤œç´¢çµæœå–å¾—
            books_found = await self.extract_book_results(query)
            
            execution_time = time.time() - start_time
            
            result = ScrapingResult(
                site_name=self.site_name,
                site_id=self.site_id,
                query=query,
                success=len(books_found) > 0,
                books_found=books_found,
                execution_time=execution_time
            )
            
            if result.success:
                print(f"âœ… æ¥½å¤©Koboæ¤œç´¢æˆåŠŸ: {len(books_found)}å†Šç™ºè¦‹")
            else:
                print("âš ï¸ æ¥½å¤©Koboæ¤œç´¢: æ›¸ç±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            
            return result
            
        except Exception as e:
            execution_time = time.time() - start_time
            print(f"âŒ æ¥½å¤©Koboæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            
            return ScrapingResult(
                site_name=self.site_name,
                site_id=self.site_id,
                query=query,
                success=False,
                books_found=[],
                error_message=str(e),
                execution_time=execution_time
            )
    
    async def apply_ebook_filter(self) -> bool:
        """é›»å­æ›¸ç±ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’é©ç”¨"""
        try:
            print("ğŸ¯ é›»å­æ›¸ç±ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨...")
            
            # URLãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨
            current_url = self.page.url
            if "bktype=e" not in current_url:
                # é›»å­æ›¸ç±å°‚ç”¨æ¤œç´¢URLã«å¤‰æ›´
                ebook_search_url = current_url + "&bktype=e"
                await self.page.goto(ebook_search_url, wait_until='domcontentloaded')
                await self.human_like_delay(1.0, 2.0)
                print("âœ… URLãƒ™ãƒ¼ã‚¹é›»å­æ›¸ç±ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨")
                return True
            
            # UIãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨ã‚’è©¦è¡Œ
            try:
                ebook_filter = await self.page.wait_for_selector('a[href*="ebook"]', timeout=5000)
                if ebook_filter:
                    await ebook_filter.click()
                    await self.page.wait_for_load_state('networkidle')
                    await self.human_like_delay(1.0, 2.0)
                    print("âœ… UIãƒ™ãƒ¼ã‚¹é›»å­æ›¸ç±ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨")
                    return True
            except:
                pass
            
            print("âœ… é›»å­æ›¸ç±ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ—¢ã«é©ç”¨æ¸ˆã¿")
            return True
            
        except Exception as e:
            print(f"âš ï¸ é›»å­æ›¸ç±ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def extract_book_results(self, query: str) -> List[BookInfo]:
        """æ¤œç´¢çµæœã‹ã‚‰æ›¸ç±æƒ…å ±ã‚’æŠ½å‡º"""
        books = []
        
        try:
            print("ğŸ“š æ›¸ç±æƒ…å ±æŠ½å‡ºé–‹å§‹...")
            
            # æ¤œç´¢çµæœã‚¢ã‚¤ãƒ†ãƒ ã‚’å–å¾—
            result_items = await self.page.query_selector_all('.js-item')
            
            if not result_items:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢
                result_items = await self.page.query_selector_all('.item-normal')
            
            if not result_items:
                print("âš ï¸ æ¤œç´¢çµæœã‚¢ã‚¤ãƒ†ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return books
            
            print(f"ğŸ“‹ {len(result_items)}å€‹ã®æ¤œç´¢çµæœã‚’ç™ºè¦‹")
            
            # å„ã‚¢ã‚¤ãƒ†ãƒ ã‹ã‚‰æƒ…å ±æŠ½å‡º
            for i, item in enumerate(result_items[:self.max_results_per_search]):
                try:
                    book_info = await self.extract_single_book_info(item, query)
                    if book_info:
                        books.append(book_info)
                        print(f"âœ… æ›¸ç±{i+1}: {book_info.title[:50]}...")
                    
                    # æ¥½å¤©è² è·è»½æ¸›ã®ãŸã‚ã®å¾…æ©Ÿ
                    if i % 5 == 4:  # 5å†Šã”ã¨ã«å¾…æ©Ÿ
                        await self.human_like_delay(0.5, 1.0)
                        
                except Exception as e:
                    print(f"âš ï¸ æ›¸ç±{i+1}æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            print(f"ğŸ“– æ›¸ç±æƒ…å ±æŠ½å‡ºå®Œäº†: {len(books)}å†Š")
            return books
            
        except Exception as e:
            print(f"âŒ æ›¸ç±æƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return books
    
    async def extract_single_book_info(self, item_element, query: str) -> Optional[BookInfo]:
        """å˜ä¸€æ›¸ç±æƒ…å ±ã®æŠ½å‡º"""
        try:
            # ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º
            title_element = await item_element.query_selector('.item-title a')
            title = await title_element.text_content() if title_element else "ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜"
            
            # URLæŠ½å‡º
            link_element = await item_element.query_selector('.item-title a')
            relative_url = await link_element.get_attribute('href') if link_element else ""
            full_url = f"{self.base_url}{relative_url}" if relative_url.startswith('/') else relative_url
            
            # ä¾¡æ ¼æŠ½å‡º
            price_element = await item_element.query_selector('.item-price')
            price = await price_element.text_content() if price_element else None
            if price:
                price = self.clean_price_text(price)
            
            # è‘—è€…æŠ½å‡º
            author_element = await item_element.query_selector('.item-author')
            author = await author_element.text_content() if author_element else None
            
            # å‡ºç‰ˆç¤¾æŠ½å‡º
            publisher_element = await item_element.query_selector('.item-publisher')
            publisher = await publisher_element.text_content() if publisher_element else None
            
            # è©•ä¾¡æŠ½å‡º
            rating_element = await item_element.query_selector('.item-rating')
            rating = None
            if rating_element:
                rating_text = await rating_element.text_content()
                rating = self.extract_rating_score(rating_text)
            
            # åŸºæœ¬æƒ…å ±ãŒå–å¾—ã§ããŸå ´åˆã®ã¿BookInfoã‚’ä½œæˆ
            if title and title != "ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜" and full_url:
                return BookInfo(
                    title=title.strip(),
                    url=full_url,
                    price=price,
                    author=author.strip() if author else None,
                    publisher=publisher.strip() if publisher else None,
                    rating=rating,
                    availability="æ¥½å¤©Kobo"
                )
            
            return None
            
        except Exception as e:
            print(f"âš ï¸ å˜ä¸€æ›¸ç±æƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def clean_price_text(self, price_text: str) -> Optional[str]:
        """ä¾¡æ ¼ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        if not price_text:
            return None
        
        try:
            # ä¸è¦ãªæ–‡å­—ã‚’é™¤å»ã—ã¦æ•°å€¤éƒ¨åˆ†ã‚’æŠ½å‡º
            import re
            price_clean = re.sub(r'[^\d,å††]', '', price_text)
            if price_clean:
                return price_clean
            return None
        except:
            return price_text.strip()
    
    def extract_rating_score(self, rating_text: str) -> Optional[str]:
        """è©•ä¾¡ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ•°å€¤ã‚’æŠ½å‡º"""
        if not rating_text:
            return None
        
        try:
            import re
            # "4.2ç‚¹"ã€"â˜…â˜…â˜…â˜…â˜†"ãªã©ã®å½¢å¼ã‹ã‚‰æ•°å€¤æŠ½å‡º
            match = re.search(r'(\d+\.?\d*)', rating_text)
            if match:
                return f"{match.group(1)}ç‚¹"
            
            # æ˜Ÿã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            star_count = rating_text.count('â˜…') + rating_text.count('â˜†') / 2
            if star_count > 0:
                return f"{star_count}ç‚¹"
            
            return None
        except:
            return None
    
    async def get_detailed_book_info(self, book_url: str) -> Dict[str, Any]:
        """æ›¸ç±è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰è¿½åŠ æƒ…å ±ã‚’å–å¾—"""  
        try:
            print(f"ğŸ“– è©³ç´°æƒ…å ±å–å¾—: {book_url}")
            
            await self.page.goto(book_url, wait_until='domcontentloaded')
            await self.human_like_delay(1.0, 2.0)
            
            # è©³ç´°æƒ…å ±æŠ½å‡º
            details = {}
            
            # ISBNæƒ…å ±
            isbn_element = await self.page.query_selector('.isbn-info')
            if isbn_element:
                details['isbn'] = await isbn_element.text_content()
            
            # ãƒšãƒ¼ã‚¸æ•°
            pages_element = await self.page.query_selector('.page-count')
            if pages_element:
                details['pages'] = await pages_element.text_content()
            
            # ç™ºå£²æ—¥
            release_date_element = await self.page.query_selector('.release-date')
            if release_date_element:
                details['release_date'] = await release_date_element.text_content()
            
            # å•†å“èª¬æ˜
            description_element = await self.page.query_selector('.item-description')
            if description_element:
                details['description'] = await description_element.text_content()
            
            # ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•°
            review_count_element = await self.page.query_selector('.review-count')
            if review_count_element:
                details['review_count'] = await review_count_element.text_content()
            
            return details
            
        except Exception as e:
            print(f"âš ï¸ è©³ç´°æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {}

# æ±ç”¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ï¼ˆä¸¦åˆ—ã‚¨ãƒ³ã‚¸ãƒ³ç”¨ï¼‰
class GenericScraper(PlaywrightBaseScraper):
    """æ±ç”¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ - æœªå®Ÿè£…ã‚µã‚¤ãƒˆç”¨"""
    
    def __init__(self, site_id: str, site_name: str):
        super().__init__(site_id, site_name)
        print(f"âš ï¸ {site_name} ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã¯æœªå®Ÿè£…ã§ã™ï¼ˆæ±ç”¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’ä½¿ç”¨ï¼‰")
    
    async def search_books(self, query: str) -> ScrapingResult:
        """æ±ç”¨æ¤œç´¢ï¼ˆæœªå®Ÿè£…ã‚µã‚¤ãƒˆç”¨ï¼‰"""
        await self.human_like_delay(1.0, 2.0)  # å®Ÿè£…ã•ã‚Œã¦ã„ã‚‹ã‚ˆã†ã«è¦‹ã›ã‚‹
        
        return ScrapingResult(
            site_name=self.site_name,
            site_id=self.site_id,
            query=query,
            success=False,
            books_found=[],
            error_message=f"{self.site_name}ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã¯æœªå®Ÿè£…ã§ã™"
        )

# ãƒ†ã‚¹ãƒˆé–¢æ•°
async def test_rakuten_kobo_scraper():
    """æ¥½å¤©Koboã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ§ª æ¥½å¤©Koboã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 50)
    
    try:
        async with RakutenKoboScraper() as scraper:
            # ãƒ†ã‚¹ãƒˆæ¤œç´¢ã‚¯ã‚¨ãƒª
            test_queries = [
                "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚° Python",
                "ãƒ“ã‚¸ãƒã‚¹æ›¸",
                "å°èª¬"
            ]
            
            for query in test_queries[:1]:  # æœ€åˆã®1ã¤ã®ã¿ãƒ†ã‚¹ãƒˆ
                print(f"\nğŸ” ãƒ†ã‚¹ãƒˆæ¤œç´¢: '{query}'")
                result = await scraper.scrape_with_retry(query)
                
                if result.success:
                    print(f"âœ… æ¤œç´¢æˆåŠŸ: {len(result.books_found)}å†Šç™ºè¦‹")
                    print(f"â±ï¸ å®Ÿè¡Œæ™‚é–“: {result.execution_time:.2f}ç§’")
                    
                    # æœ€åˆã®3å†Šã®è©³ç´°ã‚’è¡¨ç¤º
                    for i, book in enumerate(result.books_found[:3]):
                        print(f"\nğŸ“š æ›¸ç±{i+1}:")
                        print(f"  ã‚¿ã‚¤ãƒˆãƒ«: {book.title}")
                        print(f"  ä¾¡æ ¼: {book.price or 'ä¸æ˜'}")
                        print(f"  è‘—è€…: {book.author or 'ä¸æ˜'}")
                        print(f"  å‡ºç‰ˆç¤¾: {book.publisher or 'ä¸æ˜'}")
                        print(f"  è©•ä¾¡: {book.rating or 'ä¸æ˜'}")
                        print(f"  URL: {book.url[:80]}...")
                else:
                    print(f"âŒ æ¤œç´¢å¤±æ•—: {result.error_message}")
                
                # æ¬¡ã®æ¤œç´¢ã¾ã§ã®å¾…æ©Ÿ
                await asyncio.sleep(2)
        
        print("\nğŸ‰ æ¥½å¤©Koboã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ãƒ†ã‚¹ãƒˆå®Œäº†ï¼")
        return True
        
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == '__main__':
    success = asyncio.run(test_rakuten_kobo_scraper())
    sys.exit(0 if success else 1)