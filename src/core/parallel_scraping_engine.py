#!/usr/bin/env python3
"""
ä¸¦åˆ—ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ³ã‚¸ãƒ³ - 11ã‚µã‚¤ãƒˆåŒæ™‚å®Ÿè¡ŒåŸºç›¤
Parallel Scraping Engine - 11-site Concurrent Execution Foundation

Chrome for Testingçµ±åˆã«ã‚ˆã‚Šã€Snapåˆ¶ç´„å•é¡Œã‚’å®Œå…¨å…‹æœã—ãŸä¸¦åˆ—å®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ 
Parallel execution system with Snap constraint issues completely overcome through Chrome for Testing integration
"""
import asyncio
import time
import json
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Callable
from dataclasses import dataclass, asdict
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
import threading

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import sys
sys.path.append(str(Path(__file__).parent.parent))

from scrapers.playwright_base_scraper import PlaywrightBaseScraper, BookInfo, ScrapingResult

@dataclass
class SiteConfig:
    """ã‚µã‚¤ãƒˆè¨­å®šãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    site_id: str
    site_name: str
    enabled: bool = True
    priority: int = 1  # 1=é«˜, 2=ä¸­, 3=ä½
    timeout: int = 30  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç§’æ•°
    max_retries: int = 3
    scraper_class: Optional[str] = None

@dataclass
class ParallelScrapingRequest:
    """ä¸¦åˆ—ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    query: str
    target_sites: List[str]  # ã‚µã‚¤ãƒˆIDãƒªã‚¹ãƒˆ
    max_concurrent: int = 5  # æœ€å¤§åŒæ™‚å®Ÿè¡Œæ•°
    timeout_per_site: int = 30
    include_details: bool = False  # è©³ç´°æƒ…å ±å–å¾—ãƒ•ãƒ©ã‚°

@dataclass
class ParallelScrapingResult:
    """ä¸¦åˆ—ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœ"""
    query: str
    total_sites: int
    successful_sites: int
    failed_sites: int
    total_books_found: int
    execution_time: float
    results_per_site: Dict[str, ScrapingResult]
    errors: Dict[str, str]
    started_at: str
    completed_at: str

class ParallelScrapingEngine:
    """ä¸¦åˆ—ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ³ã‚¸ãƒ³ - Chrome for TestingåŸºç›¤"""
    
    def __init__(self):
        self.config_path = Path(__file__).parent.parent.parent / "config" / "site_selectors.json"
        self.sites_config = self._load_sites_config()
        self.scrapers = {}  # ã‚µã‚¤ãƒˆID -> ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        self.semaphore = None  # åŒæ™‚å®Ÿè¡Œåˆ¶é™
        self.executor = ThreadPoolExecutor(max_workers=10)
        
        # ãƒ­ã‚°è¨­å®š
        self._setup_logging()
        
        # 11ã‚µã‚¤ãƒˆè¨­å®š
        self.target_sites = {
            "amazon_kindle": SiteConfig("amazon_kindle", "Amazon Kindle", priority=1),
            "bookwalker": SiteConfig("bookwalker", "BOOKâ˜†WALKER", priority=1),
            "ebookjapan": SiteConfig("ebookjapan", "ebookjapan", priority=1), 
            "rakuten_kobo": SiteConfig("rakuten_kobo", "æ¥½å¤©Kobo", priority=1),
            "booklive": SiteConfig("booklive", "BookLive", priority=2),
            "honto": SiteConfig("honto", "honto", priority=2),
            "kinoppy": SiteConfig("kinoppy", "ç´€ä¼Šåœ‹å±‹æ›¸åº—Kinoppy", priority=2),
            "apple_books": SiteConfig("apple_books", "Apple Books", priority=3),
            "google_play_books": SiteConfig("google_play_books", "Google Play Books", priority=1),
            "reader_store": SiteConfig("reader_store", "Reader Store", priority=3),
            "amazon_pod": SiteConfig("amazon_pod", "Amazon POD", priority=2)
        }
        
        print(f"âœ… ä¸¦åˆ—ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº† ({len(self.target_sites)}ã‚µã‚¤ãƒˆå¯¾å¿œ)")
    
    def _setup_logging(self):
        """ãƒ­ã‚°è¨­å®š"""
        log_dir = Path(__file__).parent.parent.parent / "logs"
        log_dir.mkdir(exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_dir / "parallel_scraping.log"),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def _load_sites_config(self) -> Dict[str, Any]:
        """ã‚µã‚¤ãƒˆè¨­å®šèª­ã¿è¾¼ã¿"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âš ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    async def scrape_all_sites(self, request: ParallelScrapingRequest) -> ParallelScrapingResult:
        """å…¨ã‚µã‚¤ãƒˆä¸¦åˆ—ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ"""
        print(f"ğŸš€ ä¸¦åˆ—ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: '{request.query}'")
        print(f"ğŸ¯ å¯¾è±¡ã‚µã‚¤ãƒˆ: {len(request.target_sites)}ã‚µã‚¤ãƒˆ")
        print(f"âš¡ æœ€å¤§åŒæ™‚å®Ÿè¡Œ: {request.max_concurrent}")
        print("=" * 60)
        
        start_time = time.time()
        started_at = datetime.now().isoformat()
        
        # åŒæ™‚å®Ÿè¡Œåˆ¶é™ã‚»ãƒãƒ•ã‚©
        self.semaphore = asyncio.Semaphore(request.max_concurrent)
        
        # å¯¾è±¡ã‚µã‚¤ãƒˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        target_configs = []
        for site_id in request.target_sites:
            if site_id in self.target_sites and self.target_sites[site_id].enabled:
                target_configs.append(self.target_sites[site_id])
            else:
                print(f"âš ï¸ ç„¡åŠ¹ãªã‚µã‚¤ãƒˆID: {site_id}")
        
        # å„ªå…ˆåº¦ã§ã‚½ãƒ¼ãƒˆ
        target_configs.sort(key=lambda x: x.priority)
        
        # ä¸¦åˆ—ã‚¿ã‚¹ã‚¯ä½œæˆ
        tasks = []
        for config in target_configs:
            task = asyncio.create_task(
                self._scrape_single_site(config, request.query, request.timeout_per_site)
            )
            tasks.append(task)
        
        # ä¸¦åˆ—å®Ÿè¡Œ
        print(f"âš¡ {len(tasks)}ã‚µã‚¤ãƒˆã®ä¸¦åˆ—ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹...")
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # çµæœé›†è¨ˆ
        execution_time = time.time() - start_time
        completed_at = datetime.now().isoformat()
        
        successful_results = {}
        failed_results = {}
        errors = {}
        total_books = 0
        
        for i, result in enumerate(results):
            config = target_configs[i]
            site_id = config.site_id
            
            if isinstance(result, Exception):
                # ä¾‹å¤–ãŒç™ºç”Ÿã—ãŸå ´åˆ
                errors[site_id] = str(result)
                failed_results[site_id] = ScrapingResult(
                    site_name=config.site_name,
                    site_id=site_id,
                    query=request.query,
                    success=False,
                    books_found=[],
                    error_message=str(result)
                )
                print(f"âŒ {config.site_name}: {str(result)}")
            elif result and result.success:
                # æˆåŠŸ
                successful_results[site_id] = result
                total_books += len(result.books_found)
                print(f"âœ… {config.site_name}: {len(result.books_found)}å†Šç™ºè¦‹")
            else:
                # å¤±æ•—ï¼ˆä¾‹å¤–ã§ã¯ãªã„ï¼‰
                failed_results[site_id] = result
                if result:
                    errors[site_id] = result.error_message or "ä¸æ˜ãªã‚¨ãƒ©ãƒ¼"
                print(f"âŒ {config.site_name}: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•—")
        
        # å…¨çµæœã‚’ãƒãƒ¼ã‚¸
        all_results = {**successful_results, **failed_results}
        
        # çµæœã‚µãƒãƒªãƒ¼ä½œæˆ
        final_result = ParallelScrapingResult(
            query=request.query,
            total_sites=len(target_configs),
            successful_sites=len(successful_results),
            failed_sites=len(failed_results),
            total_books_found=total_books,
            execution_time=execution_time,
            results_per_site=all_results,
            errors=errors,
            started_at=started_at,
            completed_at=completed_at
        )
        
        # çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º
        self._print_results_summary(final_result)
        
        return final_result
    
    async def _scrape_single_site(self, config: SiteConfig, query: str, timeout: int) -> Optional[ScrapingResult]:
        """å˜ä¸€ã‚µã‚¤ãƒˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ"""
        async with self.semaphore:  # åŒæ™‚å®Ÿè¡Œåˆ¶é™
            try:
                print(f"ğŸ” {config.site_name} ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹...")
                
                # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼å–å¾—ã¾ãŸã¯ä½œæˆ
                scraper = await self._get_scraper(config)
                if not scraper:
                    raise Exception(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ä½œæˆå¤±æ•—: {config.site_id}")
                
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
                result = await asyncio.wait_for(
                    scraper.scrape_with_retry(query),
                    timeout=timeout
                )
                
                return result
                
            except asyncio.TimeoutError:
                return ScrapingResult(
                    site_name=config.site_name,
                    site_id=config.site_id,
                    query=query,
                    success=False,
                    books_found=[],
                    error_message=f"ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ ({timeout}ç§’)"
                )
            except Exception as e:
                return ScrapingResult(
                    site_name=config.site_name,
                    site_id=config.site_id,
                    query=query,
                    success=False,
                    books_found=[],
                    error_message=str(e)
                )
    
    async def _get_scraper(self, config: SiteConfig) -> Optional[PlaywrightBaseScraper]:
        """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼å–å¾—ï¼ˆãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰"""
        try:
            # ã‚µã‚¤ãƒˆåˆ¥ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ä½œæˆ
            if config.site_id == "amazon_kindle":
                from scrapers.amazon_kindle_scraper import AmazonKindleScraper
                scraper = AmazonKindleScraper()
            elif config.site_id == "rakuten_kobo":
                # æ¥½å¤©Koboã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ï¼ˆä»Šå¾Œå®Ÿè£…ï¼‰
                scraper = self._create_generic_scraper(config)
            elif config.site_id == "google_play_books":
                # Google Play Booksã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ï¼ˆä»Šå¾Œå®Ÿè£…ï¼‰
                scraper = self._create_generic_scraper(config)
            else:
                # æ±ç”¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
                scraper = self._create_generic_scraper(config)
            
            # ãƒ–ãƒ©ã‚¦ã‚¶ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
            await scraper.setup_browser(headless=True)
            return scraper
            
        except Exception as e:
            print(f"âŒ {config.site_name} ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _create_generic_scraper(self, config: SiteConfig) -> PlaywrightBaseScraper:
        """æ±ç”¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ä½œæˆ"""
        from scrapers.generic_scraper import GenericScraper
        return GenericScraper(config.site_id, config.site_name)
    
    def _print_results_summary(self, result: ParallelScrapingResult):
        """çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        print("\nğŸ“Š ä¸¦åˆ—ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœã‚µãƒãƒªãƒ¼")
        print("=" * 60)
        print(f"ğŸ” æ¤œç´¢ã‚¯ã‚¨ãƒª: '{result.query}'")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {result.successful_sites}/{result.total_sites} ({result.successful_sites/result.total_sites*100:.1f}%)")
        print(f"ğŸ“š ç·ç™ºè¦‹æ›¸ç±æ•°: {result.total_books_found}å†Š")
        print(f"â±ï¸ å®Ÿè¡Œæ™‚é–“: {result.execution_time:.2f}ç§’")
        
        if result.successful_sites > 0:
            print(f"\nâœ… æˆåŠŸã‚µã‚¤ãƒˆ ({result.successful_sites}å€‹):")
            for site_id, site_result in result.results_per_site.items():
                if site_result.success:
                    print(f"  ğŸ“– {site_result.site_name}: {len(site_result.books_found)}å†Š")
        
        if result.failed_sites > 0:
            print(f"\nâŒ å¤±æ•—ã‚µã‚¤ãƒˆ ({result.failed_sites}å€‹):")
            for site_id, error in result.errors.items():
                site_name = result.results_per_site.get(site_id, {}).site_name or site_id
                print(f"  âš ï¸ {site_name}: {error}")
    
    def save_results(self, result: ParallelScrapingResult, output_path: Optional[Path] = None) -> Path:
        """çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        if not output_path:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            results_dir = Path(__file__).parent.parent.parent / "results"
            results_dir.mkdir(exist_ok=True)
            output_path = results_dir / f"scraping_result_{timestamp}.json"
        
        # çµæœã‚’JSONå½¢å¼ã§ä¿å­˜
        result_dict = asdict(result)
        
        # ScrapingResultã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’dictåŒ–
        for site_id, site_result in result_dict["results_per_site"].items():
            result_dict["results_per_site"][site_id] = asdict(site_result)
            # BookInfoã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚‚dictåŒ–
            books_data = []
            for book in site_result.books_found:
                books_data.append(asdict(book))
            result_dict["results_per_site"][site_id]["books_found"] = books_data
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result_dict, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ çµæœä¿å­˜å®Œäº†: {output_path}")
        return output_path
    
    async def cleanup(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        print("ğŸ§¹ ä¸¦åˆ—ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ³ã‚¸ãƒ³ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—...")
        
        # å…¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        cleanup_tasks = []
        for scraper in self.scrapers.values():
            if scraper:
                cleanup_tasks.append(scraper.cleanup())
        
        if cleanup_tasks:
            await asyncio.gather(*cleanup_tasks, return_exceptions=True)
        
        # ThreadPoolExecutorçµ‚äº†
        self.executor.shutdown(wait=True)
        
        print("âœ… ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")

# ãƒ†ã‚¹ãƒˆãƒ»ãƒ‡ãƒ¢é–¢æ•°
async def demo_parallel_scraping():
    """ä¸¦åˆ—ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ³ã‚¸ãƒ³ã®ãƒ‡ãƒ¢"""
    print("ğŸ§ª ä¸¦åˆ—ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ³ã‚¸ãƒ³ ãƒ‡ãƒ¢é–‹å§‹")
    print("=" * 60)
    
    try:
        engine = ParallelScrapingEngine()
        
        # ãƒ†ã‚¹ãƒˆãƒªã‚¯ã‚¨ã‚¹ãƒˆä½œæˆ
        request = ParallelScrapingRequest(
            query="ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚° Python",
            target_sites=["amazon_kindle", "rakuten_kobo", "google_play_books"],
            max_concurrent=3,
            timeout_per_site=20
        )
        
        # ä¸¦åˆ—ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
        result = await engine.scrape_all_sites(request)
        
        # çµæœä¿å­˜
        output_path = engine.save_results(result)
        
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        await engine.cleanup()
        
        print(f"\nğŸ‰ ä¸¦åˆ—ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ‡ãƒ¢å®Œäº†ï¼")
        print(f"ğŸ“ çµæœãƒ•ã‚¡ã‚¤ãƒ«: {output_path}")
        return True
        
    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¢ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == '__main__':
    success = asyncio.run(demo_parallel_scraping())
    print(f"\nğŸ“Š ãƒ‡ãƒ¢çµæœ: {'âœ… æˆåŠŸ' if success else 'âŒ å¤±æ•—'}")
    sys.exit(0 if success else 1)