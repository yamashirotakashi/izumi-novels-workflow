#!/usr/bin/env python3
"""
é«˜åº¦ãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•åŒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆç°¡ç•¥ç‰ˆï¼‰
WSLç’°å¢ƒåˆ¶ç´„å¯¾å¿œç‰ˆ
"""
import asyncio
import sys
import os
sys.path.append('/mnt/c/Users/tky99/DEV/izumi-novels-workflow/src')

async def test_scraper_initialization():
    """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆï¼ˆChromeèµ·å‹•ä»¥å¤–ï¼‰"""
    print('=== é«˜åº¦ãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•åŒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ ===')
    
    try:
        from scraping.kinoppy_advanced_scraper import KinoppyAdvancedScraper, HumanBehavior
        from scraping.reader_store_advanced_scraper import ReaderStoreAdvancedScraper
        
        print('âœ“ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ')
        
        # ã‚¯ãƒ©ã‚¹åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ
        kinoppy = KinoppyAdvancedScraper(headless=True, timeout=30)
        reader_store = ReaderStoreAdvancedScraper(headless=True, timeout=30)
        
        print('âœ“ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹åˆæœŸåŒ–æˆåŠŸ')
        
        # è¨­å®šå€¤ç¢ºèª
        print(f'Kinoppy SITE_NAME: {kinoppy.SITE_NAME}')
        print(f'Kinoppy SEARCH_URL: {kinoppy.SEARCH_URL}')
        print(f'Reader Store SITE_NAME: {reader_store.SITE_NAME}')
        print(f'Reader Store SEARCH_URL: {reader_store.SEARCH_URL}')
        
        # äººé–“å‹•ä½œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç¢ºèª
        human_behavior = HumanBehavior()
        print(f'Human Behavior Config:')
        print(f'  - Typing Speed: {human_behavior.typing_speed_min}-{human_behavior.typing_speed_max}s')
        print(f'  - Page Load Wait: {human_behavior.page_load_wait_min}-{human_behavior.page_load_wait_max}s')
        
        # ã‚¿ã‚¤ãƒˆãƒ«ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆãƒ†ã‚¹ãƒˆ
        test_title = "èª²é•·ãŒç›®è¦šã‚ãŸã‚‰ç•°ä¸–ç•ŒSFè‰¦éšŠã®æç£ã«ãªã£ã¦ãŸä»¶ã§ã™â‘ "
        
        kinoppy_variants = kinoppy._create_kinoppy_title_variants(test_title)
        reader_store_variants = reader_store._create_reader_store_title_variants(test_title)
        
        print(f'\nKinoppyæ¤œç´¢ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ ({len(kinoppy_variants)}å€‹):')
        for i, variant in enumerate(kinoppy_variants, 1):
            print(f'  {i}. "{variant}"')
        
        print(f'\nReader Storeæ¤œç´¢ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ ({len(reader_store_variants)}å€‹):')
        for i, variant in enumerate(reader_store_variants, 1):
            print(f'  {i}. "{variant}"')
        
        # çµ±è¨ˆæƒ…å ±å–å¾—
        kinoppy_stats = kinoppy.get_stats()
        reader_store_stats = reader_store.get_stats()
        
        print(f'\nKinoppyçµ±è¨ˆæƒ…å ±:')
        for key, value in kinoppy_stats.items():
            print(f'  {key}: {value}')
        
        print(f'\nReader Storeçµ±è¨ˆæƒ…å ±:')
        for key, value in reader_store_stats.items():
            print(f'  {key}: {value}')
        
        print('\n=== åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆå®Œäº†ï¼ˆæˆåŠŸï¼‰ ===')
        
        # Chromeå®Ÿè¡Œç’°å¢ƒã«ã¤ã„ã¦ã®æ³¨æ„äº‹é …
        print('\nğŸ“Œ Chromeå®Ÿè¡Œã«ã¤ã„ã¦ã®æ³¨æ„:')
        print('- WSLç’°å¢ƒã§ã¯GUI Chromeã®ç›´æ¥å®Ÿè¡Œã«åˆ¶ç´„ãŒã‚ã‚Šã¾ã™')
        print('- å®Ÿéš›ã®ãƒ†ã‚¹ãƒˆã¯Windowsç’°å¢ƒã¾ãŸã¯Dockerç’°å¢ƒã§å®Ÿè¡Œã—ã¦ãã ã•ã„')
        print('- X11 forwardingè¨­å®šã«ã‚ˆã‚Šä¸€éƒ¨å‹•ä½œã¯å¯èƒ½ã§ã™')
        
        return True
        
    except Exception as e:
        print(f'âœ— åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}')
        import traceback
        traceback.print_exc()
        return False

def test_url_validation():
    """URLæ¤œè¨¼ãƒ­ã‚¸ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ"""
    print('\n=== URLæ¤œè¨¼ãƒ†ã‚¹ãƒˆ ===')
    
    try:
        from scraping.kinoppy_advanced_scraper import KinoppyAdvancedScraper
        from scraping.reader_store_advanced_scraper import ReaderStoreAdvancedScraper
        
        kinoppy = KinoppyAdvancedScraper()
        reader_store = ReaderStoreAdvancedScraper()
        
        # Kinoppy URLæ¤œè¨¼ãƒ†ã‚¹ãƒˆ
        kinoppy_test_urls = [
            ("https://www.kinokuniya.co.jp/dsg-01-9784123456789", True),
            ("https://www.kinokuniya.co.jp/detail/book-123", True),
            ("https://www.kinokuniya.co.jp/book/987654321", True),
            ("https://example.com/book", False),
            ("", False),
        ]
        
        print('Kinoppy URLæ¤œè¨¼çµæœ:')
        for url, expected in kinoppy_test_urls:
            result = asyncio.run(kinoppy._verify_url(url, "test"))
            status = "âœ“" if result == expected else "âœ—"
            print(f'  {status} {url[:50]}... -> {result} (æœŸå¾…: {expected})')
        
        # Reader Store URLæ¤œè¨¼ãƒ†ã‚¹ãƒˆ
        reader_store_test_urls = [
            ("https://ebookstore.sony.jp/storeProduct/123", True),
            ("https://ebookstore.sony.jp/item/456", True),
            ("https://ebookstore.sony.jp/product/789", True),
            ("https://example.com/book", False),
            ("", False),
        ]
        
        print('\nReader Store URLæ¤œè¨¼çµæœ:')
        for url, expected in reader_store_test_urls:
            result = asyncio.run(reader_store._verify_url(url, "test"))
            status = "âœ“" if result == expected else "âœ—"
            print(f'  {status} {url[:50]}... -> {result} (æœŸå¾…: {expected})')
        
        print('=== URLæ¤œè¨¼ãƒ†ã‚¹ãƒˆå®Œäº† ===')
        
    except Exception as e:
        print(f'âœ— URLæ¤œè¨¼ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}')

def test_similarity_scoring():
    """é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—ãƒ†ã‚¹ãƒˆ"""
    print('\n=== é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—ãƒ†ã‚¹ãƒˆ ===')
    
    try:
        from scraping.kinoppy_advanced_scraper import KinoppyAdvancedScraper
        
        scraper = KinoppyAdvancedScraper()
        
        test_cases = [
            ("èª²é•·ãŒç›®è¦šã‚ãŸã‚‰ç•°ä¸–ç•ŒSFè‰¦éšŠã®æç£ã«ãªã£ã¦ãŸä»¶ã§ã™â‘ ", "èª²é•·ãŒç›®è¦šã‚ãŸã‚‰ç•°ä¸–ç•ŒSFè‰¦éšŠã®æç£ã«ãªã£ã¦ãŸä»¶ã§ã™ 1"),
            ("èª²é•·ãŒç›®è¦šã‚ãŸã‚‰ç•°ä¸–ç•ŒSFè‰¦éšŠã®æç£ã«ãªã£ã¦ãŸä»¶ã§ã™â‘ ", "èª²é•·ãŒç›®è¦šã‚ãŸã‚‰ç•°ä¸–ç•ŒSFè‰¦éšŠã®æç£ã«ãªã£ã¦ãŸä»¶ã§ã™"),
            ("èª²é•·ãŒç›®è¦šã‚ãŸã‚‰ç•°ä¸–ç•ŒSFè‰¦éšŠã®æç£ã«ãªã£ã¦ãŸä»¶ã§ã™â‘ ", "ç•°ä¸–ç•Œè»¢ç”ŸRPGç‰©èª"),
            ("èª²é•·ãŒç›®è¦šã‚ãŸã‚‰ç•°ä¸–ç•ŒSFè‰¦éšŠã®æç£ã«ãªã£ã¦ãŸä»¶ã§ã™â‘ ", "èª²é•· ç•°ä¸–ç•Œ è‰¦éšŠ"),
        ]
        
        print('é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—çµæœ:')
        for query, title in test_cases:
            score = scraper.calculate_similarity_score(query, title)
            print(f'  Query: "{query[:30]}..."')
            print(f'  Title: "{title[:30]}..."')
            print(f'  Score: {score:.3f}')
            print('  ---')
        
        print('=== é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—ãƒ†ã‚¹ãƒˆå®Œäº† ===')
        
    except Exception as e:
        print(f'âœ— é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {str(e)}')

async def main():
    """ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    print('ğŸš€ é«˜åº¦ãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•åŒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ åŒ…æ‹¬ãƒ†ã‚¹ãƒˆé–‹å§‹\n')
    
    # åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ
    init_success = await test_scraper_initialization()
    
    if init_success:
        # URLæ¤œè¨¼ãƒ†ã‚¹ãƒˆ
        test_url_validation()
        
        # é¡ä¼¼åº¦è¨ˆç®—ãƒ†ã‚¹ãƒˆ
        test_similarity_scoring()
    
    print('\nğŸ åŒ…æ‹¬ãƒ†ã‚¹ãƒˆå®Œäº†')
    
    if init_success:
        print('\nâœ… Phase 1å®Ÿè£…çµæœ:')
        print('- é«˜åº¦ãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•åŒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼å®Ÿè£…å®Œäº†')
        print('- undetected-chromedriverçµ±åˆæˆåŠŸ')
        print('- äººé–“ã‚‰ã—ã„å‹•ä½œãƒ‘ã‚¿ãƒ¼ãƒ³å®Ÿè£…å®Œäº†')
        print('- botæ¤œçŸ¥å›é¿æ©Ÿèƒ½å®Ÿè£…å®Œäº†')
        print('- å¤šæ®µéšæ¤œç´¢ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè£…å®Œäº†')
        print('\nğŸ“‹ æ¬¡ã®æ®µéš:')
        print('- Windowsç’°å¢ƒã¾ãŸã¯Dockerç’°å¢ƒã§ã®å®Ÿæ©Ÿãƒ†ã‚¹ãƒˆ')
        print('- Phase 2: APIé€†è§£æã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®æ¤œè¨')
        print('- Phase 3: è¤‡åˆæ‰‹æ³•å®Ÿè£…ã®æ¤œè¨')
    else:
        print('\nâŒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼ã®ãŸã‚è¿½åŠ ãƒ†ã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ')

if __name__ == '__main__':
    asyncio.run(main())