# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å…ˆè¡Œå®Ÿè£…ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

## ğŸ“‹ æ¦‚è¦

### èƒŒæ™¯
WordPressçµ±åˆã¯ç®¡ç†ä¼šç¤¾ã®å›ç­”å¾…ã¡ã®ãŸã‚ã€å…ˆè¡Œã—ã¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ©Ÿèƒ½ã‚’ç‹¬ç«‹ã—ã¦é–‹ç™ºã™ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šï¼š
- æŠ€è¡“çš„ãªèª²é¡Œã‚’æ—©æœŸã«ç™ºè¦‹ãƒ»è§£æ±º
- å‹•ä½œã™ã‚‹ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã‚’æ—©æœŸã«æº–å‚™
- ç®¡ç†ä¼šç¤¾ã¸ã®ãƒ‡ãƒ¢ãŒå¯èƒ½ã«

### åŸºæœ¬æ–¹é‡
- **ç‹¬ç«‹å‹•ä½œ**: WordPressé€£æºãªã—ã§å˜ç‹¬å‹•ä½œå¯èƒ½
- **ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­è¨ˆ**: å¾Œã§WordPressã¨å®¹æ˜“ã«çµ±åˆå¯èƒ½
- **æ®µéšçš„å®Ÿè£…**: å®‰å®šã‚µã‚¤ãƒˆã‹ã‚‰é †æ¬¡å¯¾å¿œ

---

## ğŸ—ï¸ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ

### ã‚·ã‚¹ãƒ†ãƒ æ§‹æˆ
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Scraping Engine                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚            Core Components                  â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚    â”‚
â”‚  â”‚  â”‚ BaseScraper â”‚  â”‚ ScrapingManager  â”‚   â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚            Site Adapters                   â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚    â”‚
â”‚  â”‚  â”‚  Amazon   â”‚ â”‚  Rakuten  â”‚ â”‚  Google  â”‚â”‚    â”‚
â”‚  â”‚  â”‚  Kindle   â”‚ â”‚   Kobo    â”‚ â”‚  Play    â”‚â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚    â”‚
â”‚  â”‚  ... (ä»–8ã‚µã‚¤ãƒˆ)                           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   Output Layer   â”‚
              â”‚  - JSON Export   â”‚
              â”‚  - CSV Export    â”‚
              â”‚  - API Ready     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 
```
izumi-novels-workflow/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scraping/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base/
â”‚   â”‚   â”‚   â”œâ”€â”€ base_scraper.py      # åŸºåº•ã‚¯ãƒ©ã‚¹
â”‚   â”‚   â”‚   â”œâ”€â”€ exceptions.py        # ã‚«ã‚¹ã‚¿ãƒ ä¾‹å¤–
â”‚   â”‚   â”‚   â””â”€â”€ utils.py            # å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
â”‚   â”‚   â”œâ”€â”€ adapters/
â”‚   â”‚   â”‚   â”œâ”€â”€ amazon_kindle.py    # Amazonã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼
â”‚   â”‚   â”‚   â”œâ”€â”€ rakuten_kobo.py     # æ¥½å¤©ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼
â”‚   â”‚   â”‚   â”œâ”€â”€ google_play.py      # Google Playã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼
â”‚   â”‚   â”‚   â””â”€â”€ ... (ä»–8ã‚µã‚¤ãƒˆ)
â”‚   â”‚   â”œâ”€â”€ manager.py               # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ç®¡ç†
â”‚   â”‚   â””â”€â”€ config.py               # è¨­å®šç®¡ç†
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ book.py                  # æ›¸ç±ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«
â”‚       â””â”€â”€ sales_link.py            # è²©å£²ãƒªãƒ³ã‚¯ãƒ¢ãƒ‡ãƒ«
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/                        # ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ
â”‚   â””â”€â”€ integration/                 # çµ±åˆãƒ†ã‚¹ãƒˆ
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile                   # Playwrightç’°å¢ƒ
â”‚   â””â”€â”€ docker-compose.yml
â””â”€â”€ scripts/
    â”œâ”€â”€ validate_selectors.py        # ã‚»ãƒ¬ã‚¯ã‚¿æ¤œè¨¼
    â””â”€â”€ test_single_site.py         # å˜ä¸€ã‚µã‚¤ãƒˆãƒ†ã‚¹ãƒˆ
```

---

## ğŸ“Š å®Ÿè£…å„ªå…ˆé †ä½

### Phase A: åŸºç›¤æ§‹ç¯‰ï¼ˆ1é€±é–“ï¼‰

#### 1. é–‹ç™ºç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
```yaml
# docker-compose.yml
version: '3.8'
services:
  playwright:
    build: ./docker
    volumes:
      - ./src:/app/src
      - ./tests:/app/tests
    environment:
      - PYTHONPATH=/app
      - HEADLESS=true
    command: python -m pytest tests/ -v
```

#### 2. åŸºåº•ã‚¯ãƒ©ã‚¹è¨­è¨ˆ
```python
# base_scraper.py
from abc import ABC, abstractmethod
from playwright.async_api import async_playwright
import asyncio
from typing import Optional, Dict, List

class BaseScraper(ABC):
    """å…¨ã‚µã‚¤ãƒˆå…±é€šã®åŸºåº•ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, headless: bool = True, timeout: int = 30000):
        self.headless = headless
        self.timeout = timeout
        self.browser = None
        self.page = None
        
    async def initialize(self):
        """ãƒ–ãƒ©ã‚¦ã‚¶åˆæœŸåŒ–"""
        playwright = await async_playwright().start()
        self.browser = await playwright.chromium.launch(
            headless=self.headless,
            args=['--no-sandbox', '--disable-setuid-sandbox']
        )
        self.page = await self.browser.new_page()
        
    @abstractmethod
    async def search_book(self, n_code: str, title: str) -> Optional[str]:
        """æ›¸ç±æ¤œç´¢ï¼ˆå„ã‚µã‚¤ãƒˆã§å®Ÿè£…ï¼‰"""
        pass
        
    @abstractmethod
    async def extract_book_info(self, url: str) -> Dict:
        """æ›¸ç±æƒ…å ±æŠ½å‡ºï¼ˆå„ã‚µã‚¤ãƒˆã§å®Ÿè£…ï¼‰"""
        pass
        
    async def close(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if self.browser:
            await self.browser.close()
```

### Phase B: é«˜å®‰å®šã‚µã‚¤ãƒˆå®Ÿè£…ï¼ˆ1é€±é–“ï¼‰

#### 3. Amazon Kindleã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼
```python
# adapters/amazon_kindle.py
class AmazonKindleScraper(BaseScraper):
    BASE_URL = "https://www.amazon.co.jp"
    
    async def search_book(self, n_code: str, title: str) -> Optional[str]:
        # æ¤œç´¢ãƒšãƒ¼ã‚¸ã¸ç§»å‹•
        search_url = f"{self.BASE_URL}/s?k={title}+{n_code}"
        await self.page.goto(search_url, wait_until='networkidle')
        
        # æ¤œç´¢çµæœã‹ã‚‰è©²å½“æ›¸ç±ã‚’ç‰¹å®š
        results = await self.page.query_selector_all('[data-component-type="s-search-result"]')
        
        for result in results:
            link = await result.query_selector('h2 a')
            if link:
                href = await link.get_attribute('href')
                return f"{self.BASE_URL}{href}"
        
        return None
```

#### 4. ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«å®šç¾©
```python
# models/sales_link.py
from dataclasses import dataclass
from datetime import datetime
from typing import Optional

@dataclass
class SalesLink:
    site_name: str
    url: str
    price: Optional[int] = None
    availability: bool = True
    scraped_at: datetime = None
    error: Optional[str] = None
    
    def to_dict(self):
        return {
            'site_name': self.site_name,
            'url': self.url,
            'price': self.price,
            'availability': self.availability,
            'scraped_at': self.scraped_at.isoformat() if self.scraped_at else None,
            'error': self.error
        }
```

### Phase C: ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»æœ€é©åŒ–ï¼ˆ1é€±é–“ï¼‰

#### 5. ãƒªãƒˆãƒ©ã‚¤æ©Ÿæ§‹
```python
# base/utils.py
import asyncio
from functools import wraps

def retry_on_error(max_attempts=3, delay=1.0, backoff=2.0):
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            attempt = 0
            current_delay = delay
            
            while attempt < max_attempts:
                try:
                    return await func(*args, **kwargs)
                except Exception as e:
                    attempt += 1
                    if attempt >= max_attempts:
                        raise
                    
                    await asyncio.sleep(current_delay)
                    current_delay *= backoff
                    
        return wrapper
    return decorator
```

#### 6. ä¸¦åˆ—å‡¦ç†å®Ÿè£…
```python
# manager.py
class ScrapingManager:
    def __init__(self, max_concurrent: int = 3):
        self.max_concurrent = max_concurrent
        self.scrapers = {
            'amazon_kindle': AmazonKindleScraper,
            'rakuten_kobo': RakutenKoboScraper,
            'google_play': GooglePlayScraper,
            # ... ä»–ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
        }
    
    async def scrape_all_sites(self, n_code: str, title: str) -> Dict[str, SalesLink]:
        semaphore = asyncio.Semaphore(self.max_concurrent)
        
        async def scrape_with_limit(site_name, scraper_class):
            async with semaphore:
                scraper = scraper_class()
                try:
                    await scraper.initialize()
                    url = await scraper.search_book(n_code, title)
                    if url:
                        info = await scraper.extract_book_info(url)
                        return SalesLink(
                            site_name=site_name,
                            url=url,
                            price=info.get('price'),
                            scraped_at=datetime.now()
                        )
                except Exception as e:
                    return SalesLink(
                        site_name=site_name,
                        url=None,
                        availability=False,
                        error=str(e),
                        scraped_at=datetime.now()
                    )
                finally:
                    await scraper.close()
        
        tasks = [
            scrape_with_limit(name, scraper)
            for name, scraper in self.scrapers.items()
        ]
        
        results = await asyncio.gather(*tasks)
        return {r.site_name: r for r in results}
```

---

## ğŸ§ª ãƒ†ã‚¹ãƒˆæˆ¦ç•¥

### 1. ã‚»ãƒ¬ã‚¯ã‚¿æ¤œè¨¼ãƒ„ãƒ¼ãƒ«
```python
# scripts/validate_selectors.py
async def validate_site_selectors(site_name: str):
    """ã‚µã‚¤ãƒˆã®ã‚»ãƒ¬ã‚¯ã‚¿ãŒæœ‰åŠ¹ã‹æ¤œè¨¼"""
    scraper_class = SCRAPER_MAP.get(site_name)
    if not scraper_class:
        return
    
    scraper = scraper_class()
    await scraper.initialize()
    
    # ãƒ†ã‚¹ãƒˆæ›¸ç±ã§æ¤œè¨¼
    test_books = [
        {'n_code': 'n1234567', 'title': 'ãƒ†ã‚¹ãƒˆæ›¸ç±1'},
        # ...
    ]
    
    for book in test_books:
        try:
            url = await scraper.search_book(book['n_code'], book['title'])
            print(f"âœ… {site_name}: æ¤œç´¢æˆåŠŸ - {url}")
        except Exception as e:
            print(f"âŒ {site_name}: ã‚¨ãƒ©ãƒ¼ - {e}")
```

### 2. å˜ä½“ã‚µã‚¤ãƒˆãƒ†ã‚¹ãƒˆ
```bash
# å˜ä¸€ã‚µã‚¤ãƒˆã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
python scripts/test_single_site.py --site amazon_kindle --ncode n1234567
```

---

## ğŸ“ˆ å®Ÿè£…ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«

### Week 1: åŸºç›¤æ§‹ç¯‰
- Day 1-2: Dockerç’°å¢ƒæ§‹ç¯‰ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ ä½œæˆ
- Day 3-4: BaseScraperã€ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«å®Ÿè£…
- Day 5: ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°åŸºç›¤

### Week 2: é«˜å®‰å®šã‚µã‚¤ãƒˆå®Ÿè£…
- Day 1-2: Amazon Kindleã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼
- Day 3-4: æ¥½å¤©Koboã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼
- Day 5: Google Play Booksã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼

### Week 3: æœ€é©åŒ–ãƒ»æ‹¡å¼µ
- Day 1-2: ä¸¦åˆ—å‡¦ç†ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
- Day 3-4: ä¸­å®‰å®šæ€§ã‚µã‚¤ãƒˆï¼ˆ4ã‚µã‚¤ãƒˆï¼‰è¿½åŠ 
- Day 5: ãƒ†ã‚¹ãƒˆãƒ»ãƒ‡ãƒãƒƒã‚°

### Week 4: å®Œæˆãƒ»çµ±åˆæº–å‚™
- Day 1-2: ä½å®‰å®šæ€§ã‚µã‚¤ãƒˆï¼ˆ4ã‚µã‚¤ãƒˆï¼‰è¿½åŠ 
- Day 3: åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆå®Ÿæ–½
- Day 4-5: WordPressçµ±åˆæº–å‚™ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•´å‚™

---

## ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### 1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
```bash
# ãƒªãƒã‚¸ãƒˆãƒªã‚¯ãƒ­ãƒ¼ãƒ³
git clone https://github.com/yourusername/izumi-novels-workflow.git
cd izumi-novels-workflow

# Dockerç’°å¢ƒæ§‹ç¯‰
docker-compose build
docker-compose up -d
```

### 2. ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
```bash
# å…¨ã‚µã‚¤ãƒˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ†ã‚¹ãƒˆ
docker-compose run playwright python scripts/test_all_sites.py

# ç‰¹å®šã‚µã‚¤ãƒˆã®ã¿
docker-compose run playwright python scripts/test_single_site.py --site amazon_kindle
```

### 3. å®Ÿé‹ç”¨
```python
from src.scraping.manager import ScrapingManager

async def main():
    manager = ScrapingManager()
    results = await manager.scrape_all_sites('n1234567', 'ç•°ä¸–ç•Œè»¢ç”Ÿã—ãŸä»¶')
    
    # JSONå‡ºåŠ›
    import json
    with open('results.json', 'w', encoding='utf-8') as f:
        json.dump(
            {site: link.to_dict() for site, link in results.items()},
            f,
            ensure_ascii=False,
            indent=2
        )

asyncio.run(main())
```

---

## ğŸ“Š æˆåŠŸæŒ‡æ¨™

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™
- å˜ä¸€ã‚µã‚¤ãƒˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°: < 10ç§’
- 11ã‚µã‚¤ãƒˆä¸¦åˆ—å®Ÿè¡Œ: < 30ç§’
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: < 500MB

### å“è³ªç›®æ¨™
- æˆåŠŸç‡: > 95%ï¼ˆé«˜å®‰å®šã‚µã‚¤ãƒˆï¼‰
- ã‚¨ãƒ©ãƒ¼å¾©æ—§ç‡: > 80%
- ã‚»ãƒ¬ã‚¯ã‚¿æ›´æ–°é »åº¦: < æœˆ1å›

---

**ä½œæˆæ—¥**: 2025-01-31  
**æ¬¡å›ã‚¢ã‚¯ã‚·ãƒ§ãƒ³**: Playwrightç’°å¢ƒæ§‹ç¯‰é–‹å§‹